{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "flc9XXBF_N1n"
   },
   "outputs": [],
   "source": [
    "**Mount Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJKUbhl65okI",
    "outputId": "f1330a6d-a81d-4757-c9ba-ead716674bdf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/A2A_prediction_system\"\n",
    "PKG  = f\"{BASE}/backend/a2a\"\n",
    "os.makedirs(PKG, exist_ok=True)\n",
    "\n",
    "\n",
    "open(f\"{BASE}/backend/__init__.py\", \"a\").close()\n",
    "open(f\"{PKG}/__init__.py\", \"a\").close()\n",
    "\n",
    "print(\"Created:\", PKG)\n",
    "for p in pathlib.Path(f\"{BASE}/backend\").rglob(\"*\"):\n",
    "    print(\" -\", p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "47jx7qLj_Zqa"
   },
   "outputs": [],
   "source": [
    "**Install Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYzVkFr5WbyN",
    "outputId": "127b00ef-fcae-471a-9fe4-f7c5ed77b587"
   },
   "outputs": [],
   "source": [
    "!pip install langgraph yfinance numpy pandas pandas-datareader matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "p7yNZ6lJPEgS"
   },
   "outputs": [],
   "source": [
    "**Create index.html file in frontend folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hP2ImOAdKL_5",
    "outputId": "5c1206d5-fa2f-45dc-f180-1ce3a2979031"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ui_dir = Path(\"/content/drive/MyDrive/A2A_prediction_system/frontend\")\n",
    "ui_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "html = \"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\"><title>A2A Demo UI</title></head>\n",
    "<body>\n",
    "  <h1>A2A Stock Analytics – Demo</h1>\n",
    "  <p>If you can see this, the UI is mounted at <code>/ui/index.html</code>.</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "(ui_dir / \"index.html\").write_text(html, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", ui_dir / \"index.html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "bkf0ysqMQmlH"
   },
   "outputs": [],
   "source": [
    "**Clean reload orchestrator and collector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fcqZoo5qe4G"
   },
   "outputs": [],
   "source": [
    "import sys, importlib, os\n",
    "sys.path.append(\"/content/drive/MyDrive/A2A_prediction_system/backend\")\n",
    "\n",
    "\n",
    "import a2a.data_collector_agent as dc\n",
    "import a2a.orchestrator as orch\n",
    "importlib.reload(dc)\n",
    "importlib.reload(orch)\n",
    "from a2a.orchestrator import run_orchestrator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Aos0p1HoRAFS"
   },
   "outputs": [],
   "source": [
    "**Run the Full pipeline via orchestrator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0984addc9a324bf799de3e6102f9c5cd",
      "4143be217f95439b8dd954965ebda96b",
      "28009636108844a49b6ca0393d8f6a5c",
      "d726982537774651b08a82efaef5bcbb",
      "99853865630c455d9737c2179d83ef52",
      "950a4c9aac4c479ea2d34fd1b64e4389",
      "1475dffed26d4cd98ce54288544a3928",
      "8ce659bc97b74515b841989094d17ec9",
      "fa7457a6dcf949488a5f342ea9096e5e",
      "337b12b74ce247118e061b2d0d00139a",
      "b57e2d28143942928113c80cf74fc138"
     ]
    },
    "id": "YiAKev3CNmxD",
    "outputId": "1b5a8c0f-c044-4982-a1f4-a7c9abb2414d"
   },
   "outputs": [],
   "source": [
    "import sys, importlib, os, datetime\n",
    "sys.path.append(\"/content/drive/MyDrive/A2A_prediction_system/backend\")\n",
    "\n",
    "\n",
    "import a2a.orchestrator as orch\n",
    "import a2a.model_evaluation_agent as me\n",
    "importlib.reload(me)\n",
    "importlib.reload(orch)\n",
    "\n",
    "from a2a.orchestrator import run_orchestrator\n",
    "\n",
    "res = run_orchestrator({\n",
    "    \"tickers\": [\"AAPL\"],\n",
    "\n",
    "    \"opt_args\": {\"allow_shorts\": True, \"snr_min\": 0.7, \"strong_pct\": 60, \"w_max\": 0.40},\n",
    "})\n",
    "print(\"RUN_BASE:\", res.get(\"run_base\"))\n",
    "print(\"report_path:\", res.get(\"report_path\"))\n",
    "for k in [\"reception_status\",\"collector_status\",\"fe_status\",\"predict_status\",\"eval_status\",\n",
    "          \"xai_status\",\"risk_status\",\"opt_status\",\"summary_status\",\"report_status\"]:\n",
    "    print(k, \"->\", res.get(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "MNlqnDlVRRdx"
   },
   "outputs": [],
   "source": [
    "**index.html file for the UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOsdOnnBlvPk",
    "outputId": "1c3cebb5-42bd-44d5-b1d6-deb89f34308b"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/frontend/index.html\n",
    "\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/>\n",
    "  <title>A2A Explainable Stock Analytics Platform</title>\n",
    "  <style>\n",
    "    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Inter,Arial,sans-serif;max-width:900px;margin:32px auto;padding:0 16px}\n",
    "    .card{border:1px solid #e5e7eb;border-radius:14px;padding:16px;box-shadow:0 1px 2px rgba(0,0,0,.05)}\n",
    "    .row{display:flex;gap:12px;flex-wrap:wrap}\n",
    "    label{font-weight:600;margin-top:10px;display:block}\n",
    "    input,select,textarea,button{font:inherit}\n",
    "    input,select,textarea{width:100%;padding:10px;border:1px solid #d1d5db;border-radius:10px}\n",
    "    button{padding:10px 14px;border-radius:10px;border:1px solid #1f2937;background:#111827;color:#fff;cursor:pointer}\n",
    "    button[disabled]{opacity:.6;cursor:not-allowed}\n",
    "    .muted{color:#6b7280}\n",
    "    .ok{color:#059669}\n",
    "    .err{color:#b91c1c}\n",
    "    .link{word-break:break-all}\n",
    "    .spinner{display:inline-block;width:16px;height:16px;border:2px solid #d1d5db;border-top-color:#111827;border-radius:50%;animation:spin .7s linear infinite;vertical-align:middle;margin-right:6px}\n",
    "    @keyframes spin{to{transform:rotate(360deg)}}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>A2A Explainable Stock Analytics Platform</h1>\n",
    "  <p class=\"muted\">AI-powered stock analytics platform with forecasting, explainability, evaluation, and risk insights</p>\n",
    "\n",
    "  <div class=\"card\">\n",
    "    <label for=\"prompt\">What Do You Want to Analyze?</label>\n",
    "    <textarea id=\"prompt\" rows=\"4\">Analyze AAPL for the last 5 years and forecast the next 7 days. Include evaluation, explainability, risk, and produce an HTML report.</textarea>\n",
    "\n",
    "    <div class=\"row\">\n",
    "      <div style=\"flex:1\">\n",
    "        <label for=\"tickers\">Tickers</label>\n",
    "        <input id=\"tickers\" placeholder=\"Type your Ticker\"/>\n",
    "      </div>\n",
    "      <div style=\"width:160px\">\n",
    "        <label for=\"forecast\">Forecast days</label>\n",
    "        <select id=\"forecast\">\n",
    "          <option value=\"3\">3</option>\n",
    "          <option value=\"5\">5</option>\n",
    "          <option value=\"7\"selected>7</option>\n",
    "        </select>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-top:12px\">\n",
    "      <button id=\"runBtn\">Run analysis</button>\n",
    "      <span id=\"status\" class=\"muted\" style=\"margin-left:8px\"></span>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"result\" style=\"margin-top:14px\"></div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Embedded report viewer -->\n",
    "  <div id=\"viewer\" style=\"margin-top:16px\">\n",
    "    <iframe id=\"reportFrame\"\n",
    "            style=\"width:100%; height:70vh; border:1px solid #e5e7eb; border-radius:12px\"\n",
    "            src=\"about:blank\"></iframe>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    const statusEl = document.getElementById('status');\n",
    "    const resEl    = document.getElementById('result');\n",
    "    const runBtn   = document.getElementById('runBtn');\n",
    "    const frameEl  = document.getElementById('reportFrame');\n",
    "\n",
    "    document.getElementById('forecast').addEventListener('change', syncPrompt);\n",
    "    document.getElementById('tickers').addEventListener('blur', syncPrompt);\n",
    "\n",
    "    function syncPrompt(){\n",
    "      const base = \"Analyze {TICS} for the last 5 years and forecast the next {D} days. Include evaluation, explainability, risk, and produce an HTML report.\";\n",
    "      const d = document.getElementById('forecast').value;\n",
    "      const t = (document.getElementById('tickers').value || \"AAPL\").trim();\n",
    "      document.getElementById('prompt').value = base.replace(\"{TICS}\", t).replace(\"{D}\", d);\n",
    "    }\n",
    "\n",
    "    async function pollStatus(run_id, timeoutMs=10*60*1000, intervalMs=3000){\n",
    "      const start = Date.now();\n",
    "      while (Date.now() - start < timeoutMs){\n",
    "        try{\n",
    "          const r = await fetch(`/status/${encodeURIComponent(run_id)}`);\n",
    "          const s = await r.json();\n",
    "          if (s.ready) return s.report;\n",
    "        }catch(e){ /* ignore and keep polling */ }\n",
    "        await new Promise(res => setTimeout(res, intervalMs));\n",
    "      }\n",
    "      throw new Error(\"Timed out waiting for report.\");\n",
    "    }\n",
    "\n",
    "    runBtn.onclick = async () => {\n",
    "      resEl.innerHTML = \"\";\n",
    "      frameEl.src = \"about:blank\";\n",
    "      statusEl.innerHTML = `<span class=\"spinner\"></span>Submitting…`;\n",
    "      runBtn.disabled = true;\n",
    "\n",
    "      try {\n",
    "        const user_request = document.getElementById('prompt').value.trim();\n",
    "        const tickers = (document.getElementById('tickers').value || \"\")\n",
    "                        .split(/[,\\s]+/).filter(Boolean);\n",
    "\n",
    "        const body = { user_request, report_out_name: \"llm_report.html\" };\n",
    "        if (tickers.length) body.tickers = tickers;\n",
    "\n",
    "\n",
    "        const r = await fetch(\"/run-nl-async\", {\n",
    "          method: \"POST\",\n",
    "          headers: {\"Content-Type\":\"application/json\"},\n",
    "          body: JSON.stringify(body)\n",
    "        });\n",
    "        const data = await r.json();\n",
    "        if (!r.ok) throw new Error(data.error || JSON.stringify(data));\n",
    "\n",
    "        const run_id = data.run_id || (data.run_base || \"\").split(\"/\").pop();\n",
    "        if (!run_id) throw new Error(\"No run_id in response.\");\n",
    "\n",
    "        statusEl.innerHTML = `<span class=\"spinner\"></span>Working… generating report…`;\n",
    "\n",
    "        // Wait until the report exists, then display it\n",
    "        const reportUrl = await pollStatus(run_id);\n",
    "        resEl.innerHTML =\n",
    "          `<div><b class=\"ok\">Done.</b> Report: <a class=\"link\" href=\"${reportUrl}\" target=\"_blank\">${reportUrl}</a></div>`;\n",
    "        frameEl.src = reportUrl;\n",
    "\n",
    "        frameEl.scrollIntoView({behavior:\"smooth\", block:\"start\"});\n",
    "        statusEl.textContent = \"\";\n",
    "      } catch (err) {\n",
    "        statusEl.textContent = \"\";\n",
    "        resEl.innerHTML = `<div class=\"err\">Error: ${err.message || err}</div>`;\n",
    "      } finally {\n",
    "        runBtn.disabled = false;\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "GRzjpIIja3Nf"
   },
   "outputs": [],
   "source": [
    "**app.py - Pipeline entry point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozsarHSoXsZ_",
    "outputId": "95acda3b-5ae9-4faf-ea04-a37995cda618"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/app.py\n",
    "\n",
    "# backend/app.py\n",
    "import os\n",
    "import datetime\n",
    "from fastapi import FastAPI, Body, HTTPException, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.responses import FileResponse, JSONResponse, HTMLResponse\n",
    "\n",
    "from backend.a2a.orchestrator import run_orchestrator\n",
    "\n",
    "app = FastAPI(title=\"A2A Orchestrator API\")\n",
    "\n",
    "# --- CORS  ---\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# --- Paths ---\n",
    "A2A_ROOT = os.environ.get(\"A2A_ROOT\", \"/content/drive/MyDrive/A2A_prediction_system\")\n",
    "FRONTEND_DIR = os.path.join(A2A_ROOT, \"frontend\")\n",
    "\n",
    "# Serve Drive so images in the report load\n",
    "if os.path.isdir(A2A_ROOT):\n",
    "    app.mount(\"/files\", StaticFiles(directory=A2A_ROOT), name=\"files\")\n",
    "\n",
    "# Serve the static UI: <A2A_ROOT>/frontend/index.html -> /ui/index.html\n",
    "if os.path.isdir(FRONTEND_DIR):\n",
    "    app.mount(\"/ui\", StaticFiles(directory=FRONTEND_DIR, html=True), name=\"ui\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def _new_run_base() -> tuple[str, str]:\n",
    "    \"\"\"Create a fresh run folder and return (run_id, run_base).\"\"\"\n",
    "    run_id = datetime.datetime.now().strftime(\"RUN_%Y%m%d_%H%M%S\")\n",
    "    run_base = os.path.join(A2A_ROOT, run_id)\n",
    "    os.makedirs(os.path.join(run_base, \"reports\"), exist_ok=True)\n",
    "    return run_id, run_base\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Routes\n",
    "# ---------------------------------------------------------------------\n",
    "@app.get(\"/\")\n",
    "def root_redirect():\n",
    "    if os.path.isdir(FRONTEND_DIR):\n",
    "        return HTMLResponse('<meta http-equiv=\"refresh\" content=\"0; url=/ui/index.html\">')\n",
    "    return HTMLResponse('<meta http-equiv=\"refresh\" content=\"0; url=/docs\">')\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"ok\": True}\n",
    "\n",
    "\n",
    "# Synchronous run with structured JSON\n",
    "@app.post(\"/run\")\n",
    "def run_pipeline(payload: dict = Body(...)):\n",
    "    payload = dict(payload or {})\n",
    "    payload.setdefault(\"report_out_name\", \"llm_report.html\")\n",
    "    # Ensure a stable run_id in the response\n",
    "    if \"run_base\" not in payload:\n",
    "        run_id, run_base = _new_run_base()\n",
    "        payload[\"run_base\"] = run_base\n",
    "    else:\n",
    "        run_id = os.path.basename(payload[\"run_base\"].rstrip(\"/\"))\n",
    "    res = run_orchestrator(payload)\n",
    "    res[\"run_id\"] = run_id\n",
    "    return JSONResponse(res)\n",
    "\n",
    "\n",
    "# Synchronous natural-language run\n",
    "@app.post(\"/run-nl\")\n",
    "def run_pipeline_nl(\n",
    "    user_request: str = Body(..., embed=True),\n",
    "    report_out_name: str | None = Body(None),\n",
    "    tickers: list[str] | None = Body(None),\n",
    "):\n",
    "    run_id, run_base = _new_run_base()\n",
    "    cfg = {\n",
    "        \"user_request\": user_request,\n",
    "        \"report_out_name\": report_out_name or \"llm_report.html\",\n",
    "        \"run_base\": run_base,\n",
    "    }\n",
    "    if tickers:\n",
    "        cfg[\"tickers\"] = tickers\n",
    "    res = run_orchestrator(cfg)\n",
    "    res[\"run_id\"] = run_id\n",
    "    return JSONResponse(res)\n",
    "\n",
    "\n",
    "# Asynchronous natural-language run\n",
    "@app.post(\"/run-nl-async\")\n",
    "def run_pipeline_nl_async(\n",
    "    background_tasks: BackgroundTasks,\n",
    "    user_request: str = Body(..., embed=True),\n",
    "    report_out_name: str | None = Body(None),\n",
    "    tickers: list[str] | None = Body(None),\n",
    "):\n",
    "    run_id, run_base = _new_run_base()\n",
    "    cfg = {\n",
    "        \"user_request\": user_request,\n",
    "        \"report_out_name\": report_out_name or \"llm_report.html\",\n",
    "        \"run_base\": run_base,\n",
    "    }\n",
    "    if tickers:\n",
    "        cfg[\"tickers\"] = tickers\n",
    "\n",
    "    # Launch heavy pipeline in the background and return immediately\n",
    "    background_tasks.add_task(run_orchestrator, cfg)\n",
    "\n",
    "    return {\n",
    "        \"accepted\": True,\n",
    "        \"run_id\": run_id,\n",
    "        \"run_base\": run_base,\n",
    "        \"status_url\": f\"/status/{run_id}\",\n",
    "        \"report_url\": f\"/report-html/{run_id}\",\n",
    "        \"message\": \"Run started in background.\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Is the HTML report there yet?\n",
    "@app.get(\"/status/{run_id}\")\n",
    "def run_status(run_id: str):\n",
    "    rpt = os.path.join(A2A_ROOT, run_id, \"reports\", \"llm_report.html\")\n",
    "    ready = os.path.exists(rpt)\n",
    "    return {\n",
    "        \"run_id\": run_id,\n",
    "        \"ready\": ready,\n",
    "        \"report\": f\"/report-html/{run_id}\" if ready else None,\n",
    "    }\n",
    "\n",
    "\n",
    "# Download any report file (HTML or other)\n",
    "@app.get(\"/report/{run_id}/{filename}\")\n",
    "def get_report(run_id: str, filename: str):\n",
    "    path = os.path.join(A2A_ROOT, run_id, \"reports\", filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise HTTPException(404, f\"Report not found: {path}\")\n",
    "    return FileResponse(path)\n",
    "\n",
    "\n",
    "# HTML preview: rewrite absolute Drive paths to files\n",
    "@app.get(\"/report-html/{run_id}\")\n",
    "def preview_html(run_id: str):\n",
    "    path = os.path.join(A2A_ROOT, run_id, \"reports\", \"llm_report.html\")\n",
    "    if not os.path.exists(path):\n",
    "        raise HTTPException(404, f\"Report not found: {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    content = content.replace(A2A_ROOT, \"/files\")\n",
    "    return HTMLResponse(content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiMnhcmLFEgP"
   },
   "outputs": [],
   "source": [
    "# backend/app.py\n",
    "from fastapi import FastAPI\n",
    "from a2a.orchestrator import run_orchestrator\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"ok\": True}\n",
    "\n",
    "@app.post(\"/run\")\n",
    "def run(payload: dict):\n",
    "    return run_orchestrator(payload or {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "lyvjlOFvcR_P"
   },
   "outputs": [],
   "source": [
    "**orchestrator.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1z_-VQEHqvk",
    "outputId": "858299f1-b264-455f-954f-5f9848ac7a9a"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/orchestrator.py\n",
    "\n",
    "import os, datetime\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# import agent modules\n",
    "from . import reception                 as rec\n",
    "from . import data_collector_agent      as dc\n",
    "from . import feature_engineering_agent as fe\n",
    "from . import predictive_model_agent    as pm\n",
    "from . import model_evaluation_agent    as me\n",
    "from . import explainability_agent      as xai\n",
    "from . import risk_assessment_agent     as risk\n",
    "from . import optimisation_agent        as opt\n",
    "from . import summarizer_agent          as summ\n",
    "from .report_agent import render_report\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _ts() -> str:\n",
    "    return datetime.datetime.now().strftime(\"RUN_%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _ensure_run_base(state: Dict[str, Any]) -> str:\n",
    "    rb = state.get(\"run_base\") or os.environ.get(\"RUN_BASE\")\n",
    "    if not rb:\n",
    "        root = os.environ.get(\"A2A_ROOT\", \"/content/drive/MyDrive/A2A_prediction_system\")\n",
    "        rb = os.path.join(root, _ts())\n",
    "    os.makedirs(rb, exist_ok=True)\n",
    "    os.environ[\"RUN_BASE\"] = rb\n",
    "    return rb\n",
    "\n",
    "def _ok(res: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Consider a step ok unless it explicitly reports an error/failure.\"\"\"\n",
    "    if not isinstance(res, dict): return False\n",
    "    if res.get(\"error\"): return False\n",
    "    s = str(res.get(\"status\", \"\")).lower()\n",
    "    if any(b in s for b in (\"fail\",\"error\",\"exception\",\"not found\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _invoke(build_fn, args: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:\n",
    "    app = build_fn()\n",
    "    out = app.invoke(args)\n",
    "    return _ok(out), out\n",
    "\n",
    "def _flag(state: Dict[str, Any], *keys: str, default: bool=True) -> bool:\n",
    "    \"\"\"Return first present flag among synonyms, else default.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in state: return bool(state[k])\n",
    "    return default\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Nodes\n",
    "# =========================\n",
    "def node_init(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    rb = _ensure_run_base(state)\n",
    "\n",
    "    # Map common top-level keys\n",
    "    st = {**state, \"run_base\": rb}\n",
    "\n",
    "    # Collector args: support both start/end and date_start/date_end\n",
    "    ca = {**st.get(\"collector_args\", {})}\n",
    "    if \"start_date\" in st: ca[\"start\"] = st[\"start_date\"]\n",
    "    if \"end_date\"   in st: ca[\"end\"]   = st[\"end_date\"]\n",
    "    if \"date_start\" in st: ca[\"start\"] = st[\"date_start\"]\n",
    "    if \"date_end\"   in st: ca[\"end\"]   = st[\"date_end\"]\n",
    "    if \"tickers\"    in st: ca[\"tickers\"] = st[\"tickers\"]\n",
    "    st[\"collector_args\"] = ca\n",
    "\n",
    "    # Predict args\n",
    "    pa = {**st.get(\"predict_args\", {})}\n",
    "    if \"seq_len\" in st: pa[\"seq_len\"] = st[\"seq_len\"]\n",
    "    st[\"predict_args\"] = pa\n",
    "\n",
    "    return {**st, \"init_status\": {\"ok\": True, \"run_base\": rb}}\n",
    "\n",
    "def node_reception(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse natural-language request:\n",
    "      - tickers\n",
    "      - date_start/date_end\n",
    "      - seq_len\n",
    "    \"\"\"\n",
    "    ok, out = _invoke(rec.build_llm_reception_workflow, {\n",
    "        \"user_request\": state.get(\"user_request\", \"\"),\n",
    "        \"default_seq_len\": state.get(\"default_seq_len\", 40),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "        \"tickers\": state.get(\"tickers\"),\n",
    "    })\n",
    "\n",
    "    new_state = {**state, \"reception_status\": {\"ok\": ok}, \"reception_out\": out}\n",
    "    print(\"[RECEPTION] ack:\", out.get(\"ack_text\"))\n",
    "\n",
    "    # tickers\n",
    "    tickers = out.get(\"tickers\") or new_state.get(\"tickers\")\n",
    "    if tickers:\n",
    "        new_state[\"tickers\"] = tickers\n",
    "\n",
    "    # date window\n",
    "    ca = {**new_state.get(\"collector_args\", {})}\n",
    "    if out.get(\"date_start\"): ca[\"start\"] = out[\"date_start\"]\n",
    "    if out.get(\"date_end\"):   ca[\"end\"]   = out[\"date_end\"]\n",
    "\n",
    "    if out.get(\"start\"): ca[\"start\"] = out[\"start\"]\n",
    "    if out.get(\"end\"):   ca[\"end\"]   = out[\"end\"]\n",
    "    new_state[\"collector_args\"] = ca\n",
    "\n",
    "    # seq_len -> predict args\n",
    "    pa = {**new_state.get(\"predict_args\", {})}\n",
    "    if out.get(\"seq_len\"): pa[\"seq_len\"] = out[\"seq_len\"]\n",
    "    new_state[\"predict_args\"] = pa\n",
    "\n",
    "    # run_base\n",
    "    if out.get(\"run_base\"):\n",
    "        new_state[\"run_base\"] = out[\"run_base\"]\n",
    "        os.environ[\"RUN_BASE\"] = out[\"run_base\"]\n",
    "\n",
    "    return new_state\n",
    "\n",
    "def node_collect(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_collector\", default=True):\n",
    "        return state\n",
    "\n",
    "    ok0, out = _invoke(dc.build_data_collector_workflow, {\n",
    "        **state.get(\"collector_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "        \"tickers\": state.get(\"tickers\"),\n",
    "    })\n",
    "\n",
    "    data_path = out.get(\"data_path\")\n",
    "\n",
    "    if not data_path:\n",
    "        cand = os.path.join(state[\"run_base\"], \"data\", \"yfinance_raw_data.csv\")\n",
    "        if os.path.exists(cand): data_path = cand\n",
    "\n",
    "    exists = bool(data_path and os.path.exists(data_path))\n",
    "    ok = ok0 or exists or (\"success\" in str(out.get(\"status\",\"\")).lower())\n",
    "\n",
    "    if ok and exists:\n",
    "        fe_args = {**state.get(\"fe_args\", {})}\n",
    "        fe_args[\"data_path\"]     = data_path\n",
    "        fe_args[\"raw_data_path\"] = data_path\n",
    "        fe_args[\"input_csv\"]     = data_path\n",
    "        fe_args[\"run_base\"]      = state[\"run_base\"]\n",
    "        return {**state,\n",
    "                \"collector_status\": {\"ok\": True},\n",
    "                \"collector_out\": out,\n",
    "                \"fe_args\": fe_args}\n",
    "    else:\n",
    "        return {**state,\n",
    "                \"collector_status\": {\"ok\": False, \"error\": out.get(\"error\")},\n",
    "                \"collector_out\": out,\n",
    "                # disable downstream steps if collection failed\n",
    "                \"run_fe\": False, \"run_predict\": False, \"run_eval\": False,\n",
    "                \"run_xai\": False, \"run_risk\": False, \"run_opt\": False,\n",
    "                \"run_summary\": False, \"run_report\": False}\n",
    "\n",
    "def node_fe(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_fe\", default=True):\n",
    "        return state\n",
    "    ok, out = _invoke(fe.build_enhanced_feature_workflow, {\n",
    "        **state.get(\"fe_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "    })\n",
    "    return {**state, \"fe_status\": {\"ok\": ok}, \"fe_out\": out}\n",
    "\n",
    "def node_predict(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_predict\", default=True):\n",
    "        return state\n",
    "    ok, out = _invoke(pm.build_predictive_workflow, {\n",
    "        **state.get(\"predict_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "    })\n",
    "    return {**state, \"predict_status\": {\"ok\": ok}, \"predict_out\": out}\n",
    "\n",
    "def node_eval(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_eval\", default=True):\n",
    "        return state\n",
    "    ok, out = _invoke(me.build_evaluation_workflow, {\n",
    "        **state.get(\"eval_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "    })\n",
    "    return {**state, \"eval_status\": {\"ok\": ok}, \"eval_out\": out}\n",
    "\n",
    "def node_xai(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # accept multiple flag spellings\n",
    "    if not _flag(state, \"run_xai\", \"run_xai_shap\", \"run_xai_ig\", default=True):\n",
    "        return state\n",
    "    ok, out = _invoke(xai.build_explainability_workflow, {\n",
    "        **state.get(\"xai_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "    })\n",
    "    return {**state, \"xai_status\": {\"ok\": ok}, \"xai_out\": out}\n",
    "\n",
    "def node_risk(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_risk\", default=True):\n",
    "        return state\n",
    "    ok, out = _invoke(risk.build_risk_workflow, {\n",
    "        **state.get(\"risk_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "    })\n",
    "    return {**state, \"risk_status\": {\"ok\": ok}, \"risk_out\": out}\n",
    "\n",
    "def node_opt(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_opt\", default=True):\n",
    "        return state\n",
    "    ok, out = _invoke(opt.build_optimization_workflow, {\n",
    "        **state.get(\"opt_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "    })\n",
    "    return {**state, \"opt_status\": {\"ok\": ok}, \"opt_out\": out}\n",
    "\n",
    "def node_summary(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_summary\", \"run_summarizer\", default=True):\n",
    "        return state\n",
    "    ok, out = _invoke(summ.build_summarizer_workflow, {\n",
    "        **state.get(\"summary_args\", {}),\n",
    "        \"run_base\": state[\"run_base\"],\n",
    "    })\n",
    "    return {**state, \"summary_status\": {\"ok\": ok}, \"summary_out\": out}\n",
    "\n",
    "def node_report(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not _flag(state, \"run_report\", \"run_llm_report\", default=True):\n",
    "        return state\n",
    "    path = render_report(\n",
    "        run_base=state[\"run_base\"],\n",
    "        out_name=state.get(\"report_out_name\",\"llm_report.html\"),\n",
    "        order=state.get(\"tickers\"),\n",
    "    )\n",
    "    return {**state, \"report_status\": {\"ok\": bool(path)}, \"report_path\": path}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Workflow\n",
    "# =========================\n",
    "def build_orchestrator_workflow():\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"init\",      node_init)\n",
    "    g.add_node(\"reception\", node_reception)\n",
    "    g.add_node(\"collect\",   node_collect)\n",
    "    g.add_node(\"features\",  node_fe)\n",
    "    g.add_node(\"predict\",   node_predict)\n",
    "    g.add_node(\"eval\",      node_eval)\n",
    "    g.add_node(\"xai\",       node_xai)\n",
    "    g.add_node(\"risk\",      node_risk)\n",
    "    g.add_node(\"opt\",       node_opt)\n",
    "    g.add_node(\"summary\",   node_summary)\n",
    "    g.add_node(\"report\",    node_report)\n",
    "\n",
    "    g.set_entry_point(\"init\")\n",
    "    g.add_edge(\"init\", \"reception\")\n",
    "    g.add_edge(\"reception\",\"collect\")\n",
    "    g.add_edge(\"collect\",\"features\")\n",
    "    g.add_edge(\"features\",\"predict\")\n",
    "    g.add_edge(\"predict\",\"eval\")\n",
    "    g.add_edge(\"eval\",\"xai\")\n",
    "    g.add_edge(\"xai\",\"risk\")\n",
    "    g.add_edge(\"risk\",\"opt\")\n",
    "    g.add_edge(\"opt\",\"summary\")\n",
    "    g.add_edge(\"summary\",\"report\")\n",
    "    g.set_finish_point(\"report\")\n",
    "    return g.compile()\n",
    "\n",
    "def run_orchestrator(config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    print(\"[ENTRY]\", \"NL mode\" if config.get(\"user_request\") else \"Structured mode\")\n",
    "    app = build_orchestrator_workflow()\n",
    "    out = app.invoke(config or {})\n",
    "    import os\n",
    "    run_base = out.get(\"run_base\") or \"\"\n",
    "    run_id   = os.path.basename(run_base) if run_base else None\n",
    "    return {\n",
    "        \"status\": \"done\",\n",
    "        \"run_id\": run_id,\n",
    "        \"run_base\": run_base,\n",
    "        \"report_path\": out.get(\"report_path\"),\n",
    "        \"reception_status\": out.get(\"reception_status\"),\n",
    "        \"reception_out\": out.get(\"reception_out\"),\n",
    "        \"collector_status\": out.get(\"collector_status\"),\n",
    "        \"fe_status\": out.get(\"fe_status\"),\n",
    "        \"predict_status\": out.get(\"predict_status\"),\n",
    "        \"eval_status\": out.get(\"eval_status\"),\n",
    "        \"xai_status\": out.get(\"xai_status\"),\n",
    "        \"risk_status\": out.get(\"risk_status\"),\n",
    "        \"opt_status\": out.get(\"opt_status\"),\n",
    "        \"summary_status\": out.get(\"summary_status\"),\n",
    "        \"report_status\": out.get(\"report_status\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lINELTQYma-W",
    "outputId": "471b37d8-9675-419e-ae7f-0ceffc9abc8a"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/reception.py\n",
    "import re\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "\n",
    "def _now_date() -> pd.Timestamp:\n",
    "    return pd.Timestamp.today().normalize()\n",
    "\n",
    "def _parse_rel_window(text: str) -> Tuple[Optional[pd.Timestamp], Optional[pd.Timestamp]]:\n",
    "    \"\"\"Parse phrases like 'last 5 years', 'last 6 months', 'last 30 days'.\"\"\"\n",
    "    t = text.lower()\n",
    "    end = _now_date()\n",
    "    if re.search(r'\\bytd\\b|\\byear[-\\s]*to[-\\s]*date\\b', t):\n",
    "        start = pd.Timestamp(end.year, 1, 1)\n",
    "        return start, end\n",
    "    m = re.search(r'last\\s+(\\d+)\\s*(year|years|yr|yrs|month|months|mo|mos|week|weeks|day|days)', t)\n",
    "    if m:\n",
    "        n = int(m.group(1))\n",
    "        unit = m.group(2)\n",
    "        if unit.startswith(('year', 'yr')):\n",
    "            start = end - pd.DateOffset(years=n)\n",
    "        elif unit.startswith('month') or unit.startswith('mo'):\n",
    "            start = end - pd.DateOffset(months=n)\n",
    "        elif unit.startswith('week'):\n",
    "            start = end - pd.Timedelta(days=7 * n)\n",
    "        else:\n",
    "            start = end - pd.Timedelta(days=n)\n",
    "        return start.normalize(), end\n",
    "    return None, None\n",
    "\n",
    "def _parse_explicit_dates(text: str) -> Tuple[Optional[pd.Timestamp], Optional[pd.Timestamp]]:\n",
    "    \"\"\"Parse explicit 'from YYYY-MM-DD' / 'to YYYY-MM-DD' spans.\"\"\"\n",
    "    start = end = None\n",
    "    m = re.search(r'(from|since|start[,:\\s]*)\\s*([0-9]{4}[-/][0-9]{1,2}[-/][0-9]{1,2})', text, flags=re.I)\n",
    "    if m:\n",
    "        try: start = pd.to_datetime(m.group(2))\n",
    "        except Exception: pass\n",
    "    m = re.search(r'(to|until|end[,:\\s]*)\\s*([0-9]{4}[-/][0-9]{1,2}[-/][0-9]{1,2})', text, flags=re.I)\n",
    "    if m:\n",
    "        try: end = pd.to_datetime(m.group(2))\n",
    "        except Exception: pass\n",
    "    return start, end\n",
    "\n",
    "def _default_dates(start: Optional[pd.Timestamp], end: Optional[pd.Timestamp]) -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
    "    \"\"\"If dates missing, default to last 3 years.\"\"\"\n",
    "    e = (end or _now_date()).normalize()\n",
    "    s = (start or (e - pd.DateOffset(years=3))).normalize()\n",
    "    if s > e:\n",
    "        s, e = e - pd.DateOffset(years=1), e\n",
    "    return s, e\n",
    "\n",
    "def _parse_seq_len(text: str, default: int = 30) -> int:\n",
    "    m = re.search(r'(seq[_\\s-]*len|sequence\\s*length)\\s*[:=]?\\s*(\\d+)', text, flags=re.I)\n",
    "    if m:\n",
    "        try:\n",
    "            return max(2, int(m.group(2)))\n",
    "        except Exception:\n",
    "            return default\n",
    "    return default\n",
    "\n",
    "def _parse_tickers(text: str) -> List[str]:\n",
    "    \"\"\"General ticker parser.\"\"\"\n",
    "    t = text.upper()\n",
    "    STOP = {\n",
    "        \"I\",\"ME\",\"MY\",\"WE\",\"OUR\",\"YOU\",\"YOUR\",\"PLEASE\",\n",
    "        \"WANT\",\"KNOW\",\"SHOW\",\"TELL\",\"TREND\",\"TRENDS\",\"STOCK\",\"MARKET\",\n",
    "        \"FOR\",\"OF\",\"THE\",\"A\",\"AN\",\"AND\",\"LAST\",\"YEARS\",\"YEAR\",\n",
    "        \"MONTHS\",\"MONTH\",\"WEEKS\",\"WEEK\",\"DAYS\",\"DAY\",\n",
    "        \"IS\",\"IT\",\"OKAY\",\"TO\",\"TRADE\",\"NOW\",\"ABOUT\",\"ON\",\"IN\",\"OVER\",\n",
    "        \"RUN\",\"FE\",\"SEQ\",\"LEN\",\"YTD\",\"FROM\",\"SINCE\",\"TO\",\"UNTIL\",\"END\",\n",
    "        \"PREDICT\",\"EVAL\",\"RISK\",\"OPT\",\"IG\",\"SHAP\",\"SUMMARY\",\"REPORT\",\n",
    "        \"NASDAQ\",\"NYSE\",\"LSE\",\"EURONEXT\",\"AMEX\",\"TSX\",\"ASX\",\"BSE\",\"NSE\",\"SHOULD\",\n",
    "    }\n",
    "    toks: List[str] = []\n",
    "    for m in re.finditer(r'\\b(?:NYSE|NASDAQ|LSE|EURONEXT|AMEX|TSX|ASX|BSE|NSE)[:\\s]+([A-Z.-]{1,6})\\b', t):\n",
    "        tok = m.group(1).replace('-', '.'); toks.append(tok)\n",
    "    for m in re.finditer(r'\\b[A-Z][A-Z.-]{0,5}\\b', t):\n",
    "        tok = m.group(0)\n",
    "        if tok in STOP or len(tok) <= 1: continue\n",
    "        toks.append(tok.replace('-', '.'))\n",
    "    m = re.search(r'(?:\\brun\\b|\\btickers?)\\s*[:=]\\s*([A-Z0-9,.\\s-]+)', t)\n",
    "    if m:\n",
    "        more = [s.strip().replace('-', '.') for s in re.split(r'[,\\s]+', m.group(1)) if s.strip()]\n",
    "        toks.extend(more)\n",
    "    seen, out = set(), []\n",
    "    for tok in toks:\n",
    "        if re.fullmatch(r'[A-Z.]{1,6}', tok) and tok not in seen and tok not in STOP:\n",
    "            seen.add(tok); out.append(tok)\n",
    "    return out or [\"AAPL\"]\n",
    "\n",
    "def _user_intends_decision(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(kw in t for kw in [\n",
    "        \"is it okay to trade\", \"should i trade\", \"go or no-go\", \"buy\", \"enter\", \"trade now\", \"okay to trade\"\n",
    "    ])\n",
    "\n",
    "def _parse_steps(text: str) -> Dict[str, bool]:\n",
    "    \"\"\"Turn words in the prompt into pipeline flags.\"\"\"\n",
    "    t = text.lower()\n",
    "    mentioned = any(k in t for k in [\"collect\",\"fe\",\"feature\",\"predict\",\"eval\",\"risk\",\"opt\",\"ig\",\"shap\",\"summary\",\"report\"])\n",
    "    defaults = dict(\n",
    "        run_collector=True, run_fe=True, run_predict=True, run_eval=True,\n",
    "        run_xai_ig=True, run_xai_shap=True, run_risk=True, run_opt=True,\n",
    "        run_portfolio=False, run_summarizer=True, run_llm_report=True,\n",
    "    )\n",
    "    if not mentioned: return defaults\n",
    "    flags = {k: False for k in defaults}\n",
    "    def has(*keys): return any(k in t for k in keys)\n",
    "    flags[\"run_collector\"]  = has(\"collect\",\"collector\",\"download\")\n",
    "    flags[\"run_fe\"]         = has(\"fe\",\"feature\")\n",
    "    flags[\"run_predict\"]    = has(\"predict\")\n",
    "    flags[\"run_eval\"]       = has(\"eval\",\"evaluation\")\n",
    "    flags[\"run_xai_ig\"]     = has(\" ig \",\"integrated gradients\")\n",
    "    flags[\"run_xai_shap\"]   = has(\"shap\")\n",
    "    flags[\"run_risk\"]       = has(\"risk\")\n",
    "    flags[\"run_opt\"]        = has(\"opt\",\"optimization\")\n",
    "    flags[\"run_portfolio\"]  = has(\"portfolio\")\n",
    "    flags[\"run_summarizer\"] = has(\"summary\",\"summarizer\")\n",
    "    flags[\"run_llm_report\"] = has(\"report\")\n",
    "    for k, v in defaults.items():\n",
    "        if k not in flags: flags[k] = v\n",
    "    return flags\n",
    "\n",
    "def _make_run_base(base: Optional[str] = None) -> str:\n",
    "    if base: return base\n",
    "    return pd.Timestamp.now().strftime(\"RUN_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "# ---------------- main NL node ----------------\n",
    "\n",
    "def node_parse(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Reception: parse tickers, dates (explicit/relative), seq_len and flags.\n",
    "    Falls back to sensible defaults when the user doesn't specify.\n",
    "    \"\"\"\n",
    "    txt = (state.get(\"user_request\", \"\") or \"\").strip()\n",
    "    if not txt:\n",
    "        txt = \"Analyze AAPL last 3 years; seq_len 30\"\n",
    "\n",
    "    # tickers\n",
    "    tickers = state.get(\"tickers\") or _parse_tickers(txt) or [\"AAPL\"]\n",
    "\n",
    "    # dates\n",
    "    s1, e1 = _parse_explicit_dates(txt)\n",
    "    s2, e2 = _parse_rel_window(txt)\n",
    "    start, end = _default_dates(s1 or s2, e1 or e2)\n",
    "\n",
    "    # seq_len\n",
    "    seq_len = _parse_seq_len(txt, default=int(state.get(\"default_seq_len\", 30)))\n",
    "\n",
    "    # pipeline flags\n",
    "    flags = _parse_steps(txt)\n",
    "\n",
    "    # intent\n",
    "    wants_decision = _user_intends_decision(txt)\n",
    "\n",
    "    # run folder\n",
    "    run_base = _make_run_base(state.get(\"run_base\"))\n",
    "\n",
    "    # UX text\n",
    "    pretty_tics = \", \".join(tickers)\n",
    "    daterange = f\"{start.date()} \\u2192 {end.date()}\"\n",
    "    ack = (f\"Thanks! I’ll analyze {pretty_tics} over {daterange}, then coordinate \"\n",
    "           f\"data → features → prediction → evaluation → IG & SHAP → risk → optimization, and return a report.\")\n",
    "    plan = (f\"Plan: tickers={tickers}, window={start.date()}..{end.date()}, \"\n",
    "            f\"seq_len={seq_len}, steps={[k for k, v in flags.items() if v]}, run_base='{run_base}', \"\n",
    "            f\"decision_requested={wants_decision}.\")\n",
    "\n",
    "    # Per-stage args\n",
    "    base_args = {\"run_base\": run_base, \"tickers\": tickers}\n",
    "    collector_args = {**base_args, \"start\": str(start.date()), \"end\": str(end.date())}\n",
    "    fe_args = {**base_args}\n",
    "    predict_args = {**base_args, \"seq_len\": seq_len}\n",
    "    eval_args = {**base_args}\n",
    "    risk_args = {**base_args}\n",
    "    ig_args = {**base_args, \"ig_head\": \"close\"}\n",
    "    shap_args = {**base_args, \"head\": \"close\"}\n",
    "    opt_args = {**base_args, \"mode\": \"historical\", \"head\": \"close\"}\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"status\": \"parsed\",\n",
    "        \"ack_text\": ack,\n",
    "        \"plan_text\": plan,\n",
    "        \"run_base\": run_base,\n",
    "        \"tickers\": tickers,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"date_start\": str(start.date()),\n",
    "        \"date_end\": str(end.date()),\n",
    "        \"wants_decision\": wants_decision,\n",
    "        **flags,\n",
    "        \"collector_args\": collector_args,\n",
    "        \"fe_args\": fe_args,\n",
    "        \"predict_args\": predict_args,\n",
    "        \"eval_args\": eval_args,\n",
    "        \"risk_args\": risk_args,\n",
    "        \"ig_args\": ig_args,\n",
    "        \"shap_args\": shap_args,\n",
    "        \"opt_args\": opt_args,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------- optional workflow ----------------\n",
    "\n",
    "def build_llm_reception_workflow():\n",
    "    if not HAS_LG:\n",
    "        raise RuntimeError(\"LangGraph not available. Install with: pip install langgraph\")\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"parse\", node_parse)\n",
    "    g.set_entry_point(\"parse\")\n",
    "    g.set_finish_point(\"parse\")\n",
    "    return g.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "F5hDVg6Qcawn"
   },
   "outputs": [],
   "source": [
    "**Data_collector_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg0qOBw3y_zg",
    "outputId": "61515c4d-22a7-488c-b9b8-462b7f4870d1"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/data_collector_agent.py\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import TypedDict, Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "\n",
    "class DataCollectorState(TypedDict, total=False):\n",
    "    run_base: Optional[str]\n",
    "    tickers: Optional[List[str]]\n",
    "    date_start: Optional[str]\n",
    "    date_end: Optional[str]\n",
    "    data_path: Optional[str]\n",
    "    status: Optional[str]\n",
    "    error: Optional[str]\n",
    "\n",
    "\n",
    "def _drive_base() -> str:\n",
    "\n",
    "    drive_root = \"/content/drive/MyDrive/A2A_prediction_system\"\n",
    "    local_root = \"/content/A2A_prediction_system\"\n",
    "    if os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        os.makedirs(drive_root, exist_ok=True)\n",
    "        return drive_root\n",
    "    os.makedirs(local_root, exist_ok=True)\n",
    "    return local_root\n",
    "\n",
    "\n",
    "def _compute_run_base(state: DataCollectorState) -> str:\n",
    "\n",
    "    if state.get(\"run_base\"):\n",
    "        return state[\"run_base\"]\n",
    "\n",
    "    env_rb = os.environ.get(\"RUN_BASE\")\n",
    "    if env_rb:\n",
    "        os.makedirs(env_rb, exist_ok=True)\n",
    "        return env_rb\n",
    "\n",
    "    run_id = pd.Timestamp.now().strftime(\"RUN_%Y%m%d_%H%M%S\")\n",
    "    return os.path.join(_drive_base(), run_id)\n",
    "\n",
    "\n",
    "\n",
    "def _as_date(s: Optional[str], fallback: Optional[datetime]) -> datetime:\n",
    "    if s:\n",
    "        try:\n",
    "            return pd.to_datetime(s).to_pydatetime()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fallback or datetime.today()\n",
    "\n",
    "\n",
    "def _normalize_downloaded_df(df: pd.DataFrame, ticker: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with columns: Date, Open, Close, High, Low, Volume\n",
    "    Works for:\n",
    "      - standard columns (Open, High, Low, Close, Volume)\n",
    "      - MultiIndex flattened to either Open_AAPL or AAPL_Open\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    df = df.reset_index()\n",
    "    base_cols = {\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"}\n",
    "\n",
    "\n",
    "    if not isinstance(df.columns, pd.MultiIndex):\n",
    "        if base_cols.issubset(set(df.columns)):\n",
    "            return df[[\"Date\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]].copy()\n",
    "\n",
    "        return None\n",
    "\n",
    "    # MultiIndex case: flatten to strings\n",
    "    flat_cols = []\n",
    "    for col in df.columns:\n",
    "        if isinstance(col, tuple):\n",
    "            flat_cols.append(\"_\".join([c for c in col if c]).strip())\n",
    "        else:\n",
    "            flat_cols.append(str(col))\n",
    "    df.columns = flat_cols\n",
    "\n",
    "    fields = [\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]\n",
    "    # Accept BOTH orders\n",
    "    needed_field_first = [\"Date\"] + [f\"{f}_{ticker}\" for f in fields]   # Open_AAPL\n",
    "    needed_ticker_first = [\"Date\"] + [f\"{ticker}_{f}\" for f in fields]  # AAPL_Open\n",
    "\n",
    "    if all(c in df.columns for c in needed_field_first):\n",
    "        out = df[needed_field_first].copy()\n",
    "        out.columns = [\"Date\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]\n",
    "        return out\n",
    "    if all(c in df.columns for c in needed_ticker_first):\n",
    "        tmp = df[needed_ticker_first].copy()\n",
    "        rename_map = {f\"{ticker}_Open\": \"Open\",\n",
    "                      f\"{ticker}_Close\": \"Close\",\n",
    "                      f\"{ticker}_High\": \"High\",\n",
    "                      f\"{ticker}_Low\": \"Low\",\n",
    "                      f\"{ticker}_Volume\": \"Volume\"}\n",
    "        tmp = tmp.rename(columns=rename_map)\n",
    "        return tmp[[\"Date\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]].copy()\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_5yr_stock_data(state: DataCollectorState) -> DataCollectorState:\n",
    "    print(\"[DataCollector] incoming run_base:\", state.get(\"run_base\"), \"| env RUN_BASE:\", os.environ.get(\"RUN_BASE\"))\n",
    "\n",
    "    try:\n",
    "        # Resolve config (with safe fallbacks)\n",
    "        tickers = state.get(\"tickers\") or [\"AAPL\"]\n",
    "        end_date = _as_date(state.get(\"date_end\"), datetime.today())\n",
    "        start_date = _as_date(state.get(\"date_start\"), end_date - timedelta(days=5 * 365))\n",
    "        run_base = _compute_run_base(state)\n",
    "\n",
    "        output_dir = os.path.join(run_base, \"data\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, \"yfinance_raw_data.csv\")\n",
    "\n",
    "        print(f\"\\n[DataCollector] Tickers: {tickers}\")\n",
    "        print(f\"[DataCollector] Window : {start_date.date()} → {end_date.date()}\")\n",
    "        print(f\"[DataCollector] Output : {output_path}\")\n",
    "\n",
    "        # Download per ticker\n",
    "        all_data = []\n",
    "        success_count = 0\n",
    "\n",
    "        for ticker in tickers:\n",
    "            print(f\"\\nDownloading: {ticker}\")\n",
    "            try:\n",
    "                # Use field-first layout\n",
    "                df = yf.download(\n",
    "                    tickers=ticker,\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    progress=False,\n",
    "                    group_by=\"column\",\n",
    "                    auto_adjust=False,\n",
    "                    actions=False,\n",
    "                    rounding=False,\n",
    "                    threads=True,\n",
    "                )\n",
    "\n",
    "                df_clean = _normalize_downloaded_df(df, ticker)\n",
    "                if df_clean is None or df_clean.empty:\n",
    "\n",
    "                    df2 = yf.download(\n",
    "                        tickers=ticker,\n",
    "                        start=start_date,\n",
    "                        end=end_date,\n",
    "                        progress=False,\n",
    "                        auto_adjust=False,\n",
    "                        actions=False,\n",
    "                        rounding=False,\n",
    "                        threads=True,\n",
    "                    )\n",
    "                    df_clean = _normalize_downloaded_df(df2, ticker)\n",
    "\n",
    "                if df_clean is None or df_clean.empty:\n",
    "                    print(f\"  {ticker}: Could not normalize columns.\")\n",
    "                    continue\n",
    "\n",
    "                # Clean types\n",
    "                df_clean[\"Symbol\"] = ticker\n",
    "                df_clean[\"Date\"] = pd.to_datetime(df_clean[\"Date\"], errors=\"coerce\")\n",
    "                for col in [\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]:\n",
    "                    df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "                df_clean.dropna(subset=[\"Date\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\"], inplace=True)\n",
    "\n",
    "                if df_clean.empty:\n",
    "                    print(f\"  {ticker}: No valid rows after cleaning.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"  {ticker}: {len(df_clean)} rows ready.\")\n",
    "                all_data.append(df_clean)\n",
    "                success_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error fetching {ticker}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not all_data:\n",
    "            return {**state, \"run_base\": run_base, \"status\": \"Failed\", \"error\": \"No valid stock data collected.\"}\n",
    "\n",
    "        # Concatenate & save\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        final_df.sort_values([\"Symbol\", \"Date\"], inplace=True)\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"\\n[DataCollector] Saved: {output_path}\")\n",
    "        print(f\"[DataCollector] Records: {len(final_df)} from {success_count} ticker(s)\")\n",
    "        print(f\"[DataCollector] Columns: {list(final_df.columns)}\")\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"run_base\": run_base,\n",
    "            \"data_path\": output_path,\n",
    "            \"status\": f\"Success: {success_count} ticker(s) collected\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {**state, \"status\": \"Error\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "def build_data_collector_workflow():\n",
    "    if not HAS_LG:\n",
    "        raise RuntimeError(\"LangGraph not available. Install with: pip install langgraph\")\n",
    "    graph = StateGraph(dict)\n",
    "    graph.add_node(\"FetchData\", fetch_5yr_stock_data)\n",
    "    graph.set_entry_point(\"FetchData\")\n",
    "    graph.set_finish_point(\"FetchData\")\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting 5-Year Stock Data Collection Agent...\\n\")\n",
    "    demo_state: DataCollectorState = {\n",
    "        \"tickers\": [\"AAPL\"],\n",
    "        \"date_start\": (datetime.today() - timedelta(days=5*365)).date().isoformat(),\n",
    "        \"date_end\": datetime.today().date().isoformat(),\n",
    "\n",
    "    }\n",
    "    try:\n",
    "        if HAS_LG:\n",
    "            app = build_data_collector_workflow()\n",
    "            result = app.invoke(demo_state)\n",
    "        else:\n",
    "            result = fetch_5yr_stock_data(demo_state)\n",
    "\n",
    "        print(\"\\nPipeline Complete\")\n",
    "        print(f\" Status: {result.get('status')}\")\n",
    "        if result.get(\"error\"):\n",
    "            print(f\" Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\" Data saved to: {result['data_path']}\")\n",
    "    except Exception as e:\n",
    "        print(\"Fatal error:\", e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "aZ4jkY9zcijP"
   },
   "outputs": [],
   "source": [
    "**feature_engineering_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loyUHl2I3vHm",
    "outputId": "10156e9e-5dc5-4a87-bb99-53429c126fd0"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/feature_engineering_agent.py\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Optional, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "# ---------------- Logging ----------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --------------- State type --------------\n",
    "class FeatureState(TypedDict, total=False):\n",
    "    run_base: Optional[str]\n",
    "    data_path: Optional[str]\n",
    "    fe_path: Optional[str]\n",
    "    fe_model_path: Optional[str]\n",
    "    model_features: Optional[List[str]]\n",
    "    status: Optional[str]\n",
    "    error: Optional[str]\n",
    "\n",
    "    data: Optional[pd.DataFrame]\n",
    "\n",
    "# --------------- Helpers -----------------\n",
    "def _infer_run_base(state: FeatureState) -> Path:\n",
    "\n",
    "    if state.get(\"run_base\"):\n",
    "        return Path(str(state[\"run_base\"]))\n",
    "    dp = state.get(\"data_path\")\n",
    "    if dp:\n",
    "        p = Path(dp)\n",
    "        # expect .../<run_base>/data/filename.csv\n",
    "        return p.parent.parent\n",
    "    return Path(\".\")\n",
    "\n",
    "def _atomic_write_csv(df: pd.DataFrame, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    df.to_csv(tmp, index=False)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def winsorize_series(s: pd.Series, lower_q=0.01, upper_q=0.99) -> pd.Series:\n",
    "    s = s.astype(float)\n",
    "    lo = s.quantile(lower_q)\n",
    "    hi = s.quantile(upper_q)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "# ============== Load data =============\n",
    "def load_stock_data(state: FeatureState) -> FeatureState:\n",
    "    logger.info(\"Loading stock data...\")\n",
    "    try:\n",
    "        data_path = state.get(\"data_path\")\n",
    "        if not data_path or not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Raw data file not found: {data_path}\")\n",
    "\n",
    "        df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "        logger.info(f\"Loaded: {data_path} shape={df.shape}\")\n",
    "\n",
    "        # Normalize column names\n",
    "        column_mapping = {\n",
    "            'symbol': 'Symbol', 'ticker': 'Symbol', 'SYMBOL': 'Symbol',\n",
    "            'date': 'Date', 'DATE': 'Date',\n",
    "            'open': 'Open', 'OPEN': 'Open',\n",
    "            'close': 'Close', 'CLOSE': 'Close',\n",
    "            'high': 'High', 'HIGH': 'High',\n",
    "            'low': 'Low', 'LOW': 'Low',\n",
    "            'volume': 'Volume', 'VOLUME': 'Volume',\n",
    "        }\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "        expected = [\"Date\", \"Symbol\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]\n",
    "        miss = [c for c in expected if c not in df.columns]\n",
    "        if miss:\n",
    "            raise ValueError(f\"Missing required columns: {miss}\")\n",
    "\n",
    "        df = df[expected].copy()\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "        for c in [\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # Basic validation & cleaning\n",
    "        n0 = len(df)\n",
    "        df = df.dropna(subset=[\"Date\", \"Symbol\", \"Open\", \"Close\", \"High\", \"Low\", \"Volume\"])\n",
    "        df = df[df[\"Volume\"] > 0]\n",
    "        df = df[df[\"High\"] >= df[\"Low\"]]\n",
    "        df = df[df[\"Close\"] > 0]\n",
    "        df = df.drop_duplicates([\"Symbol\", \"Date\"]).sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "        logger.info(f\"Validation: {n0} -> {len(df)} rows\")\n",
    "\n",
    "        return {**state, \"data\": df, \"status\": \"Data loaded successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return {**state, \"status\": \"Failed\", \"error\": f\"Error loading data: {e}\"}\n",
    "\n",
    "# ============== Create features =======\n",
    "def create_enhanced_features(state: FeatureState) -> FeatureState:\n",
    "    logger.info(\"Creating features...\")\n",
    "    try:\n",
    "        df = state[\"data\"]\n",
    "        syms = df[\"Symbol\"].unique()\n",
    "        logger.info(f\"Symbols: {list(syms)}\")\n",
    "\n",
    "        processed: List[pd.DataFrame] = []\n",
    "\n",
    "        for sym in syms:\n",
    "            s = df[df[\"Symbol\"] == sym].copy().sort_values(\"Date\")\n",
    "            if len(s) < 50:\n",
    "                logger.warning(f\"{sym}: too few rows ({len(s)}), skip\")\n",
    "                continue\n",
    "\n",
    "            # Returns\n",
    "            s[\"Return_1d\"] = s[\"Close\"].pct_change(1)\n",
    "\n",
    "            # MAs\n",
    "            s[\"MA_5\"]  = s[\"Close\"].rolling(5).mean()\n",
    "            s[\"MA_20\"] = s[\"Close\"].rolling(20).mean()\n",
    "            s[\"Close_MA5_Ratio\"]  = s[\"Close\"] / s[\"MA_5\"].replace(0, np.nan)\n",
    "            s[\"Close_MA20_Ratio\"] = s[\"Close\"] / s[\"MA_20\"].replace(0, np.nan)\n",
    "            s[\"MA5_MA20_Ratio\"]   = s[\"MA_5\"] / s[\"MA_20\"].replace(0, np.nan)\n",
    "            s[\"MA_5_Slope\"]  = s[\"MA_5\"].diff(5) / s[\"MA_5\"].shift(5).replace(0, np.nan)\n",
    "            s[\"MA_20_Slope\"] = s[\"MA_20\"].diff(5) / s[\"MA_20\"].shift(5).replace(0, np.nan)\n",
    "\n",
    "            # RSI\n",
    "            delta = s[\"Close\"].diff()\n",
    "            gain = delta.clip(lower=0).rolling(14, min_periods=7).mean()\n",
    "            loss = (-delta.clip(upper=0)).rolling(14, min_periods=7).mean().replace(0, np.nan)\n",
    "            rs = gain / loss\n",
    "            s[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "            # MACD (histogram)\n",
    "            exp1 = s[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "            exp2 = s[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "            macd = exp1 - exp2\n",
    "            macd_sig = macd.ewm(span=9, adjust=False).mean()\n",
    "            s[\"MACD_Histogram\"] = macd - macd_sig\n",
    "\n",
    "            # Bollinger position\n",
    "            bb_ma = s[\"Close\"].rolling(20).mean()\n",
    "            bb_sd = s[\"Close\"].rolling(20).std()\n",
    "            bb_up = bb_ma + 2 * bb_sd\n",
    "            bb_lo = bb_ma - 2 * bb_sd\n",
    "            span = (bb_up - bb_lo).replace(0, np.nan)\n",
    "            s[\"BB_Position\"] = (s[\"Close\"] - bb_lo) / span\n",
    "\n",
    "            # Ratios / ranges\n",
    "            denom_hl = (s[\"High\"] - s[\"Low\"]).replace(0, np.nan)\n",
    "            s[\"High_Low_Ratio\"] = (s[\"Close\"] - s[\"Low\"]) / denom_hl\n",
    "            s[\"Daily_Range\"] = (s[\"High\"] - s[\"Low\"]) / s[\"Close\"].replace(0, np.nan)\n",
    "            s[\"Open_Close_Ratio\"] = s[\"Open\"] / s[\"Close\"].replace(0, np.nan)\n",
    "\n",
    "            # Volatility & volume signal\n",
    "            s[\"Volatility_20d\"] = s[\"Return_1d\"].rolling(20, min_periods=10).std()\n",
    "            vol_ma10 = s[\"Volume\"].rolling(10, min_periods=5).mean()\n",
    "            s[\"Volume_Ratio\"] = s[\"Volume\"] / vol_ma10.replace(0, np.nan)\n",
    "\n",
    "            # Winsorize (cap outliers)\n",
    "            for col in [\"Return_1d\", \"Volume_Ratio\", \"Daily_Range\"]:\n",
    "                if col in s.columns:\n",
    "                    s[col] = winsorize_series(s[col])\n",
    "\n",
    "            # Targets (analysis only)\n",
    "            s[\"Next_Day_Return\"] = s[\"Return_1d\"].shift(-1)\n",
    "            s[\"Next_Day_Direction\"] = (s[\"Next_Day_Return\"] > 0).astype(int)\n",
    "\n",
    "            s = s.dropna(subset=[\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"])\n",
    "            processed.append(s)\n",
    "\n",
    "        if not processed:\n",
    "            raise ValueError(\"No symbols processed successfully\")\n",
    "\n",
    "        final_df = pd.concat(processed, ignore_index=True)\n",
    "        final_df = final_df.sort_values([\"Symbol\", \"Date\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Core features used for modeling (no targets)\n",
    "        CORE_MODEL_FEATURES = [\n",
    "            \"Open\",\"Close\",\"High\",\"Low\",\"Volume\",\n",
    "            \"Return_1d\",\"Volatility_20d\",\n",
    "            \"Close_MA5_Ratio\",\"Close_MA20_Ratio\",\"MA5_MA20_Ratio\",\n",
    "            \"MA_5_Slope\",\"MA_20_Slope\",\n",
    "            \"RSI\",\"MACD_Histogram\",\"BB_Position\",\n",
    "            \"High_Low_Ratio\",\"Daily_Range\",\"Open_Close_Ratio\",\n",
    "            \"Volume_Ratio\"\n",
    "        ]\n",
    "        available_core = [c for c in CORE_MODEL_FEATURES if c in final_df.columns]\n",
    "        logger.info(f\"Model features: {len(available_core)}/{len(CORE_MODEL_FEATURES)} available\")\n",
    "\n",
    "        return {**state, \"data\": final_df, \"model_features\": available_core,\n",
    "                \"status\": \"Features created successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in create_enhanced_features: {e}\")\n",
    "        return {**state, \"status\": \"Failed\", \"error\": f\"Error in create_features: {e}\"}\n",
    "\n",
    "# ============== Handle NaNs ===========\n",
    "def handle_missing_after_fe(state: FeatureState) -> FeatureState:\n",
    "    logger.info(\"Handling NaNs after FE...\")\n",
    "    try:\n",
    "        df = state[\"data\"].copy().sort_values([\"Symbol\",\"Date\"])\n",
    "        target_cols = {\"Next_Day_Return\", \"Next_Day_Direction\"}\n",
    "\n",
    "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        feat_cols = [c for c in num_cols if c not in target_cols]\n",
    "\n",
    "        # ffill/bfill per symbol\n",
    "        df[feat_cols] = df.groupby(\"Symbol\", group_keys=False)[feat_cols].transform(lambda g: g.ffill().bfill())\n",
    "\n",
    "        # median per symbol then global\n",
    "        per_sym_med = df.groupby(\"Symbol\", group_keys=False)[feat_cols].transform(\"median\")\n",
    "        df[feat_cols] = df[feat_cols].fillna(per_sym_med)\n",
    "        df[feat_cols] = df[feat_cols].fillna(df[feat_cols].median(numeric_only=True))\n",
    "\n",
    "        # drop rows with critical NA and tail NA\n",
    "        before = len(df)\n",
    "        df = df.dropna(subset=[\"Close\", \"Return_1d\"])\n",
    "        dropped = before - len(df)\n",
    "        if \"Next_Day_Return\" in df.columns:\n",
    "            before2 = len(df)\n",
    "            df = df.dropna(subset=[\"Next_Day_Return\"])\n",
    "            dropped += before2 - len(df)\n",
    "        logger.info(f\"Dropped rows due to NA: {dropped}\")\n",
    "\n",
    "        # report remaining NaNs\n",
    "        rem = df.isna().sum()\n",
    "        rem = rem[rem > 0]\n",
    "        if len(rem):\n",
    "            logger.warning(\"Remaining NaNs:\\n\" + \"\\n\".join([f\"  {k}: {v}\" for k,v in rem.items()]))\n",
    "        else:\n",
    "            logger.info(\"All NaNs handled.\")\n",
    "\n",
    "        return {**state, \"data\": df, \"status\": \"NaNs handled after FE\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in handle_missing_after_fe: {e}\")\n",
    "        return {**state, \"status\": \"Failed\", \"error\": f\"Error in NaN handling: {e}\"}\n",
    "\n",
    "# ============== Validate/Summarize =====\n",
    "def validate_and_summarize(state: FeatureState) -> FeatureState:\n",
    "    logger.info(\"Validating & summarizing...\")\n",
    "    try:\n",
    "        df = state[\"data\"]\n",
    "        logger.info(f\"Rows: {len(df):,}\")\n",
    "        logger.info(f\"Symbols: {df['Symbol'].nunique()}\")\n",
    "        logger.info(f\"Date range: {df['Date'].min()} → {df['Date'].max()}\")\n",
    "\n",
    "        inf_cols = [c for c in df.select_dtypes(include=[np.number]).columns if np.isinf(df[c]).any()]\n",
    "        if inf_cols:\n",
    "            logger.warning(f\"Infinite values in: {inf_cols}\")\n",
    "\n",
    "        const_cols = [c for c in df.select_dtypes(include=[np.number]).columns if df[c].nunique(dropna=True) <= 1]\n",
    "        if const_cols:\n",
    "            logger.warning(f\"Constant columns (consider drop): {const_cols}\")\n",
    "\n",
    "        return {**state, \"status\": \"Features validated and summarized\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in validate_and_summarize: {e}\")\n",
    "        return {**state, \"status\": \"Failed\", \"error\": f\"Error in validation: {e}\"}\n",
    "\n",
    "# ============== Save outputs ===========\n",
    "def save_enhanced_features(state: FeatureState) -> FeatureState:\n",
    "    logger.info(\"Saving features...\")\n",
    "    try:\n",
    "        df = state[\"data\"].copy()\n",
    "        model_feats = state.get(\"model_features\", []) or []\n",
    "\n",
    "        run_base = _infer_run_base(state)\n",
    "        out_dir  = run_base / \"FE_Agent\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Full file\n",
    "        full_path = out_dir / \"features_engineered.csv\"\n",
    "        _atomic_write_csv(df, full_path)\n",
    "        logger.info(f\"Full features: {full_path}\")\n",
    "\n",
    "        # model file\n",
    "        keep_cols = [\"Date\", \"Symbol\"] + model_feats\n",
    "        slim = df[keep_cols].copy()\n",
    "        slim_path = out_dir / \"features_for_model.csv\"\n",
    "        _atomic_write_csv(slim, slim_path)\n",
    "        logger.info(f\"Model features: {slim_path}\")\n",
    "\n",
    "        # Artifacts near full_path\n",
    "        base = full_path.with_suffix(\"\")\n",
    "        ts = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        all_feats = [c for c in df.columns if c not in [\"Date\",\"Symbol\"]]\n",
    "        (base.with_name(base.name + \"_feature_list.json\")\n",
    "         ).write_text(json.dumps({\"generated_at\": ts, \"count\": len(all_feats), \"features\": all_feats}, indent=2))\n",
    "\n",
    "        (base.with_name(base.name + \"_model_feature_list.json\")\n",
    "         ).write_text(json.dumps({\"generated_at\": ts, \"count\": len(model_feats), \"features\": model_feats}, indent=2))\n",
    "\n",
    "        schema = {c: str(t) for c, t in df.dtypes.items()}\n",
    "        (base.with_name(base.name + \"_schema.json\")\n",
    "         ).write_text(json.dumps({\"generated_at\": ts, \"schema\": schema}, indent=2))\n",
    "\n",
    "        df.describe().to_csv(base.with_name(base.name + \"_summary.csv\"), index=True)\n",
    "\n",
    "        (base.with_name(base.name + \"_VERSION.txt\")\n",
    "         ).write_text(\n",
    "            f\"features_version: 2.0\\n\"\n",
    "            f\"generated_at_utc: {ts}\\n\"\n",
    "            f\"rows: {len(df)}\\n\"\n",
    "            f\"columns_full: {len(df.columns)}\\n\"\n",
    "            f\"columns_for_model: {len(keep_cols)}\\n\"\n",
    "        )\n",
    "\n",
    "        # keep state small for next agent/orchestrator\n",
    "        out_state = {\n",
    "            **state,\n",
    "            \"fe_path\": str(full_path),\n",
    "            \"fe_model_path\": str(slim_path),\n",
    "            \"model_features\": model_feats,\n",
    "            \"status\": \"Features saved successfully\",\n",
    "        }\n",
    "        out_state.pop(\"data\", None)\n",
    "        return out_state\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving features: {e}\")\n",
    "        return {**state, \"status\": \"Failed\", \"error\": f\"Error saving features: {e}\"}\n",
    "\n",
    "# ======= Simple function for orchestrator =======\n",
    "def run_fe(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    s = FeatureState(**state)\n",
    "\n",
    "    for step in (load_stock_data, create_enhanced_features, handle_missing_after_fe,\n",
    "                 validate_and_summarize, save_enhanced_features):\n",
    "        s = step(s)\n",
    "        if str(s.get(\"status\",\"\")).startswith(\"Failed\"):\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "# ============ LangGraph ============\n",
    "def build_enhanced_feature_workflow():\n",
    "    if not HAS_LG:\n",
    "        raise RuntimeError(\"LangGraph not available. Install with: pip install langgraph\")\n",
    "    graph = StateGraph(FeatureState)\n",
    "    graph.add_node(\"load\", load_stock_data)\n",
    "    graph.add_node(\"create\", create_enhanced_features)\n",
    "    graph.add_node(\"nans\", handle_missing_after_fe)\n",
    "    graph.add_node(\"validate\", validate_and_summarize)\n",
    "    graph.add_node(\"save\", save_enhanced_features)\n",
    "\n",
    "    graph.set_entry_point(\"load\")\n",
    "    graph.add_edge(\"load\", \"create\")\n",
    "    graph.add_edge(\"create\", \"nans\")\n",
    "    graph.add_edge(\"nans\", \"validate\")\n",
    "    graph.add_edge(\"validate\", \"save\")\n",
    "    graph.set_finish_point(\"save\")\n",
    "    return graph.compile()\n",
    "\n",
    "# =============== Demo run =================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Feature Engineering Agent...\\n\")\n",
    "\n",
    "    example_state: FeatureState = {\n",
    "        \"data_path\": \"data/Yfinance_real_time_data.csv\",\n",
    "        \"run_base\": \".\",\n",
    "    }\n",
    "    out = run_fe(example_state)\n",
    "    print(\"\\nStatus:\", out.get(\"status\"))\n",
    "    if out.get(\"error\"):\n",
    "        print(\"Error:\", out[\"error\"])\n",
    "    else:\n",
    "        print(\"FE path:\", out.get(\"fe_path\"))\n",
    "        print(\"Model features path:\", out.get(\"fe_model_path\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "-T4wF_GvdIp_"
   },
   "outputs": [],
   "source": [
    "**predictive_model_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPGenq4c59es",
    "outputId": "ac85c7d5-6623-4797-e15f-36531209ac38"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/predictive_model_agent.py\n",
    "import os, json, warnings, shutil\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# LangGraph\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# ============================== Utilities ==============================\n",
    "def _infer_run_base(state: Dict[str, Any]) -> str:\n",
    "    if state.get(\"run_base\"):\n",
    "        return str(state[\"run_base\"])\n",
    "    fe_path = state.get(\"fe_path\")\n",
    "    if fe_path:\n",
    "        # path../<run_base>/FE_Agent/features_engineered.csv\n",
    "        return os.path.dirname(os.path.dirname(fe_path))\n",
    "\n",
    "    return \".\"\n",
    "\n",
    "def _atomic_write_df(df: pd.DataFrame, path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    tmp = path + \".tmp\"\n",
    "    df.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "\n",
    "# ============================== Trainer ==============================\n",
    "class TwoHeadTrainer:\n",
    "    def __init__(self, seq_len: int, run_base: str):\n",
    "        self.sequence_length = int(seq_len)\n",
    "        self.RUN_BASE = run_base\n",
    "        self.ROOT = os.path.join(run_base, \"Predictive_Model\")\n",
    "        self.DIR_MODELS = os.path.join(self.ROOT, \"lstm_models\")\n",
    "        self.DIR_PRED   = os.path.join(self.ROOT, \"predictions\")\n",
    "        self.DIR_FCAST  = os.path.join(self.ROOT, \"advanced_forecasts\")\n",
    "        self.DIR_PLOTS  = os.path.join(self.ROOT, \"evaluation_plots\")\n",
    "        self.RESULTS_CSV= os.path.join(self.ROOT, \"lstm_results.csv\")\n",
    "        for d in [self.ROOT, self.DIR_MODELS, self.DIR_PRED, self.DIR_FCAST, self.DIR_PLOTS]:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "        self.models: Dict[str, tf.keras.Model] = {}\n",
    "        self.scalers: Dict[str, Dict[str, Any]] = {}\n",
    "        self.results: Dict[str, Any] = {}\n",
    "\n",
    "    # -------------------------- helpers --------------------------\n",
    "    def _next_business_day(self, d):\n",
    "        d = pd.Timestamp(d)\n",
    "        nxt = d + pd.Timedelta(days=1)\n",
    "        while nxt.weekday() >= 5:\n",
    "            nxt += pd.Timedelta(days=1)\n",
    "        return nxt\n",
    "\n",
    "    # ----------------------- feature builder ----------------------\n",
    "    def build_features(self, df: pd.DataFrame):\n",
    "        data = df.copy()\n",
    "\n",
    "        roll50 = data[\"Close\"].rolling(50)\n",
    "        data[\"Price_Norm\"] = (data[\"Close\"] - roll50.mean()) / roll50.std()\n",
    "        data[\"High_Low_Ratio\"]  = data[\"High\"] / data[\"Low\"].replace(0, np.nan)\n",
    "        data[\"Open_Close_Ratio\"] = data[\"Open\"] / data[\"Close\"].replace(0, np.nan)\n",
    "\n",
    "        for w in [5, 10, 20]:\n",
    "            ma = data[\"Close\"].rolling(w).mean()\n",
    "            data[f\"MA_{w}\"] = ma\n",
    "            data[f\"Close_MA{w}_Ratio\"] = data[\"Close\"] / ma.replace(0, np.nan)\n",
    "            data[f\"MA{w}_Slope\"] = ma.diff(5) / ma.shift(5).replace(0, np.nan)\n",
    "\n",
    "        for p in [1, 3, 5]:\n",
    "            prev = data[\"Close\"].shift(p).replace(0, np.nan)\n",
    "            data[f\"Log_Return_{p}d\"] = np.log(data[\"Close\"] / prev)\n",
    "            data[f\"Return_Volatility_{p}d\"] = data[f\"Log_Return_{p}d\"].rolling(10).std()\n",
    "\n",
    "        delta = data[\"Close\"].diff()\n",
    "        gain = delta.clip(lower=0).rolling(14).mean()\n",
    "        loss = (-delta.clip(upper=0)).rolling(14).mean().replace(0, np.nan)\n",
    "        rs = gain / loss\n",
    "        data[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "        data[\"RSI_Norm\"] = (data[\"RSI\"] - 50) / 50\n",
    "\n",
    "        exp1 = data[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "        exp2 = data[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "        data[\"MACD\"] = exp1 - exp2\n",
    "        data[\"MACD_Signal\"] = data[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "        data[\"MACD_Histogram\"] = data[\"MACD\"] - data[\"MACD_Signal\"]\n",
    "        data[\"MACD_Norm\"] = data[\"MACD\"] / data[\"Close\"].replace(0, np.nan)\n",
    "\n",
    "        vol_ma = data[\"Volume\"].rolling(20).mean().replace(0, np.nan)\n",
    "        data[\"Volume_MA\"] = vol_ma\n",
    "        data[\"Volume_Ratio\"] = data[\"Volume\"] / vol_ma\n",
    "        data[\"Volume_Price_Correlation\"] = data[\"Volume\"].rolling(20).corr(data[\"Close\"])\n",
    "\n",
    "        data[\"Price_Volatility\"] = data[\"Log_Return_1d\"].rolling(20).std()\n",
    "        data[\"High_Low_Volatility\"] = np.log((data[\"High\"] / data[\"Low\"]).replace(0, np.nan)).rolling(10).mean()\n",
    "\n",
    "        roll_min = data[\"Close\"].rolling(20).min()\n",
    "        roll_max = data[\"Close\"].rolling(20).max()\n",
    "        den = (roll_max - roll_min).replace(0, np.nan)\n",
    "        data[\"Price_Position\"] = (data[\"Close\"] - roll_min) / den\n",
    "        data[\"Momentum_5\"] = data[\"Close\"] / data[\"Close\"].shift(5).replace(0, np.nan)\n",
    "        data[\"Momentum_10\"] = data[\"Close\"] / data[\"Close\"].shift(10).replace(0, np.nan)\n",
    "\n",
    "        feature_cols = [\n",
    "            \"Price_Norm\",\"High_Low_Ratio\",\"Open_Close_Ratio\",\n",
    "            \"Close_MA5_Ratio\",\"Close_MA10_Ratio\",\"Close_MA20_Ratio\",\n",
    "            \"MA5_Slope\",\"MA10_Slope\",\"MA20_Slope\",\n",
    "            \"Log_Return_1d\",\"Log_Return_3d\",\"Log_Return_5d\",\n",
    "            \"Return_Volatility_1d\",\"Return_Volatility_3d\",\n",
    "            \"RSI_Norm\",\"MACD_Norm\",\"MACD_Histogram\",\n",
    "            \"Volume_Ratio\",\"Volume_Price_Correlation\",\n",
    "            \"Price_Volatility\",\"High_Low_Volatility\",\n",
    "            \"Price_Position\",\"Momentum_5\",\"Momentum_10\",\n",
    "        ]\n",
    "        avail = [c for c in feature_cols if c in data.columns]\n",
    "        data[avail] = (data[avail].replace([np.inf, -np.inf], np.nan)\n",
    "                                 .fillna(method=\"ffill\")\n",
    "                                 .fillna(method=\"bfill\"))\n",
    "        return data, avail\n",
    "\n",
    "    def _prepare_sequences(self, X: np.ndarray, Y: np.ndarray, seq: int):\n",
    "        xs, ys = [], []\n",
    "        for i in range(seq, len(X)):\n",
    "            xs.append(X[i-seq:i])\n",
    "            ys.append(Y[i])\n",
    "        return np.array(xs), np.array(ys)\n",
    "\n",
    "    def _build_model(self, input_shape, *,\n",
    "                     arch=\"lstm_gru\", layers=2,\n",
    "                     units1=64, units2=32, units3=16,\n",
    "                     dropout=0.2, batch_norm=True,\n",
    "                     dense_units=16, lr=1e-3, optimizer=\"adam\"):\n",
    "        m = Sequential()\n",
    "        if arch == \"lstm_gru\":\n",
    "            m.add(LSTM(units1, return_sequences=True, input_shape=input_shape))\n",
    "            if batch_norm: m.add(BatchNormalization())\n",
    "            m.add(Dropout(dropout))\n",
    "            m.add(GRU(units2, return_sequences=(layers > 2)))\n",
    "            if batch_norm: m.add(BatchNormalization())\n",
    "            m.add(Dropout(dropout))\n",
    "            if layers >= 3:\n",
    "                m.add(GRU(units3))\n",
    "                if batch_norm: m.add(BatchNormalization())\n",
    "                m.add(Dropout(dropout))\n",
    "        else:\n",
    "            m.add(GRU(units1, return_sequences=(layers > 1), input_shape=input_shape))\n",
    "            if batch_norm: m.add(BatchNormalization())\n",
    "            m.add(Dropout(dropout))\n",
    "            if layers >= 2:\n",
    "                m.add(LSTM(units2, return_sequences=(layers > 2)))\n",
    "                if batch_norm: m.add(BatchNormalization())\n",
    "                m.add(Dropout(dropout))\n",
    "            if layers >= 3:\n",
    "                m.add(LSTM(units3))\n",
    "                if batch_norm: m.add(BatchNormalization())\n",
    "                m.add(Dropout(dropout))\n",
    "        if dense_units > 0:\n",
    "            m.add(Dense(dense_units, activation=\"relu\"))\n",
    "            m.add(Dropout(dropout))\n",
    "        m.add(Dense(2, activation=\"linear\"))\n",
    "        opt = Adam(learning_rate=lr) if optimizer == \"adam\" else RMSprop(learning_rate=lr)\n",
    "        m.compile(optimizer=opt, loss=\"mse\", metrics=[\"mae\"])\n",
    "        return m\n",
    "\n",
    "    def _tiny_search(self, Xtr, Ytr, Xva, Yva, input_shape):\n",
    "        grid = [\n",
    "            dict(arch=\"lstm_gru\", layers=2, units1=64, units2=32, dropout=0.2, batch_norm=True,  dense_units=16, lr=1e-3, optimizer=\"adam\"),\n",
    "            dict(arch=\"lstm_gru\", layers=2, units1=64, units2=32, dropout=0.3, batch_norm=True,  dense_units=16, lr=1e-3, optimizer=\"adam\"),\n",
    "            dict(arch=\"lstm_gru\", layers=3, units1=64, units2=32, dropout=0.2, batch_norm=True,  dense_units=16, lr=5e-4, optimizer=\"adam\"),\n",
    "        ]\n",
    "        best = {\"val_loss\": np.inf, \"model\": None, \"config\": None, \"history\": None}\n",
    "        for i, cfg in enumerate(grid, 1):\n",
    "            m = self._build_model(input_shape, **cfg)\n",
    "            cbs = [\n",
    "                EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6),\n",
    "            ]\n",
    "            h = m.fit(Xtr, Ytr, validation_data=(Xva, Yva),\n",
    "                      epochs=60, batch_size=min(64, max(8, len(Xtr)//4)),\n",
    "                      verbose=0, callbacks=cbs)\n",
    "            v = float(np.min(h.history[\"val_loss\"]))\n",
    "            print(f\"   tried {i}/{len(grid)} -> val_loss={v:.6f}\")\n",
    "            if v < best[\"val_loss\"]:\n",
    "                best = {\"val_loss\": v, \"model\": m, \"config\": cfg, \"history\": h}\n",
    "        print(f\"   best config: {best['config']} (val_loss={best['val_loss']:.6f})\")\n",
    "        return best\n",
    "\n",
    "    # -------------------------- train one ticker --------------------------\n",
    "    def train_model(self, ticker: str, fe_path: str):\n",
    "        print(f\"\\n Training model for {ticker}...\")\n",
    "        try:\n",
    "            allfe = pd.read_csv(fe_path)\n",
    "            tdf = allfe[allfe[\"Symbol\"] == ticker].copy().sort_values(\"Date\").reset_index(drop=True)\n",
    "            if len(tdf) < 80:\n",
    "                print(f\"   Not enough rows for {ticker}\")\n",
    "                return None\n",
    "\n",
    "            fe, feats = self.build_features(tdf)\n",
    "            prev_close = fe[\"Close\"].shift(1)\n",
    "            fe[\"Open_Ret\"]  = fe[\"Open\"]/prev_close - 1.0\n",
    "            fe[\"Close_Ret\"] = fe[\"Close\"]/prev_close - 1.0\n",
    "\n",
    "            cols_needed = feats + [\"Open_Ret\", \"Close_Ret\", \"Date\", \"Open\", \"Close\"]\n",
    "            fe = fe.dropna(subset=cols_needed).reset_index(drop=True)\n",
    "\n",
    "            dates = pd.to_datetime(fe[\"Date\"]).values\n",
    "            X = fe[feats].values\n",
    "            Y = fe[[\"Open_Ret\",\"Close_Ret\"]].values\n",
    "\n",
    "            N = len(X); trn = int(0.70*N); val = int(0.15*N)\n",
    "            Xtr, Xva, Xte = X[:trn], X[trn:trn+val], X[trn+val:]\n",
    "            Ytr, Yva, Yte = Y[:trn], Y[trn:trn+val], Y[trn+val:]\n",
    "            dates_te = dates[trn+val:]\n",
    "\n",
    "            fs = StandardScaler()\n",
    "            Xtr_s = fs.fit_transform(pd.DataFrame(Xtr, columns=feats))\n",
    "            Xva_s = fs.transform(pd.DataFrame(Xva, columns=feats))\n",
    "            Xte_s = fs.transform(pd.DataFrame(Xte, columns=feats))\n",
    "\n",
    "            ys = StandardScaler()\n",
    "            Ytr_s = ys.fit_transform(Ytr)\n",
    "            Yva_s = ys.transform(Yva)\n",
    "            Yte_s = ys.transform(Yte)\n",
    "\n",
    "            seq = self.sequence_length\n",
    "            Xtr_seq, Ytr_seq = self._prepare_sequences(Xtr_s, Ytr_s, seq)\n",
    "            Xva_seq, Yva_seq = self._prepare_sequences(Xva_s, Yva_s, seq)\n",
    "            Xte_seq, Yte_seq = self._prepare_sequences(Xte_s, Yte_s, seq)\n",
    "            print(f\"   Sequences - Train: {len(Xtr_seq)}, Val: {len(Xva_seq)}, Test: {len(Xte_seq)}\")\n",
    "\n",
    "            input_shape = (seq, len(feats))\n",
    "            best = self._tiny_search(Xtr_seq, Ytr_seq, Xva_seq, Yva_seq, input_shape)\n",
    "            model = best[\"model\"]; hist = best[\"history\"]\n",
    "\n",
    "            tr_pred = ys.inverse_transform(model.predict(Xtr_seq, verbose=0))\n",
    "            va_pred = ys.inverse_transform(model.predict(Xva_seq, verbose=0))\n",
    "            te_pred = ys.inverse_transform(model.predict(Xte_seq, verbose=0))\n",
    "\n",
    "            tr_true = Ytr[seq:]\n",
    "            va_true = Yva[seq:]\n",
    "            te_true = Yte[seq:]\n",
    "\n",
    "            def head_metrics(y_true, y_pred, head=0):\n",
    "                if len(y_true) < 2:\n",
    "                    return 0.0, float(\"nan\"), float(\"nan\")\n",
    "                r2  = r2_score(y_true[:, head], y_pred[:, head])\n",
    "                rmse= float(np.sqrt(mean_squared_error(y_true[:, head], y_pred[:, head])))\n",
    "                dir = float(np.mean((y_true[:, head] > 0) == (y_pred[:, head] > 0))*100.0)\n",
    "                return r2, rmse, dir\n",
    "\n",
    "            m_tr_open = head_metrics(tr_true, tr_pred, 0)\n",
    "            m_tr_close= head_metrics(tr_true, tr_pred, 1)\n",
    "            m_va_open = head_metrics(va_true, va_pred, 0)\n",
    "            m_va_close= head_metrics(va_true, va_pred, 1)\n",
    "            m_te_open = head_metrics(te_true, te_pred, 0)\n",
    "            m_te_close= head_metrics(te_true, te_pred, 1)\n",
    "\n",
    "            print(\"   Results (Open/Close heads):\")\n",
    "            print(f\"      Train Open : R²={m_tr_open[0]:.4f} RMSE={m_tr_open[1]:.4f} DirAcc={m_tr_open[2]:.1f}%\")\n",
    "            print(f\"      Train Close: R²={m_tr_close[0]:.4f} RMSE={m_tr_close[1]:.4f} DirAcc={m_tr_close[2]:.1f}%\")\n",
    "            print(f\"      Val   Open : R²={m_va_open[0]:.4f} RMSE={m_va_open[1]:.4f} DirAcc={m_va_open[2]:.1f}%\")\n",
    "            print(f\"      Val   Close: R²={m_va_close[0]:.4f} RMSE={m_va_close[1]:.4f} DirAcc={m_va_close[2]:.1f}%\")\n",
    "            print(f\"      Test  Open : R²={m_te_open[0]:.4f} RMSE={m_te_open[1]:.4f} DirAcc={m_te_open[2]:.1f}%\")\n",
    "            print(f\"      Test  Close: R²={m_te_close[0]:.4f} RMSE={m_te_close[1]:.4f} DirAcc={m_te_close[2]:.1f}%\")\n",
    "\n",
    "            # Align for price reconstruction\n",
    "            L = len(te_pred)\n",
    "            base_close = fe[\"Close\"].iloc[trn+val+seq-1 : trn+val+seq-1 + (L+1)].values\n",
    "            L = min(L, len(base_close)-1)\n",
    "            te_pred = te_pred[:L]; te_true = te_true[:L]\n",
    "            dates_te_adj = pd.to_datetime(dates_te[seq: seq+L])\n",
    "\n",
    "            pred_close_price = base_close[:-1] * (1.0 + te_pred[:, 1])\n",
    "            actual_close_price = base_close[1:]\n",
    "            pred_open_price = base_close[:-1] * (1.0 + te_pred[:, 0])\n",
    "            actual_open_price = fe[\"Open\"].iloc[trn+val+seq : trn+val+seq+L].values\n",
    "\n",
    "            out_pred = pd.DataFrame({\n",
    "                \"Date\": dates_te_adj.astype(\"datetime64[ns]\").astype(str),\n",
    "                \"Actual_Open\":  actual_open_price,\n",
    "                \"Pred_Open\":    pred_open_price,\n",
    "                \"Actual_Close\": actual_close_price,\n",
    "                \"Pred_Close\":   pred_close_price,\n",
    "                \"Actual_Return\":  te_true[:,1],\n",
    "                \"Pred_Return\":    te_pred[:,1]\n",
    "            })\n",
    "            _atomic_write_df(out_pred, os.path.join(self.DIR_PRED, f\"{ticker}_test_predictions.csv\"))\n",
    "\n",
    "            self.models[ticker] = model\n",
    "            self.scalers[ticker] = {\"feature_scaler\": fs, \"target_scaler\": ys, \"features\": feats}\n",
    "            self.results[ticker] = {\n",
    "                \"train_open\": m_tr_open, \"train_close\": m_tr_close,\n",
    "                \"val_open\": m_va_open, \"val_close\": m_va_close,\n",
    "                \"test_open\": m_te_open, \"test_close\": m_te_close,\n",
    "                \"best_config\": best[\"config\"], \"history\": hist.history,\n",
    "            }\n",
    "\n",
    "            # Save generated outputs\n",
    "            model.save(os.path.join(self.DIR_MODELS, f\"{ticker}_twohead.keras\"))\n",
    "            joblib.dump(fs, os.path.join(self.DIR_MODELS, f\"{ticker}_feature_scaler.pkl\"))\n",
    "            joblib.dump(ys, os.path.join(self.DIR_MODELS, f\"{ticker}_target_scaler.pkl\"))\n",
    "            with open(os.path.join(self.DIR_MODELS, f\"{ticker}_feature_names.json\"), \"w\") as f:\n",
    "                json.dump(feats, f)\n",
    "            with open(os.path.join(self.DIR_MODELS, f\"{ticker}_seq_len.txt\"), \"w\") as f:\n",
    "                f.write(str(self.sequence_length))\n",
    "            with open(os.path.join(self.DIR_MODELS, f\"{ticker}_best_config.json\"), \"w\") as f:\n",
    "                json.dump(best[\"config\"], f, indent=2)\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(12,5))\n",
    "            plt.plot(dates_te_adj, actual_close_price, label=\"Actual Close\")\n",
    "            plt.plot(dates_te_adj, pred_close_price, label=\"Pred Close\")\n",
    "            plt.title(f\"{ticker} — Test Close: Actual vs Predicted\")\n",
    "            plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.DIR_PLOTS, f\"{ticker}_test_close.png\"), dpi=220)\n",
    "            plt.close()\n",
    "\n",
    "            # Tomorrow-only prediction\n",
    "            fe[\"Date\"] = pd.to_datetime(fe[\"Date\"])\n",
    "            latest_date = fe[\"Date\"].iloc[-1]\n",
    "            latest_close = float(fe[\"Close\"].iloc[-1])\n",
    "            latest_open  = float(fe[\"Open\"].iloc[-1])\n",
    "\n",
    "            last_win = fe[feats].tail(self.sequence_length).values\n",
    "            last_win_s = fs.transform(last_win).reshape(1, self.sequence_length, len(feats))\n",
    "            tom_pred = ys.inverse_transform(model.predict(last_win_s, verbose=0))[0]\n",
    "            pred_open_ret, pred_close_ret = float(tom_pred[0]), float(tom_pred[1])\n",
    "\n",
    "            tom_pred_open  = latest_close * (1.0 + pred_open_ret)\n",
    "            tom_pred_close = latest_close * (1.0 + pred_close_ret)\n",
    "\n",
    "            for_date = self._next_business_day(latest_date)\n",
    "            tomorrow_df = pd.DataFrame([{\n",
    "                \"For_Date\": for_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"Latest_Actual_Open\":  latest_open,\n",
    "                \"Latest_Actual_Close\": latest_close,\n",
    "                \"Pred_Open_Return\":  pred_open_ret,\n",
    "                \"Pred_Close_Return\": pred_close_ret,\n",
    "                \"Pred_Open\":  tom_pred_open,\n",
    "                \"Pred_Close\": tom_pred_close\n",
    "            }])\n",
    "            _atomic_write_df(tomorrow_df, os.path.join(self.DIR_PRED, f\"{ticker}_tomorrow.csv\"))\n",
    "            print(f\"   Tomorrow-only prediction saved → {os.path.join(self.DIR_PRED, f'{ticker}_tomorrow.csv')}\")\n",
    "\n",
    "            # XAI compatibility alias\n",
    "            src = os.path.join(self.DIR_MODELS, f\"{ticker}_twohead.keras\")\n",
    "            dst = os.path.join(self.DIR_MODELS, f\"{ticker}_lstm.keras\")\n",
    "            try:\n",
    "                shutil.copyfile(src, dst)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            return self.results[ticker]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error training {ticker}: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def forecast_recursive(self, ticker: str, fe_path: str, days: int = 7):\n",
    "        if ticker not in self.models:\n",
    "            print(\"    No trained model found. Train first.\")\n",
    "            return None\n",
    "\n",
    "        allfe = pd.read_csv(fe_path)\n",
    "        df = allfe[allfe[\"Symbol\"] == ticker].copy().sort_values(\"Date\").reset_index(drop=True)\n",
    "        fe, feats = self.build_features(df)\n",
    "        fe = fe.dropna(subset=feats + [\"Date\", \"Close\"]).reset_index(drop=True)\n",
    "        fe[\"Date\"] = pd.to_datetime(fe[\"Date\"])\n",
    "        model = self.models[ticker]\n",
    "        fs = self.scalers[ticker][\"feature_scaler\"]\n",
    "        ys = self.scalers[ticker][\"target_scaler\"]\n",
    "\n",
    "        seq = fs.transform(fe[feats].tail(self.sequence_length).values)\n",
    "\n",
    "        try:\n",
    "            lr1_idx = feats.index(\"Log_Return_1d\")\n",
    "        except ValueError:\n",
    "            lr1_idx = None\n",
    "\n",
    "        current_close = float(fe[\"Close\"].iloc[-1])\n",
    "        start_date = fe[\"Date\"].iloc[-1]\n",
    "        dates, pred_open, pred_close, ret_open, ret_close = [], [], [], [], []\n",
    "\n",
    "        d = self._next_business_day(start_date)\n",
    "        for _ in range(days):\n",
    "            x = seq.reshape(1, self.sequence_length, len(feats))\n",
    "            pred = ys.inverse_transform(model.predict(x, verbose=0))[0]\n",
    "            oret, cret = float(pred[0]), float(pred[1])\n",
    "\n",
    "            next_open  = current_close * (1.0 + oret)\n",
    "            next_close = current_close * (1.0 + cret)\n",
    "\n",
    "            dates.append(d); pred_open.append(next_open); pred_close.append(next_close)\n",
    "            ret_open.append(oret); ret_close.append(cret)\n",
    "\n",
    "            new_row = seq[-1].copy()\n",
    "            if lr1_idx is not None:\n",
    "                mu = getattr(fs, \"mean_\", None); sc = getattr(fs, \"scale_\", None)\n",
    "                raw_lr1 = np.log(1.0 + cret)\n",
    "                if mu is not None and sc is not None and sc[lr1_idx] != 0:\n",
    "                    new_row[lr1_idx] = (raw_lr1 - mu[lr1_idx]) / sc[lr1_idx]\n",
    "            seq = np.vstack([seq[1:], new_row])\n",
    "            current_close = next_close\n",
    "            d = self._next_business_day(d)\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"Date\": pd.to_datetime(dates).strftime(\"%Y-%m-%d\"),\n",
    "            \"Pred_Open_Return\":  ret_open,\n",
    "            \"Pred_Close_Return\": ret_close,\n",
    "            \"Pred_Open\": pred_open,\n",
    "            \"Pred_Close\": pred_close\n",
    "        })\n",
    "        out_path = os.path.join(self.DIR_FCAST, f\"{ticker}_forecast_{days}d.csv\")\n",
    "        _atomic_write_df(out, out_path)\n",
    "        print(f\"   {days}-day forecast saved → {out_path}\")\n",
    "\n",
    "        # Plot\n",
    "        tailN = 60\n",
    "        tail_dates = fe[\"Date\"].tail(tailN).tolist()\n",
    "        tail_prices= fe[\"Close\"].tail(tailN).tolist()\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.plot(tail_dates, tail_prices, label=\"Actual Close (last 60d)\")\n",
    "        plt.plot(pd.to_datetime(dates), pred_close, label=f\"Forecast Close (+{days}d)\")\n",
    "        plt.title(f\"{ticker} — Last 60d Close + {days}-Day Forecast\")\n",
    "        plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.DIR_PLOTS, f\"{ticker}_forecast_close_{days}d.png\"), dpi=220)\n",
    "        plt.close()\n",
    "        return out\n",
    "\n",
    "    def save_results_summary(self):\n",
    "        if not self.results:\n",
    "            print(\" No results to report\")\n",
    "            return None\n",
    "        rows = []\n",
    "        for t, r in self.results.items():\n",
    "            te_o, te_c = r[\"test_open\"], r[\"test_close\"]\n",
    "            rows.append({\n",
    "                \"Ticker\": t,\n",
    "                \"Test_R2_Open\":  round(te_o[0], 4),\n",
    "                \"Test_RMSE_Open\":round(te_o[1], 4),\n",
    "                \"Test_DirAcc_Open(%)\": round(te_o[2], 1),\n",
    "                \"Test_R2_Close\":  round(te_c[0], 4),\n",
    "                \"Test_RMSE_Close\":round(te_c[1], 4),\n",
    "                \"Test_DirAcc_Close(%)\": round(te_c[2], 1),\n",
    "                \"Chosen\": \"lstm_gru\",\n",
    "            })\n",
    "        df = pd.DataFrame(rows)\n",
    "        _atomic_write_df(df, self.RESULTS_CSV)\n",
    "        print(f\" Results summary saved → {self.RESULTS_CSV}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# ============================== LangGraph nodes ==============================\n",
    "def node_init(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    run_base = _infer_run_base(state)\n",
    "    seq_len  = int(state.get(\"seq_len\", 20))\n",
    "    tickers  = state.get(\"tickers\") or [\"AAPL\"]\n",
    "    days     = int(state.get(\"forecast_days\", 7))\n",
    "    fe_path  = state.get(\"fe_path\") or os.path.join(run_base, \"FE_Agent\", \"features_engineered.csv\")\n",
    "    if not os.path.exists(fe_path):\n",
    "        raise FileNotFoundError(f\"FE file not found: {fe_path}\")\n",
    "    trainer = TwoHeadTrainer(seq_len=seq_len, run_base=run_base)\n",
    "    return {**state, \"run_base\": run_base, \"fe_path\": fe_path, \"trainer\": trainer,\n",
    "            \"tickers\": tickers, \"forecast_days\": days, \"status\": \"init_ok\"}\n",
    "\n",
    "def node_train_all(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    tr, fe_path = state[\"trainer\"], state[\"fe_path\"]\n",
    "    trained = []\n",
    "    for t in state[\"tickers\"]:\n",
    "        res = tr.train_model(t, fe_path)\n",
    "        if res is not None:\n",
    "            trained.append(t)\n",
    "    return {**state, \"trained\": trained, \"status\": \"trained\"}\n",
    "\n",
    "def node_forecast_all(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    tr, fe_path = state[\"trainer\"], state[\"fe_path\"]\n",
    "    days, tickers = state[\"forecast_days\"], state.get(\"trained\", [])\n",
    "    for t in tickers:\n",
    "        tr.forecast_recursive(t, fe_path, days=days)\n",
    "    return {**state, \"status\": \"forecast_done\"}\n",
    "\n",
    "def node_save_summary(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    tr = state[\"trainer\"]\n",
    "    df = tr.save_results_summary()\n",
    "    return {**state, \"results_csv\": getattr(tr, \"RESULTS_CSV\", None), \"status\": \"complete\"}\n",
    "\n",
    "def build_predictive_workflow():\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"init\", node_init)\n",
    "    g.add_node(\"train\", node_train_all)\n",
    "    g.add_node(\"forecast\", node_forecast_all)\n",
    "    g.add_node(\"save\", node_save_summary)\n",
    "    g.set_entry_point(\"init\")\n",
    "    g.add_edge(\"init\", \"train\")\n",
    "    g.add_edge(\"train\", \"forecast\")\n",
    "    g.add_edge(\"forecast\", \"save\")\n",
    "    g.set_finish_point(\"save\")\n",
    "    return g.compile()\n",
    "\n",
    "\n",
    "def run_predict(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    s = node_init(state)\n",
    "    s = node_train_all(s)\n",
    "    s = node_forecast_all(s)\n",
    "    s = node_save_summary(s)\n",
    "\n",
    "    tr: TwoHeadTrainer = s[\"trainer\"]\n",
    "    return {\n",
    "        **state,\n",
    "        \"run_base\": s[\"run_base\"],\n",
    "        \"predict_root\": tr.ROOT,\n",
    "        \"models_dir\": tr.DIR_MODELS,\n",
    "        \"pred_dir\": tr.DIR_PRED,\n",
    "        \"forecast_dir\": tr.DIR_FCAST,\n",
    "        \"results_csv\": tr.RESULTS_CSV,\n",
    "        \"trained\": s.get(\"trained\", []),\n",
    "        \"status\": \"predict_complete\",\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Starting Predictive Agent…\\n\")\n",
    "    app = build_predictive_workflow()\n",
    "    _ = app.invoke({\n",
    "\n",
    "        # \"fe_path\": \"<run_base>/FE_Agent/features_engineered.csv\",\n",
    "        \"seq_len\": 20,\n",
    "        \"forecast_days\": 7,\n",
    "    })\n",
    "    print(\"\\n Done. Artifacts saved under <run_base>/Predictive_Model/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "UgTF92kddQ6n"
   },
   "outputs": [],
   "source": [
    "**Model_evaluation_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyfKi4S39TES",
    "outputId": "aaea6b64-8958-4809-d8c9-d46f33f87cfa"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/model_evaluation_agent.py\n",
    "import os, json\n",
    "from typing import Dict, Any, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "\n",
    "# LangGraph\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "# ----------------- IO utils -----------------\n",
    "def _atomic_write_df(df: pd.DataFrame, path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    tmp = path + \".tmp\"\n",
    "    df.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _infer_run_base(state: Dict[str, Any]) -> str:\n",
    "    if state.get(\"run_base\"):\n",
    "        return str(state[\"run_base\"])\n",
    "    pred_dir = state.get(\"predictions_dir\")\n",
    "    if pred_dir:\n",
    "        # expected path .../<run_base>/Predictive_Model/predictions\n",
    "        return os.path.dirname(os.path.dirname(pred_dir))\n",
    "    return \".\"\n",
    "\n",
    "# ----------------- Normalizer -----------------\n",
    "def normalize_pred_file(df: pd.DataFrame, head: str = \"close\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize to columns: Date, Actual_Return, Pred_Return, Actual_Close, Pred_Close.\n",
    "    \"\"\"\n",
    "    required_single = {\"Actual_Return\", \"Pred_Return\", \"Actual_Close\", \"Pred_Close\"}\n",
    "    if required_single.issubset(df.columns):\n",
    "        return df\n",
    "\n",
    "    head = (head or \"close\").lower().strip()\n",
    "    if head not in {\"close\", \"open\"}:\n",
    "        head = \"close\"\n",
    "\n",
    "    if head == \"close\":\n",
    "        req = {\"Actual_Return\", \"Pred_Return\", \"Actual_Close\", \"Pred_Close\"}\n",
    "        # Predictive file already uses these names for close\n",
    "        mapped = {\"Actual_Close_Return\": \"Actual_Return\", \"Pred_Close_Return\": \"Pred_Return\"}\n",
    "        if req.issubset(df.columns):\n",
    "            return df\n",
    "        if set(mapped.keys()).issubset(df.columns) and {\"Actual_Close\",\"Pred_Close\"}.issubset(df.columns):\n",
    "            df2 = df.copy()\n",
    "            df2.rename(columns=mapped, inplace=True)\n",
    "            return df2\n",
    "\n",
    "    if head == \"open\":\n",
    "        req = {\"Actual_Open\", \"Pred_Open\"}\n",
    "        ret = {\"Actual_Open_Return\": \"Actual_Return\", \"Pred_Open_Return\": \"Pred_Return\"}\n",
    "        if req.issubset(df.columns) and set(ret.keys()).issubset(df.columns):\n",
    "            df2 = df.copy()\n",
    "            df2.rename(columns=ret, inplace=True)\n",
    "            # For generic naming, map open->close slots so metrics/plots reuse code\n",
    "            df2[\"Actual_Close\"] = df2[\"Actual_Open\"]\n",
    "            df2[\"Pred_Close\"]   = df2[\"Pred_Open\"]\n",
    "            return df2\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Prediction file missing required columns. \"\n",
    "        \"Expected either {Actual_Return, Pred_Return, Actual_Close, Pred_Close} \"\n",
    "        \"or open-head equivalents.\"\n",
    "    )\n",
    "\n",
    "# ----------------- Metrics  -----------------\n",
    "def _price_metrics(a_p: np.ndarray, p_p: np.ndarray) -> Dict[str, float]:\n",
    "    if len(a_p) <= 1 or len(p_p) <= 1:\n",
    "        return {\"RMSE_price\": np.nan, \"MAE_price\": np.nan, \"MAPE\": np.nan, \"R2(price)\": np.nan}\n",
    "    mse  = float(np.mean((a_p - p_p) ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae  = float(np.mean(np.abs(a_p - p_p)))\n",
    "    r2   = float(r2_score(a_p, p_p))\n",
    "    mape = float(pd.Series(a_p).pipe(lambda s: np.mean(np.abs((a_p - p_p) / np.where(s != 0, s, np.nan)))) * 100.0)\n",
    "    return {\"RMSE_price\": rmse, \"MAE_price\": mae, \"MAPE\": mape, \"R2(price)\": r2}\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def eval_metrics(df: pd.DataFrame) -> dict:\n",
    "    if df is None or df.empty:\n",
    "        return {k: (np.nan) for k in\n",
    "                [\"RMSE_price\",\"MAE_price\",\"MAPE\",\"R2(price)\",\"RMSE_ret\",\"R2(returns)\",\"DirAcc(%)\",\"Sharpe_ann\",\n",
    "                 \"Rows\",\"Test_Start\",\"Test_End\"]}\n",
    "\n",
    "    if \"Date\" in df.columns:\n",
    "        d = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "        test_start, test_end = d.min(), d.max()\n",
    "    else:\n",
    "        test_start = test_end = None\n",
    "\n",
    "    a_r = df[\"Actual_Return\"].to_numpy()\n",
    "    p_r = df[\"Pred_Return\"].to_numpy()\n",
    "    a_p = df[\"Actual_Close\"].to_numpy()\n",
    "    p_p = df[\"Pred_Close\"].to_numpy()\n",
    "\n",
    "    # price metrics\n",
    "    price = _price_metrics(a_p, p_p)\n",
    "\n",
    "    # returns metrics\n",
    "    if len(a_r) > 1:\n",
    "        rmse_ret = float(np.sqrt(mean_squared_error(a_r, p_r)))\n",
    "        r2_ret   = float(r2_score(a_r, p_r))\n",
    "        diracc   = float(np.mean(np.sign(a_r) == np.sign(p_r)) * 100.0)\n",
    "        ex_ret   = (p_r - a_r)\n",
    "        sharpe_d = float(np.mean(ex_ret) / np.std(ex_ret)) if np.std(ex_ret) > 0 else np.nan\n",
    "        sharpe_a = float(sharpe_d * np.sqrt(252)) if not np.isnan(sharpe_d) else np.nan\n",
    "    else:\n",
    "        rmse_ret = r2_ret = diracc = sharpe_a = np.nan\n",
    "\n",
    "    out = {\n",
    "        **{k: (round(v, 6) if isinstance(v, float) and not np.isnan(v) else (v if not isinstance(v, float) else np.nan))\n",
    "           for k, v in price.items()},\n",
    "        \"RMSE_ret\": round(rmse_ret, 6) if not np.isnan(rmse_ret) else np.nan,\n",
    "        \"R2(returns)\": round(r2_ret, 4) if not np.isnan(r2_ret) else np.nan,\n",
    "        \"DirAcc(%)\": round(diracc, 1) if not np.isnan(diracc) else np.nan,\n",
    "        \"Sharpe_ann\": round(sharpe_a, 3) if not np.isnan(sharpe_a) else np.nan,\n",
    "        \"Rows\": int(len(df)),\n",
    "        \"Test_Start\": \"\" if test_start is None else str(getattr(test_start, \"date\", lambda: \"\")()),\n",
    "        \"Test_End\": \"\" if test_end is None else str(getattr(test_end, \"date\", lambda: \"\")()),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# ----------------- Baselines -----------------\n",
    "def eval_baselines(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Baseline A (returns): predict zero return.\n",
    "    Baseline B (price): carry-forward yesterday's actual close.\n",
    "    \"\"\"\n",
    "    out = {\"BL_DirAcc(%)\": np.nan, \"BL_RMSE_ret\": np.nan, \"BL_RMSE_price\": np.nan, \"BL_R2(price)\": np.nan}\n",
    "    if df.empty:\n",
    "        return out\n",
    "\n",
    "    # A: returns baseline (0)\n",
    "    a_r = df[\"Actual_Return\"].to_numpy()\n",
    "    p0_r = np.zeros_like(a_r)\n",
    "    if len(a_r) > 1:\n",
    "        out[\"BL_RMSE_ret\"] = float(np.sqrt(mean_squared_error(a_r, p0_r)))\n",
    "        out[\"BL_DirAcc(%)\"] = float(np.mean(np.sign(a_r) == np.sign(p0_r)) * 100.0)\n",
    "\n",
    "    # B: price baseline (carry-forward)\n",
    "    a_p = df[\"Actual_Close\"].to_numpy()\n",
    "    if len(a_p) > 2:\n",
    "        p0_p = np.r_[a_p[0], a_p[:-1]]  # yesterday's price\n",
    "        pm = _price_metrics(a_p, p0_p)\n",
    "        out[\"BL_RMSE_price\"] = pm[\"RMSE_price\"]\n",
    "        out[\"BL_R2(price)\"]  = pm[\"R2(price)\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "# ========================= LangGraph NODES =========================\n",
    "def node_init(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      run_base: str\n",
    "      predictions_dir: str   (defaults to <run_base>/Predictive_Model/predictions)\n",
    "      eval_root: str         (defaults to <run_base>/Model_Evaluation)\n",
    "      eval_head: \"close\"|\"open\" (default \"close\")\n",
    "      strong_pct: int        (default 75)  # percentile on |Pred_Return|\n",
    "    \"\"\"\n",
    "    run_base = _infer_run_base(state)\n",
    "    predictions_dir = state.get(\"predictions_dir\") or os.path.join(run_base, \"Predictive_Model\", \"predictions\")\n",
    "    eval_root = state.get(\"eval_root\") or os.path.join(run_base, \"Model_Evaluation\")\n",
    "    eval_head = (state.get(\"eval_head\", \"close\") or \"close\").lower()\n",
    "    strong_pct = int(state.get(\"strong_pct\", 75))\n",
    "\n",
    "    if not os.path.isdir(predictions_dir):\n",
    "        raise FileNotFoundError(f\"Predictions folder not found: {predictions_dir}\")\n",
    "\n",
    "    sum_dir = os.path.join(eval_root, \"summaries\")\n",
    "    plot_dir = os.path.join(eval_root, \"plots\")\n",
    "    os.makedirs(sum_dir, exist_ok=True)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    paths = {\n",
    "        \"CSV_FULL\":    os.path.join(sum_dir,  \"model_evaluation_summary.csv\"),\n",
    "        \"JSON_FULL\":   os.path.join(sum_dir,  \"model_evaluation_summary.json\"),\n",
    "        \"CSV_STRONG\":  os.path.join(sum_dir,  \"model_evaluation_strong_signals.csv\"),\n",
    "        \"JSON_STRONG\": os.path.join(sum_dir,  \"model_evaluation_strong_signals.json\"),\n",
    "        \"DIRACC_PNG\":  os.path.join(plot_dir, \"diracc_full_vs_strong.png\"),\n",
    "        \"SCATTER_PNG\": lambda ticker: os.path.join(plot_dir, f\"{ticker}_strong_signal_pred_vs_actual.png\"),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"run_base\": run_base,\n",
    "        \"predictions_dir\": predictions_dir,\n",
    "        \"eval_root\": eval_root,\n",
    "        \"sum_dir\": sum_dir,\n",
    "        \"plot_dir\": plot_dir,\n",
    "        \"paths\": paths,\n",
    "        \"eval_head\": \"open\" if eval_head == \"open\" else \"close\",\n",
    "        \"strong_pct\": strong_pct,\n",
    "        \"status\": \"init_ok\",\n",
    "    }\n",
    "\n",
    "def node_discover_files(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    files = [fn for fn in sorted(os.listdir(state[\"predictions_dir\"])) if fn.endswith(\"_test_predictions.csv\")]\n",
    "    tickers = [fn.split(\"_\")[0] for fn in files]\n",
    "    return {**state, \"files\": files, \"tickers\": tickers, \"status\": \"files_ok\"}\n",
    "\n",
    "def node_evaluate(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    head = state[\"eval_head\"]\n",
    "    pred_dir = state[\"predictions_dir\"]\n",
    "\n",
    "    full_rows: List[dict] = []\n",
    "    strong_rows: List[dict] = []\n",
    "\n",
    "    print(f\" Model evaluation (head='{head}') — price & return metrics …\\n\")\n",
    "    for fname in state[\"files\"]:\n",
    "        ticker = fname.split(\"_\")[0]\n",
    "        path = os.path.join(pred_dir, fname)\n",
    "        try:\n",
    "            raw = pd.read_csv(path)\n",
    "            df = normalize_pred_file(raw, head=head)\n",
    "\n",
    "            need = {\"Actual_Return\", \"Pred_Return\", \"Actual_Close\", \"Pred_Close\"}\n",
    "            if df.empty or not need.issubset(df.columns):\n",
    "                print(f\"  {ticker}: normalized file missing columns — skip.\")\n",
    "                continue\n",
    "\n",
    "            # Full-sample metrics\n",
    "            fm = eval_metrics(df)\n",
    "            bl = eval_baselines(df)\n",
    "            fm.update(bl)\n",
    "            fm[\"Ticker\"] = ticker\n",
    "            fm[\"Head\"] = head\n",
    "            full_rows.append(fm)\n",
    "\n",
    "            print(f\" {ticker}: R2(price)={fm['R2(price)']} | RMSEp={fm['RMSE_price']} | DirAcc={fm['DirAcc(%)']}%  \"\n",
    "                  f\"| BL DirAcc={fm.get('BL_DirAcc(%)')}%\")\n",
    "\n",
    "            # Strong-signal subset (by |Pred_Return|)\n",
    "            try:\n",
    "                thr = df[\"Pred_Return\"].abs().quantile(state[\"strong_pct\"]/100.0)\n",
    "                strong = df.loc[df[\"Pred_Return\"].abs() >= thr].copy()\n",
    "                if not strong.empty:\n",
    "                    sm = eval_metrics(strong)\n",
    "                    sm[\"Ticker\"] = ticker\n",
    "                    sm[\"Head\"] = head\n",
    "                    sm[\"Subset\"] = f\"|Pred_Return|>={state['strong_pct']}pct\"\n",
    "                    strong_rows.append(sm)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  {ticker}: error {e}\")\n",
    "\n",
    "    return {**state, \"full_rows\": full_rows, \"strong_rows\": strong_rows, \"status\": \"evaluated\"}\n",
    "\n",
    "def node_save_tables(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    paths = state[\"paths\"]\n",
    "    full_rows = state.get(\"full_rows\", [])\n",
    "    strong_rows = state.get(\"strong_rows\", [])\n",
    "\n",
    "    df_full = pd.DataFrame(full_rows).sort_values([\"Ticker\", \"Head\"]) if full_rows else pd.DataFrame()\n",
    "    df_str  = pd.DataFrame(strong_rows).sort_values([\"Ticker\", \"Head\"]) if strong_rows else pd.DataFrame()\n",
    "\n",
    "    if not df_full.empty:\n",
    "        _atomic_write_df(df_full, paths[\"CSV_FULL\"])\n",
    "        with open(paths[\"JSON_FULL\"], \"w\") as f:\n",
    "            json.dump(full_rows, f, indent=2)\n",
    "\n",
    "    if not df_str.empty:\n",
    "        _atomic_write_df(df_str, paths[\"CSV_STRONG\"])\n",
    "        with open(paths[\"JSON_STRONG\"], \"w\") as f:\n",
    "            json.dump(strong_rows, f, indent=2)\n",
    "\n",
    "    return {**state, \"df_full\": df_full, \"df_strong\": df_str, \"status\": \"tables_saved\"}\n",
    "\n",
    "def node_make_plots(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    df_full = state.get(\"df_full\", pd.DataFrame())\n",
    "    paths = state[\"paths\"]\n",
    "    head = state[\"eval_head\"]\n",
    "    pred_dir = state[\"predictions_dir\"]\n",
    "\n",
    "    # Per-ticker plots from prediction files\n",
    "    files = [fn for fn in sorted(os.listdir(pred_dir)) if fn.endswith(\"_test_predictions.csv\")]\n",
    "    for fname in files:\n",
    "        ticker = fname.split(\"_\")[0]\n",
    "        path = os.path.join(pred_dir, fname)\n",
    "        try:\n",
    "            raw = pd.read_csv(path)\n",
    "            df = normalize_pred_file(raw, head=head)\n",
    "\n",
    "            # Sort by date\n",
    "            if \"Date\" in df.columns:\n",
    "                try:\n",
    "                    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "                    df = df.sort_values(\"Date\")\n",
    "                    x = df[\"Date\"]\n",
    "                    x_label = \"Date\"\n",
    "                except Exception:\n",
    "                    x = np.arange(len(df)); x_label = \"Index\"\n",
    "            else:\n",
    "                x = np.arange(len(df)); x_label = \"Index\"\n",
    "\n",
    "            # Line plot: Actual vs Predicted Price\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(x, df[\"Actual_Close\"], label=\"Actual Price\", linewidth=1.5)\n",
    "            plt.plot(x, df[\"Pred_Close\"],   label=\"Predicted Price\", linewidth=1.5)\n",
    "            plt.title(f\"{ticker} — {head.capitalize()} Price: Actual vs Predicted\")\n",
    "            plt.xlabel(x_label); plt.ylabel(\"Price\"); plt.legend(); plt.grid(alpha=0.3)\n",
    "            out_line = os.path.join(state[\"plot_dir\"], f\"{ticker}_{head}_price_line.png\")\n",
    "            plt.tight_layout(); plt.savefig(out_line, dpi=300); plt.close()\n",
    "\n",
    "            # Scatter: Predicted vs Actual Price\n",
    "            plt.figure(figsize=(6.5, 6))\n",
    "            plt.scatter(df[\"Actual_Close\"], df[\"Pred_Close\"], alpha=0.6)\n",
    "            lo = float(min(df[\"Actual_Close\"].min(), df[\"Pred_Close\"].min()))\n",
    "            hi = float(max(df[\"Actual_Close\"].max(), df[\"Pred_Close\"].max()))\n",
    "            plt.plot([lo, hi], [lo, hi], \"r--\", linewidth=1)\n",
    "            plt.title(f\"{ticker} — {head.capitalize()} Price: Predicted vs Actual\")\n",
    "            plt.xlabel(\"Actual Price\"); plt.ylabel(\"Predicted Price\"); plt.grid(alpha=0.3)\n",
    "            out_scatter = os.path.join(state[\"plot_dir\"], f\"{ticker}_{head}_price_scatter.png\")\n",
    "            plt.tight_layout(); plt.savefig(out_scatter, dpi=300); plt.close()\n",
    "\n",
    "            # Copy to the Predictive_Model/evaluation_plots folder\n",
    "            legacy_dir = os.path.join(state[\"run_base\"], \"Predictive_Model\", \"evaluation_plots\")\n",
    "            os.makedirs(legacy_dir, exist_ok=True)\n",
    "            shutil.copyfile(out_line,    os.path.join(legacy_dir, f\"{ticker}_{head}_price_line.png\"))\n",
    "            shutil.copyfile(out_scatter, os.path.join(legacy_dir, f\"{ticker}_{head}_price_scatter.png\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  {ticker}: plotting error: {e}\")\n",
    "\n",
    "    # Aggregate DirAcc bar\n",
    "    df_str = state.get(\"df_strong\", pd.DataFrame())\n",
    "    if not df_full.empty and not df_str.empty:\n",
    "        tickers = sorted(set(df_full[\"Ticker\"]).intersection(set(df_str[\"Ticker\"])))\n",
    "        if tickers:\n",
    "            full_da = [float(df_full[df_full[\"Ticker\"]==t][\"DirAcc(%)\"].values[0]) for t in tickers]\n",
    "            str_da  = [float(df_str[df_str[\"Ticker\"]==t][\"DirAcc(%)\"].values[0]) for t in tickers]\n",
    "            x = np.arange(len(tickers))\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.bar(x - 0.2, full_da, width=0.4, label=\"Full\")\n",
    "            plt.bar(x + 0.2, str_da,  width=0.4, label=\"Strong\")\n",
    "            plt.xticks(x, tickers)\n",
    "            plt.ylabel(\"Direction Accuracy (%)\")\n",
    "            plt.title(\"DirAcc: Full vs Strong-signal subset\")\n",
    "            plt.legend(); plt.grid(axis=\"y\", alpha=0.3)\n",
    "            plt.tight_layout(); plt.savefig(paths[\"DIRACC_PNG\"], dpi=300); plt.close()\n",
    "\n",
    "    print(\"\\nSaved plots in:\", state[\"plot_dir\"])\n",
    "    return {**state, \"status\": \"plots_saved\"}\n",
    "\n",
    "# ========================= Build + run LangGraph =========================\n",
    "def build_evaluation_workflow():\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"init\", node_init)\n",
    "    g.add_node(\"discover\", node_discover_files)\n",
    "    g.add_node(\"evaluate\", node_evaluate)\n",
    "    g.add_node(\"tables\", node_save_tables)\n",
    "    g.add_node(\"plots\", node_make_plots)\n",
    "\n",
    "    g.set_entry_point(\"init\")\n",
    "    g.add_edge(\"init\", \"discover\")\n",
    "    g.add_edge(\"discover\", \"evaluate\")\n",
    "    g.add_edge(\"evaluate\", \"tables\")\n",
    "    g.add_edge(\"tables\", \"plots\")\n",
    "    g.set_finish_point(\"plots\")\n",
    "    return g.compile()\n",
    "\n",
    "# -------- Simple function for orchestrator --------\n",
    "def run_evaluate(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    s = node_init(state)\n",
    "    s = node_discover_files(s)\n",
    "    s = node_evaluate(s)\n",
    "    s = node_save_tables(s)\n",
    "    s = node_make_plots(s)\n",
    "    return {\n",
    "        **state,\n",
    "        \"run_base\": s[\"run_base\"],\n",
    "        \"eval_root\": s[\"eval_root\"],\n",
    "        \"eval_summary_csv\": s[\"paths\"][\"CSV_FULL\"],\n",
    "        \"eval_summary_json\": s[\"paths\"][\"JSON_FULL\"],\n",
    "        \"eval_strong_csv\": s[\"paths\"][\"CSV_STRONG\"],\n",
    "        \"eval_diracc_png\": s[\"paths\"][\"DIRACC_PNG\"],\n",
    "        \"status\": \"evaluation_complete\",\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Running Evaluation Agent …\")\n",
    "    app = build_evaluation_workflow()\n",
    "    _ = app.invoke({\n",
    "        # Provide run_base or predictions_dir; defaults to <run_base>/Predictive_Model/predictions\n",
    "\n",
    "        \"eval_head\": \"close\",   # or \"open\"\n",
    "        \"strong_pct\": 75\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "tgzOQzAJdaCD"
   },
   "outputs": [],
   "source": [
    "**Explainability_agent.py - IG + SHAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQflLx0rJjsb",
    "outputId": "d098de4d-c10c-4c58-eb86-627de2e0d729"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/explainability_agent.py\n",
    "# Explainability Agent — runs IG first, then SHAP\n",
    "\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# LangGraph\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "# SHAP\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ----------------------------- path helpers -----------------------------\n",
    "def _infer_paths(state):\n",
    "    \"\"\"\n",
    "    Returns (models_dir, fe_path, explain_root, ig_root, shap_root)\n",
    "    \"\"\"\n",
    "    run_base = state.get(\"run_base\")\n",
    "    if run_base:\n",
    "        models_dir   = state.get(\"models_dir\", os.path.join(run_base, \"Predictive_Model\", \"lstm_models\"))\n",
    "        fe_path      = state.get(\"fe_path\",   os.path.join(run_base, \"FE_Agent\", \"features_engineered.csv\"))\n",
    "        explain_root = state.get(\"explain_root\", os.path.join(run_base, \"Explainability\"))\n",
    "    else:\n",
    "        models_dir   = state.get(\"models_dir\", \"Predictive_Model/lstm_models\")\n",
    "        fe_path      = state.get(\"fe_path\",   \"FE_Agent/features_engineered.csv\")\n",
    "        explain_root = state.get(\"explain_root\", \"Explainability\")\n",
    "\n",
    "    ig_root   = os.path.join(explain_root, \"IG_XAI\")\n",
    "    shap_root = os.path.join(explain_root, \"SHAP_XAI\")\n",
    "    return models_dir, fe_path, explain_root, ig_root, shap_root\n",
    "\n",
    "def _atomic_write_df(df: pd.DataFrame, path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    tmp = path + \".tmp\"\n",
    "    df.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "# ======================================================================\n",
    "# =============================== IG ===============================\n",
    "# ======================================================================\n",
    "\n",
    "def build_improved_features(ticker_df: pd.DataFrame):\n",
    "    df = ticker_df.copy()\n",
    "    roll50 = df[\"Close\"].rolling(50)\n",
    "    df[\"Price_Norm\"] = (df[\"Close\"] - roll50.mean()) / roll50.std()\n",
    "    df[\"High_Low_Ratio\"]  = df[\"High\"] / df[\"Low\"].replace(0, np.nan)\n",
    "    df[\"Open_Close_Ratio\"] = df[\"Open\"] / df[\"Close\"].replace(0, np.nan)\n",
    "    for w in [5, 10, 20]:\n",
    "        ma = df[\"Close\"].rolling(w).mean()\n",
    "        df[f\"MA_{w}\"] = ma\n",
    "        df[f\"Close_MA{w}_Ratio\"] = df[\"Close\"] / ma.replace(0, np.nan)\n",
    "        df[f\"MA{w}_Slope\"] = ma.diff(5) / ma.shift(5).replace(0, np.nan)\n",
    "    for p in [1, 3, 5]:\n",
    "        prev = df[\"Close\"].shift(p).replace(0, np.nan)\n",
    "        df[f\"Log_Return_{p}d\"] = np.log(df[\"Close\"] / prev)\n",
    "        df[f\"Return_Volatility_{p}d\"] = df[f\"Log_Return_{p}d\"].rolling(10).std()\n",
    "    delta = df[\"Close\"].diff()\n",
    "    gain = delta.clip(lower=0).rolling(14).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(14).mean().replace(0, np.nan)\n",
    "    rs = gain / loss\n",
    "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "    df[\"RSI_Norm\"] = (df[\"RSI\"] - 50) / 50\n",
    "    exp1 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = exp1 - exp2\n",
    "    df[\"MACD_Signal\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"MACD_Histogram\"] = df[\"MACD\"] - df[\"MACD_Signal\"]\n",
    "    df[\"MACD_Norm\"] = df[\"MACD\"] / df[\"Close\"].replace(0, np.nan)\n",
    "    vol_ma = df[\"Volume\"].rolling(20).mean().replace(0, np.nan)\n",
    "    df[\"Volume_MA\"] = vol_ma\n",
    "    df[\"Volume_Ratio\"] = df[\"Volume\"] / vol_ma\n",
    "    df[\"Volume_Price_Correlation\"] = df[\"Volume\"].rolling(20).corr(df[\"Close\"])\n",
    "    df[\"Price_Volatility\"] = df[\"Log_Return_1d\"].rolling(20).std()\n",
    "    df[\"High_Low_Volatility\"] = np.log((df[\"High\"] / df[\"Low\"]).replace(0, np.nan)).rolling(10).mean()\n",
    "    roll_min = df[\"Close\"].rolling(20).min()\n",
    "    roll_max = df[\"Close\"].rolling(20).max()\n",
    "    den = (roll_max - roll_min).replace(0, np.nan)\n",
    "    df[\"Price_Position\"] = (df[\"Close\"] - roll_min) / den\n",
    "    df[\"Momentum_5\"] = df[\"Close\"] / df[\"Close\"].shift(5).replace(0, np.nan)\n",
    "    df[\"Momentum_10\"] = df[\"Close\"] / df[\"Close\"].shift(10).replace(0, np.nan)\n",
    "\n",
    "    feature_columns = [\n",
    "        \"Price_Norm\",\"High_Low_Ratio\",\"Open_Close_Ratio\",\n",
    "        \"Close_MA5_Ratio\",\"Close_MA10_Ratio\",\"Close_MA20_Ratio\",\n",
    "        \"MA5_Slope\",\"MA10_Slope\",\"MA20_Slope\",\n",
    "        \"Log_Return_1d\",\"Log_Return_3d\",\"Log_Return_5d\",\n",
    "        \"Return_Volatility_1d\",\"Return_Volatility_3d\",\n",
    "        \"RSI_Norm\",\"MACD_Norm\",\"MACD_Histogram\",\n",
    "        \"Volume_Ratio\",\"Volume_Price_Correlation\",\n",
    "        \"Price_Volatility\",\"High_Low_Volatility\",\n",
    "        \"Price_Position\",\"Momentum_5\",\"Momentum_10\",\n",
    "    ]\n",
    "    avail = [c for c in feature_columns if c in df.columns]\n",
    "    df[avail] = df[avail].replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    return df, avail\n",
    "\n",
    "def prepare_sequences_ig(features: np.ndarray, seq_len: int) -> np.ndarray:\n",
    "    if len(features) <= seq_len:\n",
    "        return np.empty((0, seq_len, features.shape[1]))\n",
    "    return np.array([features[i - seq_len:i] for i in range(seq_len, len(features))])\n",
    "\n",
    "def integrated_gradients_for_head(model, inputs, head_idx: int, steps: int = 50):\n",
    "    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "    baseline = tf.zeros_like(inputs)\n",
    "    output_dim = int(model.output_shape[-1]) if hasattr(model, \"output_shape\") else 1\n",
    "\n",
    "    @tf.function\n",
    "    def f_head(x):\n",
    "        y = model(x, training=False)\n",
    "        if output_dim == 1:\n",
    "            return y\n",
    "        return y[:, head_idx:head_idx+1]\n",
    "\n",
    "    alphas = tf.reshape(tf.linspace(0.0, 1.0, steps + 1), [-1, 1, 1, 1])\n",
    "    xb = tf.expand_dims(inputs, 0)\n",
    "    x0 = tf.expand_dims(baseline, 0)\n",
    "    path_inputs = x0 + alphas * (xb - x0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(path_inputs)\n",
    "        flat = tf.reshape(path_inputs, [-1, inputs.shape[1], inputs.shape[2]])\n",
    "        preds = f_head(flat)\n",
    "        s = tf.reduce_sum(preds)\n",
    "    grads = tape.gradient(s, path_inputs)\n",
    "    avg_grads = tf.reduce_mean(grads, axis=0)\n",
    "    ig = (inputs - baseline) * avg_grads\n",
    "    return ig.numpy()\n",
    "\n",
    "def ig_completeness_residual(model, x_batch, ig_vals, head_idx=1):\n",
    "    x = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "    x0 = tf.zeros_like(x)\n",
    "    def f_head(z):\n",
    "        y = model(z, training=False)\n",
    "        return y[:, head_idx]\n",
    "    delta = f_head(x) - f_head(x0)\n",
    "    ig_sum = tf.reduce_sum(ig_vals, axis=[1,2])\n",
    "    resid = delta - ig_sum\n",
    "    return float(tf.reduce_mean(tf.abs(resid)).numpy()), resid.numpy()\n",
    "\n",
    "def ig_discover(state):\n",
    "    models_dir = state.get(\"models_dir\", \"Predictive_Model/lstm_models\")\n",
    "    data_path  = state.get(\"data_path\",  \"FE_Agent/features_engineered.csv\")\n",
    "    out_dir    = state.get(\"out_dir\",    \"IG_XAI\")\n",
    "    if not os.path.isdir(models_dir):\n",
    "        raise FileNotFoundError(f\"Models directory not found: {models_dir}\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Engineered features not found: {data_path}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"environment.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"tensorflow_version\": tf.__version__,\n",
    "            \"numpy_version\": np.__version__,\n",
    "            \"pandas_version\": pd.__version__,\n",
    "            \"matplotlib_version\": matplotlib.__version__,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    tickers, artifacts = [], {}\n",
    "    for fname in os.listdir(models_dir):\n",
    "        if fname.endswith(\"_twohead.keras\") or fname.endswith(\"_lstm.keras\"):\n",
    "            t = fname.replace(\"_twohead.keras\", \"\").replace(\"_lstm.keras\", \"\")\n",
    "            tickers.append(t)\n",
    "    tickers = sorted(set(tickers))\n",
    "    for t in tickers:\n",
    "        two = os.path.join(models_dir, f\"{t}_twohead.keras\")\n",
    "        lstm= os.path.join(models_dir, f\"{t}_lstm.keras\")\n",
    "        artifacts[t] = {\n",
    "            \"model_path\": two if os.path.exists(two) else lstm,\n",
    "            \"feature_scaler_path\": os.path.join(models_dir, f\"{t}_feature_scaler.pkl\"),\n",
    "            \"feature_names_path\":  os.path.join(models_dir, f\"{t}_feature_names.json\"),\n",
    "            \"seq_len_path\":        os.path.join(models_dir, f\"{t}_seq_len.txt\"),\n",
    "        }\n",
    "    return {**state, \"tickers\": tickers, \"artifacts\": artifacts}\n",
    "\n",
    "def ig_prepare(state):\n",
    "    df = pd.read_csv(state[\"data_path\"])\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    per_ticker = {}\n",
    "    for t in state[\"tickers\"]:\n",
    "        tdf = df[df[\"Symbol\"] == t].copy().sort_values(\"Date\").reset_index(drop=True)\n",
    "        if tdf.empty:\n",
    "            continue\n",
    "        fe_df, rebuilt_feats = build_improved_features(tdf)\n",
    "        feats_path = state[\"artifacts\"][t][\"feature_names_path\"]\n",
    "        if os.path.exists(feats_path):\n",
    "            with open(feats_path, \"r\") as f:\n",
    "                trained_feats = json.load(f)\n",
    "            feats = [c for c in trained_feats if c in fe_df.columns]\n",
    "        else:\n",
    "            feats = rebuilt_feats\n",
    "        fe_df = fe_df.dropna(subset=feats).copy()\n",
    "        per_ticker[t] = {\"full_df\": fe_df, \"feature_columns\": feats}\n",
    "    return {**state, \"per_ticker\": per_ticker}\n",
    "\n",
    "def ig_compute(state):\n",
    "    out_dir  = state.get(\"out_dir\", \"IG_XAI\")\n",
    "    steps    = int(state.get(\"ig_steps\", 50))\n",
    "    samples  = int(state.get(\"ig_samples\", 80))\n",
    "    head     = state.get(\"ig_head\", \"close\")\n",
    "    head_idx = 0 if str(head).lower() == \"open\" else 1\n",
    "\n",
    "    results = {}\n",
    "    for t, bundle in state.get(\"per_ticker\", {}).items():\n",
    "        try:\n",
    "            art = state[\"artifacts\"][t]\n",
    "            model = load_model(art[\"model_path\"])\n",
    "            scaler = joblib.load(art[\"feature_scaler_path\"])\n",
    "\n",
    "            seq_len = 20\n",
    "            if os.path.exists(art[\"seq_len_path\"]):\n",
    "                with open(art[\"seq_len_path\"], \"r\") as f:\n",
    "                    seq_len = int(f.read().strip())\n",
    "\n",
    "            features = bundle[\"feature_columns\"]\n",
    "            fe_df = bundle[\"full_df\"].copy()\n",
    "            X_all = fe_df[features].values\n",
    "            N = len(X_all)\n",
    "            if N < seq_len + 10:\n",
    "                print(f\"[IG] {t}: not enough rows, skipping\")\n",
    "                continue\n",
    "            trn = int(0.70 * N); val = int(0.15 * N)\n",
    "            X_test = X_all[trn + val:]\n",
    "            X_test_s = scaler.transform(pd.DataFrame(X_test, columns=features))\n",
    "            X_seq = prepare_sequences_ig(X_test_s, seq_len)\n",
    "            if len(X_seq) == 0:\n",
    "                print(f\"[IG] {t}: no test sequences, skipping\")\n",
    "                continue\n",
    "            K = min(samples, len(X_seq))\n",
    "            X_exp = X_seq[-K:]\n",
    "            ig_vals = integrated_gradients_for_head(model, X_exp, head_idx=head_idx, steps=steps)\n",
    "            res_mean, resid = ig_completeness_residual(model, X_exp, ig_vals, head_idx=head_idx)\n",
    "\n",
    "            mean_abs_ig = np.mean(np.abs(ig_vals), axis=(0,1))\n",
    "            imp = pd.DataFrame({\"feature\": features, \"mean_abs_ig\": mean_abs_ig}).sort_values(\"mean_abs_ig\", ascending=False)\n",
    "\n",
    "            tdir = os.path.join(out_dir, t); os.makedirs(tdir, exist_ok=True)\n",
    "            np.savetxt(os.path.join(tdir, f\"ig_completeness_residual_{head}.txt\"), resid, fmt=\"%.6e\")\n",
    "            imp.to_csv(os.path.join(tdir, f\"ig_feature_importance_{head}.csv\"), index=False)\n",
    "\n",
    "            avg_time_feat = np.mean(np.abs(ig_vals), axis=0)\n",
    "            vmax = np.percentile(avg_time_feat, 99) if np.isfinite(avg_time_feat).all() else None\n",
    "            plt.figure(figsize=(10, 4.8))\n",
    "            plt.imshow(avg_time_feat.T, aspect='auto', interpolation='nearest', vmin=0.0, vmax=(vmax if (vmax and vmax>0) else None))\n",
    "            plt.colorbar(label=\"mean |IG|\")\n",
    "            plt.yticks(ticks=np.arange(len(features)), labels=features)\n",
    "            plt.xlabel(\"Time step (old → new)\"); plt.ylabel(\"Feature\")\n",
    "            plt.title(f\"{t} — IG Heatmap ({head} head)\")\n",
    "            plt.tight_layout(); plt.savefig(os.path.join(tdir, f\"ig_heatmap_{head}.png\"), dpi=300); plt.close()\n",
    "\n",
    "            topN = min(20, len(features))\n",
    "            plt.figure(figsize=(8, max(4, int(topN*0.35))))\n",
    "            plt.barh(imp[\"feature\"].head(topN)[::-1], imp[\"mean_abs_ig\"].head(topN)[::-1])\n",
    "            plt.title(f\"{t} — Global Feature Importance (mean |IG|, {head} head)\")\n",
    "            plt.tight_layout(); plt.savefig(os.path.join(tdir, f\"ig_global_importance_{head}.png\"), dpi=300); plt.close()\n",
    "\n",
    "            results[t] = {\n",
    "                \"head\": head, \"seq_len\": int(seq_len), \"rows_used\": int(K),\n",
    "                \"ig_steps\": int(steps), \"ig_completeness_mean_abs_residual\": float(res_mean),\n",
    "                \"top_features\": imp.head(10).to_dict(orient=\"records\"),\n",
    "                \"outputs_dir\": tdir\n",
    "            }\n",
    "            print(f\"[IG] {t}: saved → {tdir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[IG] {t}: failed — {e}\")\n",
    "\n",
    "\n",
    "    summary_path = os.path.join(out_dir, \"ig_xai_summary.json\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump({\"historical\": results, \"forecast\": {}}, f, indent=2)\n",
    "    print(f\" Saved IG summary → {summary_path}\")\n",
    "    return {**state, \"ig_results\": results}\n",
    "\n",
    "# ======================================================================\n",
    "# ============================== SHAP ==============================\n",
    "# ======================================================================\n",
    "\n",
    "def build_features_like_trainer(ticker_df: pd.DataFrame):\n",
    "    df = ticker_df.copy()\n",
    "    roll50 = df[\"Close\"].rolling(50)\n",
    "    df[\"Price_Norm\"] = (df[\"Close\"] - roll50.mean()) / roll50.std()\n",
    "    df[\"High_Low_Ratio\"] = df[\"High\"] / df[\"Low\"].replace(0, np.nan)\n",
    "    df[\"Open_Close_Ratio\"] = df[\"Open\"] / df[\"Close\"].replace(0, np.nan)\n",
    "    for w in [5, 10, 20]:\n",
    "        ma = df[\"Close\"].rolling(w).mean()\n",
    "        df[f\"MA_{w}\"] = ma\n",
    "        df[f\"Close_MA{w}_Ratio\"] = df[\"Close\"] / ma.replace(0, np.nan)\n",
    "        df[f\"MA{w}_Slope\"] = ma.diff(5) / ma.shift(5).replace(0, np.nan)\n",
    "    for p in [1, 3, 5]:\n",
    "        prev = df[\"Close\"].shift(p).replace(0, np.nan)\n",
    "        df[f\"Log_Return_{p}d\"] = np.log(df[\"Close\"] / prev)\n",
    "        df[f\"Return_Volatility_{p}d\"] = df[f\"Log_Return_{p}d\"].rolling(10).std()\n",
    "    delta = df[\"Close\"].diff()\n",
    "    gain = delta.clip(lower=0).rolling(14).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(14).mean().replace(0, np.nan)\n",
    "    rs = gain / loss\n",
    "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "    df[\"RSI_Norm\"] = (df[\"RSI\"] - 50) / 50\n",
    "    exp1 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = exp1 - exp2\n",
    "    df[\"MACD_Signal\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"MACD_Histogram\"] = df[\"MACD\"] - df[\"MACD_Signal\"]\n",
    "    df[\"MACD_Norm\"] = df[\"MACD\"] / df[\"Close\"].replace(0, np.nan)\n",
    "    vol_ma = df[\"Volume\"].rolling(20).mean().replace(0, np.nan)\n",
    "    df[\"Volume_MA\"] = vol_ma\n",
    "    df[\"Volume_Ratio\"] = df[\"Volume\"] / vol_ma\n",
    "    df[\"Volume_Price_Correlation\"] = df[\"Volume\"].rolling(20).corr(df[\"Close\"])\n",
    "    df[\"Price_Volatility\"] = df[\"Log_Return_1d\"].rolling(20).std()\n",
    "    df[\"High_Low_Volatility\"] = np.log((df[\"High\"] / df[\"Low\"]).replace(0, np.nan)).rolling(10).mean()\n",
    "    roll_min = df[\"Close\"].rolling(20).min()\n",
    "    roll_max = df[\"Close\"].rolling(20).max()\n",
    "    den = (roll_max - roll_min).replace(0, np.nan)\n",
    "    df[\"Price_Position\"] = (df[\"Close\"] - roll_min) / den\n",
    "    df[\"Momentum_5\"] = df[\"Close\"] / df[\"Close\"].shift(5).replace(0, np.nan)\n",
    "    df[\"Momentum_10\"] = df[\"Close\"] / df[\"Close\"].shift(10).replace(0, np.nan)\n",
    "\n",
    "    feature_columns = [\n",
    "        \"Price_Norm\", \"High_Low_Ratio\", \"Open_Close_Ratio\",\n",
    "        \"Close_MA5_Ratio\", \"Close_MA10_Ratio\", \"Close_MA20_Ratio\",\n",
    "        \"MA5_Slope\", \"MA10_Slope\", \"MA20_Slope\",\n",
    "        \"Log_Return_1d\", \"Log_Return_3d\", \"Log_Return_5d\",\n",
    "        \"Return_Volatility_1d\", \"Return_Volatility_3d\",\n",
    "        \"RSI_Norm\", \"MACD_Norm\", \"MACD_Histogram\",\n",
    "        \"Volume_Ratio\", \"Volume_Price_Correlation\",\n",
    "        \"Price_Volatility\", \"High_Low_Volatility\",\n",
    "        \"Price_Position\", \"Momentum_5\", \"Momentum_10\",\n",
    "    ]\n",
    "    avail = [c for c in feature_columns if c in df.columns]\n",
    "    df[avail] = df[avail].replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    return df, avail\n",
    "\n",
    "def prepare_sequences_shap(features: np.ndarray, seq_len: int) -> np.ndarray:\n",
    "    return np.array([features[i - seq_len:i] for i in range(seq_len, len(features))])\n",
    "\n",
    "def shap_discover(state):\n",
    "    models_dir = state.get(\"models_dir\", \"Predictive_Model/lstm_models\")\n",
    "    if not os.path.exists(models_dir):\n",
    "        print(f\" Models dir not found: {models_dir}\")\n",
    "        return {**state, \"tickers\": [], \"artifacts\": {}}\n",
    "\n",
    "    tickers, artifacts = [], {}\n",
    "    for fname in os.listdir(models_dir):\n",
    "        if fname.endswith(\"_twohead.keras\"):\n",
    "            tickers.append(fname.split(\"_twohead.keras\")[0])\n",
    "        elif fname.endswith(\"_lstm.keras\"):\n",
    "            tickers.append(fname.split(\"_lstm.keras\")[0])\n",
    "    tickers = sorted(set(tickers))\n",
    "    for t in tickers:\n",
    "        model_path = os.path.join(models_dir, f\"{t}_twohead.keras\")\n",
    "        if not os.path.exists(model_path):\n",
    "            model_path = os.path.join(models_dir, f\"{t}_lstm.keras\")\n",
    "        artifacts[t] = {\n",
    "            \"model_path\": model_path,\n",
    "            \"feature_scaler_path\": os.path.join(models_dir, f\"{t}_feature_scaler.pkl\"),\n",
    "            \"feature_names_path\":   os.path.join(models_dir, f\"{t}_feature_names.json\"),\n",
    "            \"seq_len_path\":         os.path.join(models_dir, f\"{t}_seq_len.txt\"),\n",
    "        }\n",
    "    return {**state, \"tickers\": tickers, \"artifacts\": artifacts}\n",
    "\n",
    "def shap_prepare(state):\n",
    "    data_path = state.get(\"data_path\", \"FE_Agent/features_engineered.csv\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\" Engineered data not found: {data_path}\")\n",
    "        return {**state, \"per_ticker\": {}}\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    per_ticker = {}\n",
    "    for t in state.get(\"tickers\", []):\n",
    "        tdf = df[df[\"Symbol\"] == t].copy().sort_values(\"Date\").reset_index(drop=True)\n",
    "        if len(tdf) < 200:\n",
    "            print(f\"   {t}: too few rows; skipping\")\n",
    "            continue\n",
    "        fe_df, built_features = build_features_like_trainer(tdf)\n",
    "        art = state[\"artifacts\"][t]\n",
    "        features = built_features\n",
    "        fpath = art[\"feature_names_path\"]\n",
    "        if os.path.exists(fpath):\n",
    "            try:\n",
    "                with open(fpath, \"r\") as f:\n",
    "                    saved_feats = json.load(f)\n",
    "                features = [c for c in saved_feats if c in fe_df.columns]\n",
    "            except Exception:\n",
    "                pass\n",
    "        fe_df = fe_df.dropna(subset=features + [\"Date\"]).copy()\n",
    "        per_ticker[t] = {\"full_df\": fe_df, \"feature_columns\": features}\n",
    "    return {**state, \"per_ticker\": per_ticker}\n",
    "\n",
    "def shap_compute(state):\n",
    "    if not HAS_SHAP:\n",
    "        print(\"[SHAP] shap not installed; skipping SHAP\")\n",
    "        return {**state, \"results\": {}}\n",
    "\n",
    "    out_dir = state.get(\"out_dir\", \"SHAP_XAI\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    head_name = str(state.get(\"head\", \"close\")).lower()\n",
    "    head_idx = 0 if head_name == \"open\" else 1\n",
    "    k_last = int(state.get(\"k_last\", 120))\n",
    "    bg_cap = int(state.get(\"bg_cap\", 100))\n",
    "\n",
    "    results = {}\n",
    "    for t, bundle in state.get(\"per_ticker\", {}).items():\n",
    "        try:\n",
    "            art = state[\"artifacts\"][t]\n",
    "            model = load_model(art[\"model_path\"])\n",
    "            feat_scaler = joblib.load(art[\"feature_scaler_path\"])\n",
    "\n",
    "            seq_len = 20\n",
    "            if os.path.exists(art[\"seq_len_path\"]):\n",
    "                try:\n",
    "                    with open(art[\"seq_len_path\"], \"r\") as f:\n",
    "                        seq_len = int(f.read().strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            features = bundle[\"feature_columns\"]\n",
    "            fe_df = bundle[\"full_df\"].copy()\n",
    "            X_all = fe_df[features].values\n",
    "            dates_all = fe_df[\"Date\"].values\n",
    "\n",
    "            total = len(X_all)\n",
    "            trn = int(total * 0.70)\n",
    "            val = int(total * 0.15)\n",
    "            X_train = X_all[:trn]\n",
    "            X_val   = X_all[trn: trn + val]\n",
    "            X_test  = X_all[trn + val:]\n",
    "            dates_test = dates_all[trn + val:]\n",
    "\n",
    "            X_train_s = feat_scaler.transform(pd.DataFrame(X_train, columns=features))\n",
    "            X_val_s   = feat_scaler.transform(pd.DataFrame(X_val,   columns=features))\n",
    "            X_test_s  = feat_scaler.transform(pd.DataFrame(X_test,  columns=features))\n",
    "\n",
    "            def _seq(x): return prepare_sequences_shap(x, seq_len)\n",
    "            X_train_seq = _seq(X_train_s)\n",
    "            X_val_seq   = _seq(X_val_s)\n",
    "            X_test_seq  = _seq(X_test_s)\n",
    "            if len(X_test_seq) < 5 or len(X_train_seq) < 20:\n",
    "                print(f\"    {t}: not enough sequences; skipping\")\n",
    "                continue\n",
    "\n",
    "            B = min(bg_cap, len(X_train_seq))\n",
    "            bg_idx = np.random.RandomState(42).choice(len(X_train_seq), size=B, replace=False)\n",
    "            background = X_train_seq[bg_idx]\n",
    "\n",
    "            k = min(k_last, len(X_test_seq))\n",
    "            X_shap = X_test_seq[-k:]\n",
    "\n",
    "            shap_vals = None\n",
    "            try:\n",
    "                explainer = shap.DeepExplainer(model, background)\n",
    "                sv = explainer.shap_values(X_shap)\n",
    "                shap_vals = sv[head_idx] if isinstance(sv, list) else sv\n",
    "            except Exception as e:\n",
    "                print(f\"    {t}: DeepExplainer failed ({e}). Trying GradientExplainer…\")\n",
    "                try:\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "                    sv = explainer.shap_values(X_shap)\n",
    "                    shap_vals = sv[head_idx] if isinstance(sv, list) else sv\n",
    "                except Exception as e2:\n",
    "                    print(f\"    {t}: GradientExplainer failed ({e2}). Using KernelExplainer (slow)…\")\n",
    "                    X_bg_flat   = background.reshape((background.shape[0], -1))\n",
    "                    X_shap_flat = X_shap.reshape((X_shap.shape[0], -1))\n",
    "                    def f(z):\n",
    "                        z3 = z.reshape((-1, seq_len, len(features)))\n",
    "                        preds = model.predict(z3, verbose=0)\n",
    "                        if preds.shape[-1] == 2:\n",
    "                            return preds[:, head_idx]\n",
    "                        return preds.reshape((-1,))\n",
    "                    explainer = shap.KernelExplainer(f, X_bg_flat[:min(50, len(X_bg_flat))])\n",
    "                    sv = explainer.shap_values(X_shap_flat, nsamples=200)\n",
    "                    shap_vals = np.array(sv).reshape((X_shap.shape[0], seq_len, len(features)))\n",
    "\n",
    "            try:\n",
    "                exp_val_raw = explainer.expected_value\n",
    "                base_value = float(exp_val_raw[head_idx]) if isinstance(exp_val_raw, (list, tuple, np.ndarray)) else float(exp_val_raw)\n",
    "            except Exception:\n",
    "                preds_bg = model.predict(background, verbose=0)\n",
    "                base_value = float(np.mean(preds_bg[:, head_idx])) if preds_bg.ndim == 2 else float(np.mean(preds_bg))\n",
    "\n",
    "            mean_abs_shap = np.mean(np.abs(shap_vals), axis=(0, 1))\n",
    "            feat_importance = pd.DataFrame({\"feature\": features, \"mean_abs_shap\": mean_abs_shap}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "            ticker_dir = os.path.join(out_dir, t); os.makedirs(ticker_dir, exist_ok=True)\n",
    "            suffix = head_name.lower()\n",
    "\n",
    "            # SHAP files:\n",
    "            feat_importance.to_csv(os.path.join(ticker_dir, f\"{t}_feature_importance_{suffix}.csv\"), index=False)\n",
    "\n",
    "            topN = min(20, len(features))\n",
    "            plt.figure(figsize=(8, max(4, int(topN * 0.35))))\n",
    "            plt.barh(feat_importance[\"feature\"].head(topN)[::-1],\n",
    "                     feat_importance[\"mean_abs_shap\"].head(topN)[::-1])\n",
    "            plt.title(f\"{t} — Global Feature Importance (mean |SHAP|, head='{suffix}')\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(ticker_dir, f\"{t}_global_importance_{suffix}.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            heat = np.mean(np.abs(shap_vals), axis=0)\n",
    "            vmax = np.percentile(heat, 99) if np.isfinite(heat).all() else None\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.imshow(heat.T, aspect='auto', interpolation='nearest',\n",
    "                       vmin=0.0, vmax=vmax if (vmax is not None and vmax > 0) else None)\n",
    "            plt.colorbar(label=\"mean |SHAP|\")\n",
    "            plt.yticks(ticks=np.arange(len(features)), labels=features)\n",
    "            plt.xlabel(\"Time step (old → new)\"); plt.ylabel(\"Feature\")\n",
    "            plt.title(f\"{t} — SHAP Heatmap (head='{suffix}')\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(ticker_dir, f\"{t}_heatmap_{suffix}.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            # plots\n",
    "            X_last_step = X_shap[:, -1, :]\n",
    "            shap_last_step = shap_vals[:, -1, :]\n",
    "\n",
    "            try:\n",
    "                shap.summary_plot(shap_last_step, X_last_step,\n",
    "                                  feature_names=features, show=False,\n",
    "                                  plot_type=\"dot\", max_display=20)\n",
    "                plt.title(f\"{t} — SHAP Summary (last step, head='{suffix}')\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(ticker_dir, f\"{t}_summary_beeswarm_{suffix}.png\"),\n",
    "                            dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"    {t}: summary_plot failed ({e}) — continuing.\")\n",
    "\n",
    "            try:\n",
    "                idx = -1\n",
    "                fv = X_last_step[idx]\n",
    "                sv = shap_last_step[idx]\n",
    "                order = np.argsort(-np.abs(sv))\n",
    "                vals = shap.Explanation(\n",
    "                    values=sv[order],\n",
    "                    base_values=np.array([base_value]),\n",
    "                    data=fv[order],\n",
    "                    feature_names=[features[i] for i in order]\n",
    "                )\n",
    "                shap.plots.waterfall(vals, max_display=20, show=False)\n",
    "                plt.title(f\"{t} — Local Waterfall (last sample, head='{suffix}')\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(ticker_dir, f\"{t}_local_waterfall_last_{suffix}.png\"),\n",
    "                            dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"    {t}: local waterfall failed ({e}) — continuing.\")\n",
    "\n",
    "            try:\n",
    "                force = shap.force_plot(base_value,\n",
    "                                        shap_last_step[-1],\n",
    "                                        X_last_step[-1],\n",
    "                                        feature_names=features,\n",
    "                                        matplotlib=False)\n",
    "                shap.save_html(os.path.join(ticker_dir, f\"{t}_force_last_{suffix}.html\"), force)\n",
    "            except Exception as e:\n",
    "                print(f\"    {t}: force_plot (HTML) failed ({e})\")\n",
    "            try:\n",
    "                plt.figure(figsize=(10, 2.8))\n",
    "                shap.force_plot(base_value,\n",
    "                                shap_last_step[-1],\n",
    "                                X_last_step[-1],\n",
    "                                feature_names=features,\n",
    "                                matplotlib=True,\n",
    "                                show=False)\n",
    "                plt.title(f\"{t} — SHAP Force (last sample, head='{suffix}')\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(ticker_dir, f\"{t}_force_last_{suffix}.png\"),\n",
    "                            dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"    {t}: force_plot (PNG) failed ({e})\")\n",
    "\n",
    "            try:\n",
    "                n_overlay = min(50, shap_last_step.shape[0])\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                shap.decision_plot(base_value,\n",
    "                                   shap_last_step[-n_overlay:],\n",
    "                                   feature_names=features,\n",
    "                                   show=False)\n",
    "                plt.title(f\"{t} — SHAP Decision Plot (last {n_overlay} samples, head='{suffix}')\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(ticker_dir, f\"{t}_decision_overlay_{suffix}.png\"),\n",
    "                            dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"    {t}: decision_plot (overlay) failed ({e})\")\n",
    "            try:\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                shap.decision_plot(base_value,\n",
    "                                   shap_last_step[-1],\n",
    "                                   feature_names=features,\n",
    "                                   show=False)\n",
    "                plt.title(f\"{t} — SHAP Decision Plot (last sample, head='{suffix}')\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(ticker_dir, f\"{t}_decision_last_{suffix}.png\"),\n",
    "                            dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"    {t}: decision_plot (single) failed ({e})\")\n",
    "\n",
    "            results[t] = {\n",
    "                \"head\": head_name, \"rows_used\": int(len(X_shap)),\n",
    "                \"seq_len\": int(seq_len), \"features\": features,\n",
    "                \"top_features\": feat_importance.head(10).to_dict(orient=\"records\"),\n",
    "                \"outputs_dir\": ticker_dir\n",
    "            }\n",
    "            print(f\"    {t}: SHAP completed (head='{suffix}')\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   {t}: SHAP failed — {e}\")\n",
    "\n",
    "    # summary file\n",
    "    summary_path = os.path.join(out_dir, \"shap_xai_summary.json\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\" Saved summary → {summary_path}\")\n",
    "    return {**state, \"results\": results}\n",
    "\n",
    "# ======================================================================\n",
    "# =========================== COMBINED RUNNER ===========================\n",
    "# ======================================================================\n",
    "\n",
    "def run_explainability(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Runs IG first (saving under Explainability/IG_XAI), then SHAP\n",
    "    (Explainability/SHAP_XAI). Artifacts & filenames saved under this folder.\n",
    "    \"\"\"\n",
    "    models_dir, fe_path, explain_root, ig_root, shap_root = _infer_paths(state)\n",
    "    os.makedirs(explain_root, exist_ok=True)\n",
    "\n",
    "    # -------- IG phase --------\n",
    "    ig_state = {\n",
    "        \"models_dir\": models_dir,\n",
    "        \"data_path\":  fe_path,\n",
    "        \"out_dir\":    ig_root,\n",
    "        \"ig_steps\":   int(state.get(\"ig_steps\", 50)),\n",
    "        \"ig_samples\": int(state.get(\"ig_samples\", 80)),\n",
    "        \"ig_head\":    state.get(\"ig_head\", \"close\"),\n",
    "    }\n",
    "    ig_state = ig_discover(ig_state)\n",
    "    ig_state = ig_prepare(ig_state)\n",
    "    ig_state = ig_compute(ig_state)\n",
    "\n",
    "    # -------- SHAP phase --------\n",
    "    shap_state = {\n",
    "        \"models_dir\": models_dir,\n",
    "        \"data_path\":  fe_path,\n",
    "        \"out_dir\":    shap_root,\n",
    "        \"head\":       state.get(\"shap_head\", state.get(\"ig_head\", \"close\")),\n",
    "        \"k_last\":     int(state.get(\"k_last\", 120)),\n",
    "        \"bg_cap\":     int(state.get(\"bg_cap\", 100)),\n",
    "    }\n",
    "    shap_state = shap_discover(shap_state)\n",
    "    shap_state = shap_prepare(shap_state)\n",
    "    shap_state = shap_compute(shap_state)\n",
    "\n",
    "    # Combined summary\n",
    "    summary_path = os.path.join(explain_root, \"explainability_summary.json\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"IG\": ig_state.get(\"ig_results\", {}),\n",
    "            \"SHAP\": shap_state.get(\"results\", {}),\n",
    "            \"roots\": {\"IG_XAI\": ig_root, \"SHAP_XAI\": shap_root}\n",
    "        }, f, indent=2)\n",
    "    print(f\"[Explainability] Combined summary → {summary_path}\")\n",
    "\n",
    "    return {\n",
    "        \"explain_root\": explain_root,\n",
    "        \"ig_root\": ig_root,\n",
    "        \"shap_root\": shap_root,\n",
    "        \"ig_summary\": os.path.join(ig_root, \"ig_xai_summary.json\"),\n",
    "        \"shap_summary\": os.path.join(shap_root, \"shap_xai_summary.json\"),\n",
    "        \"combined_summary\": summary_path,\n",
    "    }\n",
    "\n",
    "# LangGraph wrapper (runs IG → SHAP)\n",
    "def build_explainability_workflow():\n",
    "    if not HAS_LG:\n",
    "        raise RuntimeError(\"LangGraph not available. Install with: pip install langgraph\")\n",
    "\n",
    "    def _runner(state):\n",
    "        return {**state, **run_explainability(state)}\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"run\", _runner)\n",
    "    g.set_entry_point(\"run\")\n",
    "    g.set_finish_point(\"run\")\n",
    "    return g.compile()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Running Explainability Agent (IG → SHAP)…\")\n",
    "    _ = run_explainability({\n",
    "        # \"run_base\": \"/content/drive/MyDrive/A2A_prediction_system/RUN_YYYYMMDD_HHMMSS\",\n",
    "        \"models_dir\": \"Predictive_Model/lstm_models\",\n",
    "        \"fe_path\":    \"FE_Agent/features_engineered.csv\",\n",
    "        \"ig_head\": \"close\", \"ig_steps\": 50, \"ig_samples\": 80,\n",
    "        \"shap_head\": \"close\", \"k_last\": 120, \"bg_cap\": 100,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "6FSwnYoGdnGS"
   },
   "outputs": [],
   "source": [
    "**risk_assessment.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKe_AEosSY2i",
    "outputId": "88d44ab0-0724-4969-d711-54d3d4718e53"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/risk_assessment_agent.py\n",
    "\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LANGGRAPH = True\n",
    "except Exception:\n",
    "    HAS_LANGGRAPH = False\n",
    "\n",
    "# ---------- Defaults ----------\n",
    "FE_PATH_DEFAULT           = \"FE_Agent/features_engineered.csv\"\n",
    "PRED_DIR_DEFAULT          = \"Predictive_Model/predictions\"\n",
    "FCAST_DIR_DEFAULT         = \"Predictive_Model/advanced_forecasts\"\n",
    "SHAP_DIR_DEFAULT          = \"SHAP_XAI\"\n",
    "OUT_DIR_DEFAULT           = \"Risk_Assessment\"\n",
    "\n",
    "ALPHA_DEFAULT             = 0.10         # 90% PIs\n",
    "CALIB_FRAC_DEFAULT        = 0.70         # residuals split: first 70% for calibration\n",
    "CALIB_MIN_DEFAULT         = 60           # min calibration points\n",
    "ROLL_COVER_WIN_DEFAULT    = 60\n",
    "BOOTSTRAP_PATHS_DEFAULT   = 200\n",
    "VOL_LOOKBACK_DEFAULT      = 20\n",
    "ADAPT_WIDTH_WIN           = 252          # derive \"wide band\" threshold from last 1y widths\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# ---------- run_base helper ----------\n",
    "def _under_run_base(state: Dict[str, Any], rel_or_abs: str) -> str:\n",
    "    \"\"\"If `run_base` is present, resolve paths under it; otherwise use as-is.\"\"\"\n",
    "    if not rel_or_abs:\n",
    "        return rel_or_abs\n",
    "    rb = state.get(\"run_base\")\n",
    "    if rb and not os.path.isabs(rel_or_abs):\n",
    "        return os.path.join(rb, rel_or_abs)\n",
    "    return rel_or_abs\n",
    "\n",
    "# ---------- Basic helpers ----------\n",
    "def pct_returns(series: pd.Series) -> pd.Series:\n",
    "    return series.pct_change().dropna()\n",
    "\n",
    "def rolling_vol(returns: pd.Series, window: int = VOL_LOOKBACK_DEFAULT) -> pd.Series:\n",
    "    return returns.rolling(window).std()\n",
    "\n",
    "def drawdown(close: pd.Series) -> pd.Series:\n",
    "    peak = close.cummax()\n",
    "    return (close - peak) / peak\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series, annualization: int = 252) -> float:\n",
    "    r = returns.dropna()\n",
    "    if len(r) < 2 or r.std() == 0: return float(\"nan\")\n",
    "    return float(np.sqrt(annualization) * r.mean() / r.std())\n",
    "\n",
    "def var_percentile(returns: pd.Series, q: float = 0.05) -> float:\n",
    "    r = returns.dropna()\n",
    "    if len(r) == 0: return float(\"nan\")\n",
    "    return float(np.quantile(r, q))\n",
    "\n",
    "def winkler_score(y: np.ndarray, lo: np.ndarray, hi: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    width = hi - lo\n",
    "    below, above = y < lo, y > hi\n",
    "    score = width.copy()\n",
    "    score[below] += (2.0/alpha) * (lo[below] - y[below])\n",
    "    score[above] += (2.0/alpha) * (y[above] - hi[above])\n",
    "    return score\n",
    "\n",
    "# ---------- I/O helpers ----------\n",
    "def load_test_predictions(pred_dir: str, ticker: str) -> Optional[pd.DataFrame]:\n",
    "    path = os.path.join(pred_dir, f\"{ticker}_test_predictions.csv\")\n",
    "    if not os.path.exists(path): return None\n",
    "    df = pd.read_csv(path)\n",
    "    # normalize names\n",
    "    ren = {}\n",
    "    if \"Actual_Close_Return\" in df.columns: ren[\"Actual_Close_Return\"] = \"Actual_Return\"\n",
    "    if \"Pred_Close_Return\"   in df.columns: ren[\"Pred_Close_Return\"]   = \"Pred_Return\"\n",
    "    df = df.rename(columns=ren)\n",
    "    need = {\"Actual_Return\",\"Pred_Return\"}\n",
    "    if not need.issubset(df.columns): return None\n",
    "    if \"Date\" in df.columns: df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    return df\n",
    "\n",
    "def load_tomorrow(pred_dir: str, ticker: str) -> Optional[dict]:\n",
    "    path = os.path.join(pred_dir, f\"{ticker}_tomorrow.csv\")\n",
    "    if not os.path.exists(path): return None\n",
    "    return pd.read_csv(path).iloc[0].to_dict()\n",
    "\n",
    "def load_forecast(fdir: str, ticker: str) -> Optional[pd.DataFrame]:\n",
    "    path = os.path.join(fdir, f\"{ticker}_forecast_7d.csv\")\n",
    "    if not os.path.exists(path): return None\n",
    "    df = pd.read_csv(path, parse_dates=[\"Date\"])\n",
    "    if \"Pred_Close_Return\" in df.columns: df = df.rename(columns={\"Pred_Close_Return\":\"Pred_Return\"})\n",
    "    if \"Predicted_Close\" in df.columns: df[\"Pred_Close\"] = df[\"Predicted_Close\"]\n",
    "    need = {\"Date\",\"Pred_Return\",\"Pred_Close\"}\n",
    "    return df[list(need)] if need.issubset(df.columns) else None\n",
    "\n",
    "def load_top_features(shap_dir: str, ticker: str, head: str = \"close\", top_n: int = 5) -> Optional[List[str]]:\n",
    "    path = os.path.join(shap_dir, ticker, f\"{ticker}_feature_importance_{head}.csv\")\n",
    "    if not os.path.exists(path): return None\n",
    "    try:\n",
    "        imp = pd.read_csv(path)\n",
    "        col_feat = \"feature\" if \"feature\" in imp.columns else imp.columns[0]\n",
    "        col_score= \"mean_abs_shap\" if \"mean_abs_shap\" in imp.columns else imp.columns[1]\n",
    "        imp = imp.sort_values(col_score, ascending=False)\n",
    "        return imp[col_feat].head(top_n).tolist()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- Risk helpers ----------\n",
    "def volatility_marks(returns: pd.Series, lookback: int = VOL_LOOKBACK_DEFAULT):\n",
    "    rv = returns.rolling(lookback).std().dropna()\n",
    "    if len(rv) < 30: return None\n",
    "    return {\"recent\": float(rv.iloc[-1]),\n",
    "            \"p75\": float(np.quantile(rv, 0.75)),\n",
    "            \"p90\": float(np.quantile(rv, 0.90))}\n",
    "\n",
    "def risk_label(interval_pct: float, crosses_zero: bool, marks: Optional[dict], width_thresh: float):\n",
    "    triggers = []\n",
    "    if interval_pct >= width_thresh: triggers.append(\"uncertainty_wide\")\n",
    "    if crosses_zero: triggers.append(\"direction_uncertain\")\n",
    "    if marks:\n",
    "        if marks[\"recent\"] >= marks[\"p90\"]: triggers.append(\"volatility_extreme\")\n",
    "        elif marks[\"recent\"] >= marks[\"p75\"]: triggers.append(\"volatility_high\")\n",
    "    if \"volatility_extreme\" in triggers or (\"uncertainty_wide\" in triggers and \"direction_uncertain\" in triggers):\n",
    "        lvl = \"HIGH\"\n",
    "    elif triggers:\n",
    "        lvl = \"MEDIUM\"\n",
    "    else:\n",
    "        lvl = \"LOW\"\n",
    "    return lvl, triggers\n",
    "\n",
    "def return_to_price_interval(latest_close: float, pred_return: float, q_abs_ret: float):\n",
    "    lo_r, hi_r = pred_return - q_abs_ret, pred_return + q_abs_ret\n",
    "    lo_p = latest_close * (1.0 + lo_r)\n",
    "    hi_p = latest_close * (1.0 + hi_r)\n",
    "    return (lo_r, hi_r), (lo_p, hi_p)\n",
    "\n",
    "# ---------- Conformal calibration ----------\n",
    "def calibrate_from_residuals(df_test: pd.DataFrame,\n",
    "                             alpha: float,\n",
    "                             calib_frac: float,\n",
    "                             calib_min: int,\n",
    "                             roll_cov_win: int) -> Dict[str, Any]:\n",
    "    y  = df_test[\"Actual_Return\"].values.astype(float)\n",
    "    yh = df_test[\"Pred_Return\"].values.astype(float)\n",
    "    resid = y - yh\n",
    "    abs_resid = np.abs(resid)\n",
    "    n = len(abs_resid)\n",
    "    if n < max(calib_min, 20):\n",
    "        return {\"ok\": False, \"reason\": \"not_enough_points\"}\n",
    "\n",
    "    n_cal = max(calib_min, int(calib_frac * n))\n",
    "    n_cal = min(n_cal, n - 5)\n",
    "    cal = abs_resid[:n_cal].copy()\n",
    "    lo, hi = np.quantile(cal, [0.01, 0.99])\n",
    "    cal = np.clip(cal, lo, hi)\n",
    "    q = float(np.quantile(cal, 1 - alpha))\n",
    "\n",
    "    # Diagnostics on evaluation tail\n",
    "    y_ev, yh_ev = y[n_cal:], yh[n_cal:]\n",
    "    lo_ev, hi_ev = yh_ev - q, yh_ev + q\n",
    "    cov = float(np.mean((y_ev >= lo_ev) & (y_ev <= hi_ev))) if len(y_ev) else np.nan\n",
    "    wink = float(np.mean(winkler_score(y_ev, lo_ev, hi_ev, alpha))) if len(y_ev) else np.nan\n",
    "\n",
    "    # Rolling coverage (on full series)\n",
    "    lo_all, hi_all = yh - q, yh + q\n",
    "    ok_all = (y >= lo_all) & (y <= hi_all)\n",
    "    roll_cov = pd.Series(ok_all).rolling(roll_cov_win).mean().values\n",
    "\n",
    "    # Adaptive width history\n",
    "    width_hist = None\n",
    "    if {\"Pred_Close\",\"Actual_Close\"}.issubset(df_test.columns):\n",
    "        base = df_test[\"Actual_Close\"].shift(1).values.astype(float)\n",
    "        width_price = base * (2.0 * q)  # approximate price width from return width\n",
    "        rel_width = width_price / np.maximum(df_test[\"Pred_Close\"].values.astype(float), 1e-8)\n",
    "        width_hist = pd.Series(rel_width).rolling(ADAPT_WIDTH_WIN).mean().dropna().values\n",
    "\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"alpha\": alpha,\n",
    "        \"q_abs_ret\": q,\n",
    "        \"coverage_eval\": cov,\n",
    "        \"winkler_mean_eval\": wink,\n",
    "        \"indices\": {\"calib_end\": int(n_cal), \"total\": int(n)},\n",
    "        \"roll_cov\": roll_cov.tolist(),\n",
    "        \"width_hist\": width_hist.tolist() if width_hist is not None else None\n",
    "    }\n",
    "\n",
    "# ---------- Fan chart (residual bootstrapping) ----------\n",
    "def bootstrap_fan_chart(start_price: float,\n",
    "                        pred_returns: np.ndarray,\n",
    "                        calib_residuals: np.ndarray,\n",
    "                        n_paths: int = BOOTSTRAP_PATHS_DEFAULT) -> Dict[str, np.ndarray]:\n",
    "    T = len(pred_returns)\n",
    "    paths = np.zeros((n_paths, T))\n",
    "    for i in range(n_paths):\n",
    "        p = start_price\n",
    "        eps = rng.choice(calib_residuals, size=T, replace=True)\n",
    "        for t in range(T):\n",
    "            r = float(pred_returns[t]) + float(eps[t])\n",
    "            p = p * (1.0 + r)\n",
    "            paths[i, t] = p\n",
    "    return {\"p10\": np.percentile(paths, 10, axis=0),\n",
    "            \"p50\": np.percentile(paths, 50, axis=0),\n",
    "            \"p90\": np.percentile(paths, 90, axis=0)}\n",
    "\n",
    "# =========================================================\n",
    "# -------------------- Pipeline nodes ---------------------\n",
    "# =========================================================\n",
    "def node_load(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    fe_path   = _under_run_base(state, state.get(\"data_path\", FE_PATH_DEFAULT))\n",
    "    pred_dir  = _under_run_base(state, state.get(\"predictions_dir\", PRED_DIR_DEFAULT))\n",
    "    fcast_dir = _under_run_base(state, state.get(\"forecasts_dir\", FCAST_DIR_DEFAULT))\n",
    "    shap_dir  = _under_run_base(state, state.get(\"shap_dir\", SHAP_DIR_DEFAULT))\n",
    "\n",
    "    tickers   = state.get(\"tickers\")\n",
    "    days_win  = state.get(\"days_window\", None)\n",
    "\n",
    "    if not os.path.exists(fe_path):\n",
    "        raise FileNotFoundError(f\"Engineered features not found: {fe_path}\")\n",
    "\n",
    "    fe = pd.read_csv(fe_path, parse_dates=[\"Date\"])\n",
    "    need = {\"Date\",\"Symbol\",\"Close\"}\n",
    "    if not need.issubset(fe.columns):\n",
    "        raise ValueError(f\"{fe_path} missing columns {need}\")\n",
    "    fe = fe[[\"Date\",\"Symbol\",\"Close\"]].dropna().sort_values([\"Symbol\",\"Date\"]).reset_index(drop=True)\n",
    "    if not tickers:\n",
    "        tickers = sorted(fe[\"Symbol\"].unique().tolist())\n",
    "\n",
    "    per_ticker = {}\n",
    "    for t in tickers:\n",
    "        hist = fe[fe[\"Symbol\"] == t].copy()\n",
    "        if hist.empty: continue\n",
    "        if days_win and len(hist) > days_win:\n",
    "            hist = hist.tail(days_win).copy()\n",
    "\n",
    "        test = load_test_predictions(pred_dir, t)\n",
    "        tom  = load_tomorrow(pred_dir, t)\n",
    "        fc   = load_forecast(fcast_dir, t)\n",
    "        top5 = load_top_features(shap_dir, t, head=state.get(\"head\",\"close\"))\n",
    "\n",
    "        per_ticker[t] = {\n",
    "            \"hist\": hist,              # Date, Close\n",
    "            \"test_preds\": test,        # residuals source\n",
    "            \"tomorrow\": tom,           # next-day point forecast\n",
    "            \"forecast\": fc,            # 7-day point forecasts\n",
    "            \"top_features\": top5       # enrichment\n",
    "        }\n",
    "\n",
    "    return {**state,\n",
    "            \"per_ticker\": per_ticker,\n",
    "            \"tickers\": list(per_ticker.keys()),\n",
    "            \"fe_path\": fe_path,\n",
    "            \"pred_dir\": pred_dir,\n",
    "            \"fcast_dir\": fcast_dir,\n",
    "            \"shap_dir\": shap_dir,\n",
    "            \"status\": \"loaded\"}\n",
    "\n",
    "def node_assess(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    alpha      = float(state.get(\"alpha\", ALPHA_DEFAULT))\n",
    "    calib_frac = float(state.get(\"calib_frac\", CALIB_FRAC_DEFAULT))\n",
    "    calib_min  = int(state.get(\"calib_min\", CALIB_MIN_DEFAULT))\n",
    "    roll_win   = int(state.get(\"roll_cov_win\", ROLL_COVER_WIN_DEFAULT))\n",
    "    ann        = int(state.get(\"annualization\", 252))\n",
    "    lookback   = int(state.get(\"vol_lookback\", VOL_LOOKBACK_DEFAULT))\n",
    "\n",
    "    for t, bundle in state[\"per_ticker\"].items():\n",
    "        hist = bundle[\"hist\"].copy().set_index(\"Date\")\n",
    "        close = hist[\"Close\"].astype(float)\n",
    "        ret   = pct_returns(close)\n",
    "        vol   = rolling_vol(ret, window=lookback)\n",
    "        dd    = drawdown(close)\n",
    "        marks = volatility_marks(ret, lookback=lookback)\n",
    "\n",
    "        # Metrics (historical)\n",
    "        metrics = {\n",
    "            \"Obs\": int(len(close)),\n",
    "            \"Daily_Vol\": float(ret.std()) if len(ret) else np.nan,\n",
    "            \"Sharpe\": sharpe_ratio(ret, ann),\n",
    "            \"Max_Drawdown\": float(dd.min()) if len(dd) else np.nan,\n",
    "            \"VaR_5pct_1d\": var_percentile(ret, 0.05),\n",
    "        }\n",
    "\n",
    "        # Conformal calibration\n",
    "        calib = {\"ok\": False, \"reason\": \"no_test_predictions\"}\n",
    "        if bundle[\"test_preds\"] is not None and len(bundle[\"test_preds\"]) >= max(calib_min, 20):\n",
    "            calib = calibrate_from_residuals(bundle[\"test_preds\"], alpha, calib_frac, calib_min, roll_win)\n",
    "\n",
    "        # Adaptive width threshold\n",
    "        width_thresh = 0.02\n",
    "        if calib.get(\"width_hist\"):\n",
    "            w = np.array(calib[\"width_hist\"])\n",
    "            if len(w) > 20:\n",
    "                width_thresh = float(np.nanpercentile(w, 80))\n",
    "\n",
    "        # Tomorrow assessment\n",
    "        risk = None\n",
    "        predicted_rows = []\n",
    "        if bundle[\"tomorrow\"] and calib.get(\"ok\"):\n",
    "            tom   = bundle[\"tomorrow\"]\n",
    "            last_close = float(tom.get(\"Latest_Actual_Close\", close.iloc[-1]))\n",
    "            r_pred = float(tom.get(\"Pred_Close_Return\", tom.get(\"Pred_Return\", 0.0)))\n",
    "            p_pred = float(tom.get(\"Pred_Close\", last_close * (1 + r_pred)))\n",
    "            (lo_r, hi_r), (lo_p, hi_p) = return_to_price_interval(last_close, r_pred, calib[\"q_abs_ret\"])\n",
    "            band_pct = (hi_p - lo_p) / max(p_pred, 1e-8)\n",
    "            crosses0 = (lo_r <= 0.0 <= hi_r)\n",
    "            label, triggers = risk_label(band_pct, crosses0, marks, width_thresh)\n",
    "\n",
    "            snr = abs(r_pred) / max(calib[\"q_abs_ret\"], 1e-12)\n",
    "\n",
    "            risk = {\n",
    "                \"Pred_Close\": p_pred,\n",
    "                \"PI_Return\": [lo_r, hi_r],\n",
    "                \"PI_Close\": [lo_p, hi_p],\n",
    "                \"Uncertainty_Band_Pct\": float(band_pct),\n",
    "                \"Crosses_Zero\": bool(crosses0),\n",
    "                \"Risk_Label\": label,\n",
    "                \"Risk_Triggers\": triggers,\n",
    "                \"SNR\": float(snr),\n",
    "                \"Width_Thresh_Used\": width_thresh,\n",
    "                \"Top_Features\": bundle.get(\"top_features\")\n",
    "            }\n",
    "\n",
    "            predicted_rows.append({\n",
    "                \"For_Date\": tom.get(\"For_Date\"),\n",
    "                \"Pred_Close\": p_pred,\n",
    "                \"PI_Close_Lo\": lo_p,\n",
    "                \"PI_Close_Hi\": hi_p,\n",
    "                \"BandPct\": band_pct,\n",
    "                \"CrossesZero\": crosses0,\n",
    "                \"Risk_Label\": label,\n",
    "                \"Triggers\": \";\".join(triggers),\n",
    "                \"Pred_Close_Return\": r_pred,\n",
    "                \"SNR\": snr,\n",
    "                \"Top_Features\": \",\".join(bundle.get(\"top_features\") or [])\n",
    "            })\n",
    "\n",
    "        # 7-day fan chart (forecast + calibration)\n",
    "        forecast_bands = None\n",
    "        if bundle[\"forecast\"] is not None and calib.get(\"ok\"):\n",
    "            df_test = bundle[\"test_preds\"]\n",
    "            n_cal = calib[\"indices\"][\"calib_end\"] if calib.get(\"indices\") else max(60, int(0.7*len(df_test)))\n",
    "            eps = (df_test[\"Actual_Return\"].values[:n_cal] - df_test[\"Pred_Return\"].values[:n_cal])\n",
    "            fc = bundle[\"forecast\"].copy()\n",
    "            fan = bootstrap_fan_chart(float(close.iloc[-1]),\n",
    "                                      fc[\"Pred_Return\"].values.astype(float),\n",
    "                                      eps,\n",
    "                                      n_paths=state.get(\"bootstrap_paths\", BOOTSTRAP_PATHS_DEFAULT))\n",
    "            fc[\"PI10\"] = fan[\"p10\"]; fc[\"PI50\"] = fan[\"p50\"]; fc[\"PI90\"] = fan[\"p90\"]\n",
    "            forecast_bands = fc\n",
    "\n",
    "\n",
    "            q = calib[\"q_abs_ret\"]\n",
    "            base = float(close.iloc[-1])\n",
    "            for i, r_k in enumerate(fc[\"Pred_Return\"].values.astype(float)):\n",
    "                (lo_r, hi_r), (lo_p, hi_p) = return_to_price_interval(base, r_k, q)\n",
    "                band_pct = (hi_p - lo_p) / max(fc[\"Pred_Close\"].iloc[i], 1e-8)\n",
    "                crosses0 = (lo_r <= 0.0 <= hi_r)\n",
    "                label, triggers = risk_label(band_pct, crosses0, marks, width_thresh)\n",
    "                predicted_rows.append({\n",
    "                    \"For_Date\": fc[\"Date\"].iloc[i].strftime(\"%Y-%m-%d\"),\n",
    "                    \"Pred_Close\": float(fc[\"Pred_Close\"].iloc[i]),\n",
    "                    \"PI_Close_Lo\": lo_p,\n",
    "                    \"PI_Close_Hi\": hi_p,\n",
    "                    \"BandPct\": band_pct,\n",
    "                    \"CrossesZero\": crosses0,\n",
    "                    \"Risk_Label\": label,\n",
    "                    \"Triggers\": \";\".join(triggers),\n",
    "                    \"Pred_Close_Return\": r_k,\n",
    "                    \"SNR\": abs(r_k)/max(q,1e-12),\n",
    "                    \"Top_Features\": \",\".join(bundle.get(\"top_features\") or [])\n",
    "                })\n",
    "\n",
    "        # Persist in bundle\n",
    "        bundle.update({\n",
    "            \"close\": close, \"returns\": ret, \"rolling_vol\": vol, \"drawdown\": dd,\n",
    "            \"vol_marks\": marks, \"calibration\": calib, \"risk\": risk,\n",
    "            \"forecast_bands\": forecast_bands,\n",
    "            \"metrics\": metrics,\n",
    "            \"predicted_rows\": predicted_rows\n",
    "        })\n",
    "\n",
    "    return {**state, \"status\": \"assessed\"}\n",
    "\n",
    "def node_save(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out_dir = _under_run_base(state, state.get(\"out_dir\", OUT_DIR_DEFAULT))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(out_dir, \"plots\"), exist_ok=True)\n",
    "\n",
    "    summary, rows = {}, []\n",
    "\n",
    "    for t, b in state[\"per_ticker\"].items():\n",
    "        hist_idx = b[\"close\"].index\n",
    "        # ---- Historical CSV\n",
    "        pd.DataFrame({\n",
    "            \"Date\": hist_idx,\n",
    "            \"Close\": b[\"close\"].values,\n",
    "            \"Return\": b[\"returns\"].reindex(hist_idx).values,\n",
    "            \"RollingVol20\": b[\"rolling_vol\"].reindex(hist_idx).values,\n",
    "            \"Drawdown\": b[\"drawdown\"].reindex(hist_idx).values\n",
    "        }).to_csv(os.path.join(out_dir, f\"{t}_historical_risk.csv\"), index=False)\n",
    "\n",
    "        # ---- Predicted CSV (tomorrow + 7-day)\n",
    "        if b.get(\"predicted_rows\"):\n",
    "            pd.DataFrame(b[\"predicted_rows\"]).to_csv(os.path.join(out_dir, f\"{t}_predicted_risk.csv\"), index=False)\n",
    "\n",
    "        # ---- Plots\n",
    "        idx = b[\"close\"].index\n",
    "        n = len(idx)\n",
    "        i_tr = int(0.70 * n)          # end of train\n",
    "        i_va = int(0.85 * n)          # end of validation\n",
    "        split_dates = []\n",
    "        if n >= 10:  # safety\n",
    "            split_dates = [idx[i_tr], idx[i_va]]\n",
    "\n",
    "        def _plot_with_splits(series, title, ylabel, fname):\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(series.index, series.values, linewidth=1.3)\n",
    "\n",
    "            for x in split_dates:\n",
    "                plt.axvline(x=x, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "            if split_dates:\n",
    "                ylim_top = plt.gca().get_ylim()[1]\n",
    "                plt.text(split_dates[0], ylim_top, \"Train/Val split\", va=\"bottom\", ha=\"left\", fontsize=8)\n",
    "                plt.text(split_dates[1], ylim_top, \"Val/Test split\",  va=\"bottom\", ha=\"left\", fontsize=8)\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(out_dir, \"plots\", fname), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        _plot_with_splits(b[\"close\"],      f\"{t} — Close\",                    \"Price (USD)\",          f\"{t}_price.png\")\n",
    "        # Drawdown and rolling volatility plot\n",
    "        _plot_with_splits(b[\"drawdown\"],   f\"{t} — Drawdown\",                 \"Drawdown (fraction)\",  f\"{t}_drawdown.png\")\n",
    "        _plot_with_splits(b[\"rolling_vol\"],f\"{t} — Rolling Volatility (20d)\", \"Std. dev. (daily)\",    f\"{t}_rolling_vol.png\")\n",
    "\n",
    "        # Daily returns histogram\n",
    "        rets = b[\"returns\"].dropna()\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(rets, bins=60, alpha=0.85)\n",
    "        plt.axvline(0.0, linestyle=\"--\", linewidth=1)\n",
    "        if len(rets) > 0:\n",
    "            plt.axvline(rets.mean(), linestyle=\":\", linewidth=1)\n",
    "        plt.title(f\"{t} — Daily Returns Histogram\")\n",
    "        plt.xlabel(\"Daily return\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(alpha=0.25)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, \"plots\", f\"{t}_return_hist.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # ---- Residual histogram & rolling coverage\n",
    "        calib = b.get(\"calibration\", {})\n",
    "        if b.get(\"test_preds\") is not None:\n",
    "            resid = (b[\"test_preds\"][\"Actual_Return\"] - b[\"test_preds\"][\"Pred_Return\"]).values\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist(resid, bins=40, alpha=0.85)\n",
    "            if calib.get(\"ok\"):\n",
    "                q = calib[\"q_abs_ret\"]\n",
    "                plt.axvline(+q, color=\"r\", linestyle=\"--\", label=f\"+q @ {(1-calib['alpha'])*100:.0f}%\")\n",
    "                plt.axvline(-q, color=\"r\", linestyle=\"--\")\n",
    "            plt.title(f\"{t} — Test Residuals (returns)\")\n",
    "            plt.xlabel(\"Residual (Actual − Pred)\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(out_dir, \"plots\", f\"{t}_residual_hist.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        if calib.get(\"ok\") and calib.get(\"roll_cov\"):\n",
    "            rc = np.array(calib[\"roll_cov\"])\n",
    "            plt.figure(figsize=(10, 3.5))\n",
    "            plt.plot(rc, label=\"Rolling coverage\")\n",
    "            plt.axhline(1 - calib[\"alpha\"], color=\"k\", linestyle=\"--\", label=\"Target\")\n",
    "            plt.ylim(0, 1)\n",
    "            plt.title(f\"{t} — Rolling Coverage\")\n",
    "            plt.xlabel(\"Window index\")\n",
    "            plt.ylabel(\"Coverage\")\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(out_dir, \"plots\", f\"{t}_rolling_coverage.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        # ---- Tomorrow band plot\n",
    "        if b.get(\"risk\"):\n",
    "            r = b[\"risk\"]\n",
    "            p_mid = r[\"Pred_Close\"]\n",
    "            lo_p, hi_p = r[\"PI_Close\"]\n",
    "            plt.figure(figsize=(7.8, 5))\n",
    "            plt.errorbar([0], [p_mid], yerr=[[p_mid - lo_p], [hi_p - p_mid]], fmt=\"o\")\n",
    "            ttl = f\"{t} — Tomorrow Close Uncertainty ({r['Risk_Label']})\"\n",
    "            plt.title(ttl)\n",
    "            plt.xticks([])\n",
    "            plt.ylabel(\"Price\")\n",
    "            plt.grid(alpha=0.3)\n",
    "            txt = (\n",
    "                f\"Pred: {p_mid:.2f}\\n\"\n",
    "                f\"PI: [{lo_p:.2f}, {hi_p:.2f}] (width {100*r['Uncertainty_Band_Pct']:.2f}%)\\n\"\n",
    "                f\"Signal-to-noise ratio: {r['SNR']:.2f}\"\n",
    "            )\n",
    "            plt.text(0.05, 0.95, txt, transform=plt.gca().transAxes, va=\"top\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(out_dir, \"plots\", f\"{t}_tomorrow_band.png\"), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        # ---- Fan chart\n",
    "        if isinstance(b.get(\"forecast_bands\"), pd.DataFrame):\n",
    "            fc = b[\"forecast_bands\"]\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(fc[\"Date\"], fc[\"PI50\"], label=\"Median\", linewidth=2)\n",
    "            plt.fill_between(fc[\"Date\"], fc[\"PI10\"], fc[\"PI90\"], alpha=0.25, label=\"10–90% band\")\n",
    "            plt.plot(fc[\"Date\"], fc[\"Pred_Close\"], linestyle=\"--\", label=\"Model path\")\n",
    "            plt.title(f\"{t} — 7-Day Forecast Fan Chart\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Price\")\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(out_dir, \"plots\", f\"{t}_fan_chart.png\"), dpi=300)\n",
    "            plt.close()\n",
    "            fc.to_csv(os.path.join(out_dir, f\"{t}_forecast_fanchart.csv\"), index=False)\n",
    "\n",
    "        # Summary\n",
    "        summary[t] = {\n",
    "            \"metrics\": b[\"metrics\"],\n",
    "            \"calibration\": {k:v for k,v in calib.items() if k in\n",
    "                            {\"ok\",\"alpha\",\"q_abs_ret\",\"coverage_eval\",\"winkler_mean_eval\",\"indices\"}},\n",
    "            \"risk\": b.get(\"risk\"),\n",
    "            \"top_features\": b.get(\"top_features\")\n",
    "        }\n",
    "        rows.append({\n",
    "            \"Ticker\": t,\n",
    "            \"Obs\": b[\"metrics\"][\"Obs\"],\n",
    "            \"Daily_Vol\": round(b[\"metrics\"][\"Daily_Vol\"],6) if not np.isnan(b[\"metrics\"][\"Daily_Vol\"]) else np.nan,\n",
    "            \"Sharpe\": round(b[\"metrics\"][\"Sharpe\"],3) if not np.isnan(b[\"metrics\"][\"Sharpe\"]) else np.nan,\n",
    "            \"Max_Drawdown\": round(b[\"metrics\"][\"Max_Drawdown\"],4) if not np.isnan(b[\"metrics\"][\"Max_Drawdown\"]) else np.nan,\n",
    "            \"VaR_5pct_1d\": round(b[\"metrics\"][\"VaR_5pct_1d\"],4) if not np.isnan(b[\"metrics\"][\"VaR_5pct_1d\"]) else np.nan,\n",
    "            \"Risk_Label_Tomorrow\": b.get(\"risk\",{}).get(\"Risk_Label\") if b.get(\"risk\") else None\n",
    "        })\n",
    "\n",
    "    # Write summary files\n",
    "    with open(os.path.join(out_dir, \"summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    if rows:\n",
    "        pd.DataFrame(rows).sort_values(\"Ticker\").to_csv(os.path.join(out_dir, \"summary.csv\"), index=False)\n",
    "\n",
    "    print(f\" Risk assessment saved → {out_dir}\")\n",
    "    return {**state, \"status\": \"done\", \"report_dir\": out_dir}\n",
    "\n",
    "# ---------- Build / run ----------\n",
    "def build_risk_workflow():\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"load\", node_load)\n",
    "    g.add_node(\"assess\", node_assess)\n",
    "    g.add_node(\"save\", node_save)\n",
    "    g.set_entry_point(\"load\")\n",
    "    g.add_edge(\"load\", \"assess\")\n",
    "    g.add_edge(\"assess\", \"save\")\n",
    "    g.set_finish_point(\"save\")\n",
    "    return g.compile()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    payload = {\n",
    "\n",
    "        # \"run_base\": \"/content/drive/MyDrive/A2A_prediction_system/RUN_YYYYMMDD_HHMMSS\",\n",
    "\n",
    "        \"data_path\": FE_PATH_DEFAULT,\n",
    "        \"predictions_dir\": PRED_DIR_DEFAULT,\n",
    "        \"forecasts_dir\": FCAST_DIR_DEFAULT,\n",
    "        \"shap_dir\": SHAP_DIR_DEFAULT,\n",
    "        \"out_dir\": OUT_DIR_DEFAULT,\n",
    "\n",
    "        # Order of agents: Prediction → SHAP/IG → Risk (this agent) → Evaluation\n",
    "        # tickers: omit to auto-detect\n",
    "        # \"tickers\": [\"AAPL\"],\n",
    "\n",
    "        # Calibration & diagnostics\n",
    "        \"alpha\": ALPHA_DEFAULT,\n",
    "        \"calib_frac\": CALIB_FRAC_DEFAULT,\n",
    "        \"calib_min\": CALIB_MIN_DEFAULT,\n",
    "        \"roll_cov_win\": ROLL_COVER_WIN_DEFAULT,\n",
    "\n",
    "        # Risk context\n",
    "        \"annualization\": 252,\n",
    "        \"vol_lookback\": VOL_LOOKBACK_DEFAULT,\n",
    "\n",
    "        # Fan chart\n",
    "        \"bootstrap_paths\": BOOTSTRAP_PATHS_DEFAULT,\n",
    "\n",
    "        # Data window\n",
    "        \"days_window\": 252,\n",
    "\n",
    "        # SHAP head (for top-features enrichment)\n",
    "        \"head\": \"close\"\n",
    "    }\n",
    "    if HAS_LANGGRAPH:\n",
    "        app = build_risk_workflow()\n",
    "        _ = app.invoke(payload)\n",
    "    else:\n",
    "        s = node_load(payload)\n",
    "        s = node_assess(s)\n",
    "        s = node_save(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "e6UwHgRqdxPO"
   },
   "outputs": [],
   "source": [
    "**optimisation_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5U_wmnuSvbH",
    "outputId": "d6e02991-8700-46ee-9dbe-6e9cead9b01e"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/optimisation_agent.py\n",
    "\n",
    "\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, Any, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- LangGraph wiring ----\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LANGGRAPH = True\n",
    "except Exception:\n",
    "    HAS_LANGGRAPH = False\n",
    "\n",
    "# ---------------- utilities ----------------\n",
    "def ensure_dir(p: str): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _under_run_base(state: Dict[str, Any], path: str) -> str:\n",
    "    \"\"\"\n",
    "    If state has run_base and 'path' is relative, join them.\n",
    "    Otherwise, return path unchanged.\n",
    "    \"\"\"\n",
    "    rb = state.get(\"run_base\")\n",
    "    if rb and not os.path.isabs(path):\n",
    "        return os.path.join(rb, path)\n",
    "    return path\n",
    "\n",
    "def kpis(equity: pd.Series, daily_ret: pd.Series) -> Dict[str, float]:\n",
    "    equity = equity.dropna(); daily_ret = daily_ret.dropna()\n",
    "    if len(equity) < 2 or len(daily_ret) < 2:\n",
    "        return {k: float(\"nan\") for k in\n",
    "                [\"CAGR\",\"Sharpe\",\"MaxDrawdown\",\"HitRate\",\"Volatility_Daily\",\"TotalReturn\"]}\n",
    "    total_return = float(equity.iloc[-1] / equity.iloc[0] - 1.0)\n",
    "    years = max((pd.to_datetime(equity.index[-1]) - pd.to_datetime(equity.index[0])).days / 365.25, 1e-9)\n",
    "    cagr = float((equity.iloc[-1] / equity.iloc[0]) ** (1/years) - 1.0)\n",
    "    vol = float(daily_ret.std())\n",
    "    sharpe = float(np.sqrt(252) * daily_ret.mean() / vol) if vol > 0 else float(\"nan\")\n",
    "    dd = float((equity / equity.cummax() - 1.0).min())\n",
    "    hit = float((daily_ret > 0).mean() * 100.0)\n",
    "    return {\"CAGR\": cagr, \"Sharpe\": sharpe, \"MaxDrawdown\": dd,\n",
    "            \"HitRate\": hit, \"Volatility_Daily\": vol, \"TotalReturn\": total_return}\n",
    "\n",
    "# ------------- schema helpers -------------\n",
    "def _norm_predictions(df: pd.DataFrame, head: str) -> pd.DataFrame:\n",
    "    \"\"\"Return Date, Pred_Return, Actual_Return.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Date\" not in df.columns: raise ValueError(\"Predictions CSV missing 'Date'\")\n",
    "    pred_two = f\"Pred_{head.capitalize()}_Return\"\n",
    "    act_two  = f\"Actual_{head.capitalize()}_Return\"\n",
    "    if {\"Pred_Return\",\"Actual_Return\"}.issubset(df.columns):\n",
    "        out = df[[\"Date\",\"Pred_Return\",\"Actual_Return\"]].copy()\n",
    "    elif {pred_two, act_two}.issubset(df.columns):\n",
    "        out = df[[\"Date\", pred_two, act_two]].rename(\n",
    "            columns={pred_two:\"Pred_Return\", act_two:\"Actual_Return\"})\n",
    "    else:\n",
    "        raise ValueError(\"Predictions CSV must have Pred_Return & Actual_Return (or head-specific columns).\")\n",
    "    out[\"Date\"] = pd.to_datetime(out[\"Date\"])\n",
    "    return out.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "def _norm_forecast(df: pd.DataFrame, head: str) -> pd.DataFrame:\n",
    "    \"\"\"Return Date, Pred_Return.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Date\" not in df.columns: raise ValueError(\"Forecast CSV missing 'Date'\")\n",
    "    pr_two = f\"Pred_{head.capitalize()}_Return\"; pp_two = f\"Pred_{head.capitalize()}\"\n",
    "    if {\"Pred_Return\",\"Pred_Close\"}.issubset(df.columns):\n",
    "        out = df[[\"Date\",\"Pred_Return\",\"Pred_Close\"]].copy()\n",
    "    elif {pr_two, pp_two}.issubset(df.columns):\n",
    "        out = df[[\"Date\", pr_two, pp_two]].rename(columns={pr_two:\"Pred_Return\", pp_two:\"Pred_Close\"})\n",
    "    else:\n",
    "        if \"Pred_Return\" in df.columns:\n",
    "            out = df[[\"Date\",\"Pred_Return\"]].copy(); out[\"Pred_Close\"] = np.nan\n",
    "        elif pr_two in df.columns:\n",
    "            out = df[[\"Date\", pr_two]].rename(columns={pr_two:\"Pred_Return\"}); out[\"Pred_Close\"] = np.nan\n",
    "        else:\n",
    "            raise ValueError(\"Forecast CSV must contain Pred_Return (and ideally Pred_Close).\")\n",
    "    out[\"Date\"] = pd.to_datetime(out[\"Date\"])\n",
    "    return out.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# ---------------- risk helpers ---------------\n",
    "def _load_q_from_summary(risk_dir: str, ticker: str) -> Optional[float]:\n",
    "    p = os.path.join(risk_dir, \"summary.json\")\n",
    "    if not os.path.exists(p): return None\n",
    "    try:\n",
    "        obj = json.load(open(p, \"r\"))\n",
    "        r = obj.get(ticker, {})\n",
    "        cal = r.get(\"calibration\", {})\n",
    "        q = cal.get(\"q_abs_ret\")\n",
    "        return float(q) if q is not None else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _load_predicted_risk_rows(risk_dir: str, ticker: str) -> Optional[pd.DataFrame]:\n",
    "    p = os.path.join(risk_dir, f\"{ticker}_predicted_risk.csv\")\n",
    "    if not os.path.exists(p): return None\n",
    "    df = pd.read_csv(p)\n",
    "    if \"For_Date\" in df.columns: df[\"Date\"] = pd.to_datetime(df[\"For_Date\"])\n",
    "    elif \"Date\" in df.columns:   df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    else: return None\n",
    "    return df\n",
    "\n",
    "# compute q from predictions if Risk summary not present ---\n",
    "def _compute_q_from_predictions(pred_df: pd.DataFrame, alpha: float) -> Optional[float]:\n",
    "    try:\n",
    "        y  = pred_df[\"Actual_Return\"].astype(float).values\n",
    "        yh = pred_df[\"Pred_Return\"].astype(float).values\n",
    "        resid = np.abs(y - yh)\n",
    "        if len(resid) < 30:\n",
    "            return None\n",
    "        lo, hi = np.quantile(resid, [0.01, 0.99])\n",
    "        resid = np.clip(resid, lo, hi)\n",
    "        return float(np.quantile(resid, 1 - alpha))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# =========================================================\n",
    "#                       NODES\n",
    "# =========================================================\n",
    "def node_load(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    # ticker handling (single ticker for now)\n",
    "    tickers = state.get(\"tickers\") or [\"AAPL\"]\n",
    "    ticker = tickers[0]\n",
    "\n",
    "    mode     = str(state.get(\"mode\",\"historical\")).lower()\n",
    "    head     = str(state.get(\"head\",\"close\")).lower()\n",
    "\n",
    "    pred_dir = _under_run_base(state, state.get(\"predictions_dir\",\"Predictive_Model/predictions\"))\n",
    "    fcast_dir= _under_run_base(state, state.get(\"forecasts_dir\",\"Predictive_Model/advanced_forecasts\"))\n",
    "    fe_path  = _under_run_base(state, state.get(\"features_path\",\"FE_Agent/features_engineered.csv\"))\n",
    "    risk_dir = _under_run_base(state, state.get(\"risk_dir\",\"Risk_Assessment\"))\n",
    "    out_dir  = _under_run_base(state, state.get(\"out_dir\",\"Opt_results\"))\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    # try to get q from Risk assessment\n",
    "    q = _load_q_from_summary(risk_dir, ticker)\n",
    "\n",
    "    risk_rows = _load_predicted_risk_rows(risk_dir, ticker)\n",
    "\n",
    "    if mode == \"historical\":\n",
    "        ppath = os.path.join(pred_dir, f\"{ticker}_test_predictions.csv\")\n",
    "        if not os.path.exists(ppath):\n",
    "            raise FileNotFoundError(f\"Missing predictions: {ppath}\")\n",
    "        pred = _norm_predictions(pd.read_csv(ppath), head=head)\n",
    "        hist = pred[[\"Date\",\"Actual_Return\"]].copy()\n",
    "        hist[\"NextDay_Return\"] = hist[\"Actual_Return\"].shift(-1)\n",
    "        hist[\"Vol20\"] = hist[\"Actual_Return\"].rolling(20).std()\n",
    "\n",
    "        # fallback q if Risk summary not available\n",
    "        if q is None:\n",
    "            alpha = float(state.get(\"alpha\", 0.10))\n",
    "            q = _compute_q_from_predictions(pred, alpha)\n",
    "\n",
    "        bundle = {\"pred\": pred, \"hist\": hist, \"q\": q, \"risk_rows\": None}\n",
    "\n",
    "    elif mode == \"forecast\":\n",
    "        if not os.path.exists(fe_path):\n",
    "            raise FileNotFoundError(f\"Engineered features not found: {fe_path}\")\n",
    "        feats = pd.read_csv(fe_path, parse_dates=[\"Date\"])\n",
    "        feats = feats[feats[\"Symbol\"] == ticker].sort_values(\"Date\").reset_index(drop=True)\n",
    "        if len(feats) < 30:\n",
    "            raise RuntimeError(f\"Not enough {ticker} rows in engineered features.\")\n",
    "        fpath = os.path.join(fcast_dir, f\"{ticker}_forecast_7d.csv\")\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(f\"Missing forecast: {fpath}\")\n",
    "        fc = _norm_forecast(pd.read_csv(fpath), head=head)\n",
    "        feats[\"Actual_Return\"] = feats[\"Close\"].pct_change()\n",
    "        feats[\"Vol20\"] = feats[\"Actual_Return\"].rolling(20).std()\n",
    "        bundle = {\"forecast\": fc, \"hist\": feats[[\"Date\",\"Actual_Return\",\"Vol20\"]],\n",
    "                  \"q\": q, \"risk_rows\": risk_rows}\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'historical' or 'forecast'\")\n",
    "\n",
    "    print(f\"[Opt] Loaded mode={mode}, ticker={ticker}, head={head}\")\n",
    "    if q is None:\n",
    "        print(\"[Opt] Note: q (uncertainty band) unavailable — SNR gating will only use strong-pct / risk rows.\")\n",
    "    else:\n",
    "        print(f\"[Opt] q_abs_ret={q:.6f}\")\n",
    "\n",
    "    return {**state, \"ticker\": ticker, \"bundle\": bundle, \"head\": head, \"out_dir\": out_dir, \"status\":\"loaded\"}\n",
    "\n",
    "def node_signals(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    mode = state.get(\"mode\",\"historical\").lower()\n",
    "    allow_shorts = bool(state.get(\"allow_shorts\", False))\n",
    "    w_max   = float(state.get(\"w_max\", 0.30))\n",
    "    snr_min = float(state.get(\"snr_min\", 1.0))\n",
    "    strong_pct = int(state.get(\"strong_pct\", 75))\n",
    "\n",
    "    b = state[\"bundle\"]\n",
    "    rows = []\n",
    "\n",
    "    if mode == \"historical\":\n",
    "        df = b[\"pred\"].copy()  # Date, Pred_Return, Actual_Return\n",
    "        hist = b[\"hist\"].copy()\n",
    "        q = b.get(\"q\", None)\n",
    "\n",
    "        # strong-signal threshold from |Pred_Return|\n",
    "        x = np.abs(df[\"Pred_Return\"].values)\n",
    "        thr = float(np.percentile(x, strong_pct)) if len(x) >= 10 else 0.0\n",
    "\n",
    "        for _, r in df.iterrows():\n",
    "            dt = pd.to_datetime(r[\"Date\"])\n",
    "            r_pred = float(r[\"Pred_Return\"])\n",
    "            vol = hist[hist[\"Date\"] <= dt][\"Vol20\"].dropna()\n",
    "            vol = float(vol.iloc[-1]) if len(vol) else float(hist[\"Vol20\"].median() or 1e-6)\n",
    "\n",
    "            passes = True\n",
    "            if q is None:\n",
    "                passes = (abs(r_pred) >= thr)\n",
    "            else:\n",
    "                snr = abs(r_pred) / max(q, 1e-12)\n",
    "                crosses_zero = (r_pred - q) <= 0.0 <= (r_pred + q)\n",
    "                passes = (snr >= snr_min) and (not crosses_zero) and (abs(r_pred) >= thr)\n",
    "\n",
    "            if not passes or vol == 0 or np.isnan(vol):\n",
    "                w = 0.0\n",
    "            else:\n",
    "                w = r_pred / vol\n",
    "                if not allow_shorts and w < 0: w = 0.0\n",
    "                w = float(np.clip(w, -w_max, w_max))\n",
    "            rows.append({\"Date\": dt, \"Weight\": w})\n",
    "\n",
    "    else:  # forecast\n",
    "        fc = b[\"forecast\"].copy()   # Date, Pred_Return (+ Pred_Close)\n",
    "        hist = b[\"hist\"].copy()\n",
    "        risk_rows = b.get(\"risk_rows\")\n",
    "        q = b.get(\"q\", None)\n",
    "\n",
    "        for _, r in fc.iterrows():\n",
    "            dt = pd.to_datetime(r[\"Date\"])\n",
    "            r_pred = float(r[\"Pred_Return\"])\n",
    "            vol = hist[hist[\"Date\"] <= dt][\"Vol20\"].dropna()\n",
    "            vol = float(vol.iloc[-1]) if len(vol) else float(hist[\"Vol20\"].median() or 1e-6)\n",
    "\n",
    "            passes = True\n",
    "            if isinstance(risk_rows, pd.DataFrame):\n",
    "                m = risk_rows[risk_rows[\"Date\"] == dt]\n",
    "                if not m.empty:\n",
    "                    lbl = str(m[\"Risk_Label\"].iloc[0]) if \"Risk_Label\" in m.columns else None\n",
    "                    crosses = bool(m[\"CrossesZero\"].iloc[0]) if \"CrossesZero\" in m.columns else None\n",
    "                    snr_val = float(m[\"SNR\"].iloc[0]) if \"SNR\" in m.columns else None\n",
    "                    if lbl and lbl.upper() == \"HIGH\": passes = False\n",
    "                    if crosses is True:               passes = False\n",
    "                    if (snr_val is not None) and (snr_val < snr_min): passes = False\n",
    "                elif q is not None:\n",
    "                    snr = abs(r_pred) / max(q, 1e-12)\n",
    "                    crosses_zero = (r_pred - q) <= 0.0 <= (r_pred + q)\n",
    "                    passes = (snr >= snr_min) and (not crosses_zero)\n",
    "            elif q is not None:\n",
    "                snr = abs(r_pred) / max(q, 1e-12)\n",
    "                crosses_zero = (r_pred - q) <= 0.0 <= (r_pred + q)\n",
    "                passes = (snr >= snr_min) and (not crosses_zero)\n",
    "\n",
    "            if not passes or vol == 0 or np.isnan(vol):\n",
    "                w = 0.0\n",
    "            else:\n",
    "                w = r_pred / vol\n",
    "                if not allow_shorts and w < 0: w = 0.0\n",
    "                w = float(np.clip(w, -w_max, w_max))\n",
    "            rows.append({\"Date\": dt, \"Weight\": w})\n",
    "\n",
    "    W = pd.DataFrame(rows).set_index(\"Date\").sort_index()\n",
    "    print(f\"[Opt] Built weights: {len(W)} rows (nonzero {int((W['Weight']!=0).sum())})\")\n",
    "    return {**state, \"weights\": W, \"status\":\"signals_built\"}\n",
    "\n",
    "def node_simulate(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    cost_rate = float(state.get(\"cost_bps\", 0.001))   # e.g., 10 bps per unit turnover\n",
    "    alignment = str(state.get(\"alignment\",\"next_day\")).lower()\n",
    "\n",
    "    W = state[\"weights\"].copy()              # index Date, col Weight\n",
    "    b = state[\"bundle\"]\n",
    "    if \"NextDay_Return\" in b[\"hist\"].columns:\n",
    "        R = b[\"hist\"][[\"Date\",\"NextDay_Return\"]].copy().set_index(\"Date\").rename(columns={\"NextDay_Return\":\"Ret\"})\n",
    "    else:\n",
    "        # forecast fallback\n",
    "        H = b[\"hist\"].copy()\n",
    "        H[\"NextDay_Return\"] = H[\"Actual_Return\"].shift(-1)\n",
    "        R = H[[\"Date\",\"NextDay_Return\"]].set_index(\"Date\").rename(columns={\"NextDay_Return\":\"Ret\"})\n",
    "\n",
    "    idx = W.index.intersection(R.index)\n",
    "    if len(idx) == 0:\n",
    "        equity_df = pd.DataFrame(columns=[\"Equity\",\"DailyReturn\"], index=W.index)\n",
    "        print(\"[Opt] No overlap between weights and returns — empty equity curve.\")\n",
    "        return {**state, \"equity_df\": equity_df, \"weights_used\": W, \"status\":\"backtested\"}\n",
    "\n",
    "    W = W.loc[idx]\n",
    "    R = R.loc[idx]\n",
    "\n",
    "    W_used = W.shift(1) if alignment == \"next_day\" else W\n",
    "    W_used = W_used.fillna(0.0)\n",
    "\n",
    "    turnover = W.diff().abs().fillna(0.0)[\"Weight\"]\n",
    "    gross = (W_used[\"Weight\"] * R[\"Ret\"])\n",
    "    costs = turnover * cost_rate\n",
    "    net = gross - costs\n",
    "\n",
    "    equity = (1.0 + net).cumprod()\n",
    "    equity_df = pd.DataFrame({\"Equity\": equity, \"DailyReturn\": net}, index=idx)\n",
    "    print(f\"[Opt] Backtest built: {len(equity_df)} days, equity {equity_df['Equity'].iloc[-1]:.4f}\")\n",
    "    return {**state, \"equity_df\": equity_df, \"weights_used\": W_used, \"status\":\"backtested\"}\n",
    "\n",
    "def node_save_min(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    out_dir = state.get(\"out_dir\",\"Opt_results\"); ensure_dir(out_dir)\n",
    "    go_min_weight = float(state.get(\"go_min_weight\", 0.10))  # threshold to act\n",
    "    ticker = state.get(\"ticker\",\"AAPL\")\n",
    "\n",
    "    # Build GO/NO-GO series from weights_used\n",
    "    W = state[\"weights_used\"].copy() if \"weights_used\" in state else pd.DataFrame()\n",
    "    if not W.empty and \"Weight\" in W.columns:\n",
    "        go_series = np.where(np.abs(W[\"Weight\"].values) >= go_min_weight, \"GO\", \"NO-GO\")\n",
    "        go_idx = W.index\n",
    "    else:\n",
    "        go_series = np.array([], dtype=str)\n",
    "        go_idx = pd.DatetimeIndex([])\n",
    "\n",
    "    # Compose equity_curve.csv\n",
    "    ec = state[\"equity_df\"].copy() if \"equity_df\" in state else pd.DataFrame()\n",
    "    if not ec.empty and len(go_series):\n",
    "        go_t = pd.Series(go_series, index=go_idx).reindex(ec.index).fillna(\"NO-GO\")\n",
    "        ec_out = ec.copy()\n",
    "\n",
    "        ec_out[f\"GO_{ticker}\"] = go_t.values\n",
    "    elif not ec.empty:\n",
    "        ec_out = ec.copy()\n",
    "        ec_out[f\"GO_{ticker}\"] = \"NO-GO\"\n",
    "    else:\n",
    "        ec_out = pd.DataFrame(columns=[\"Equity\",\"DailyReturn\",f\"GO_{ticker}\"])\n",
    "\n",
    "    ec_out.reset_index().rename(columns={\"index\":\"Date\"}).to_csv(\n",
    "        os.path.join(out_dir, \"equity_curve.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # Latest position/decision/go-no-go\n",
    "    if not W.empty:\n",
    "        last_dt = W.index.max()\n",
    "        last_w = float(W.loc[last_dt, \"Weight\"])\n",
    "        last_decision = \"LONG\" if last_w > 0 else (\"SHORT\" if last_w < 0 else \"FLAT\")\n",
    "        last_go = \"GO\" if abs(last_w) >= go_min_weight else \"NO-GO\"\n",
    "    else:\n",
    "        last_dt, last_w, last_decision, last_go = None, 0.0, \"FLAT\", \"NO-GO\"\n",
    "\n",
    "    # KPIs\n",
    "    if \"Date\" in ec_out.columns and len(ec_out) > 0:\n",
    "        eq_series = ec_out.set_index(\"Date\")[\"Equity\"]\n",
    "        dr_series = ec_out.set_index(\"Date\")[\"DailyReturn\"]\n",
    "    else:\n",
    "        eq_series = ec_out[\"Equity\"]\n",
    "        dr_series = ec_out[\"DailyReturn\"]\n",
    "\n",
    "    k = kpis(eq_series, dr_series)\n",
    "\n",
    "    summary = {\n",
    "        \"ticker\": ticker,\n",
    "        \"kpis\": k,\n",
    "        \"params\": {\n",
    "            \"mode\": state.get(\"mode\",\"historical\"),\n",
    "            \"head\": state.get(\"head\",\"close\"),\n",
    "            \"alignment\": state.get(\"alignment\",\"next_day\"),\n",
    "            \"allow_shorts\": bool(state.get(\"allow_shorts\", False)),\n",
    "            \"w_max\": float(state.get(\"w_max\", 0.30)),\n",
    "            \"snr_min\": float(state.get(\"snr_min\", 1.0)),\n",
    "            \"strong_pct\": int(state.get(\"strong_pct\", 75)),\n",
    "            \"cost_bps\": float(state.get(\"cost_bps\", 0.001)),\n",
    "            \"go_min_weight\": go_min_weight\n",
    "        },\n",
    "        \"window\": {\n",
    "            \"start\": str(ec_out[\"Date\"].min()) if \"Date\" in ec_out.columns and len(ec_out)>0 else None,\n",
    "            \"end\":   str(ec_out[\"Date\"].max()) if \"Date\" in ec_out.columns and len(ec_out)>0 else None\n",
    "        },\n",
    "        \"latest\": {\n",
    "            \"timestamp\": str(last_dt) if last_dt is not None else None,\n",
    "            \"weight\": last_w,\n",
    "            \"decision\": last_decision,  # LONG / SHORT / FLAT\n",
    "            \"go_nogo\": last_go          # GO / NO-GO\n",
    "        }\n",
    "    }\n",
    "    json.dump(summary, open(os.path.join(out_dir, \"summary.json\"), \"w\"), indent=2)\n",
    "\n",
    "    print(f\"[Opt] Saved: equity_curve.csv  & summary.json → {out_dir}\")\n",
    "    print(f\"[Opt] Last: {summary['latest']}\")\n",
    "    return {**state, \"status\":\"saved\"}\n",
    "\n",
    "# ---------------- workflow glue ----------------\n",
    "def build_workflow():\n",
    "    if not HAS_LANGGRAPH:\n",
    "        raise RuntimeError(\"LangGraph not available. Install it or call nodes manually.\")\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"load\",     node_load)\n",
    "    g.add_node(\"signals\",  node_signals)\n",
    "    g.add_node(\"simulate\", node_simulate)\n",
    "    g.add_node(\"save\",     node_save_min)\n",
    "\n",
    "    g.set_entry_point(\"load\")\n",
    "    g.add_edge(\"load\", \"signals\")\n",
    "    g.add_edge(\"signals\", \"simulate\")\n",
    "    g.add_edge(\"simulate\", \"save\")\n",
    "    g.set_finish_point(\"save\")\n",
    "    return g.compile()\n",
    "\n",
    "\n",
    "def build_optimization_workflow():\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"load\", node_load)\n",
    "    g.add_node(\"signals\", node_signals)\n",
    "    g.add_node(\"simulate\", node_simulate)\n",
    "    g.add_node(\"save\", node_save_min)\n",
    "    g.set_entry_point(\"load\")\n",
    "    g.add_edge(\"load\",\"signals\")\n",
    "    g.add_edge(\"signals\",\"simulate\")\n",
    "    g.add_edge(\"simulate\",\"save\")\n",
    "    g.set_finish_point(\"save\")\n",
    "    return g.compile()\n",
    "\n",
    "# --------------------- run ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Running Optimization Agent…\")\n",
    "    app = build_optimization_workflow()\n",
    "    _ = app.invoke({\n",
    "        \"run_base\": \"/content/drive/MyDrive/A2A_prediction_system/RUN_XXXXXXXX_XXXXXX\",\n",
    "        \"tickers\": [\"AAPL\"],\n",
    "        \"mode\": \"historical\",\n",
    "        \"head\": \"close\",\n",
    "        \"predictions_dir\": \"Predictive_Model/predictions\",\n",
    "        \"forecasts_dir\": \"Predictive_Model/advanced_forecasts\",\n",
    "        \"features_path\": \"FE_Agent/features_engineered.csv\",\n",
    "        \"risk_dir\": \"Risk_Assessment\",\n",
    "\n",
    "        # risk gating & sizing\n",
    "        \"allow_shorts\": False,                # True = long/short\n",
    "        \"w_max\": 0.30,\n",
    "        \"snr_min\": 1.0,\n",
    "        \"strong_pct\": 75,\n",
    "        \"alignment\": \"next_day\",\n",
    "        \"cost_bps\": 0.001,                    # 10 bps per unit turnover\n",
    "        \"go_min_weight\": 0.10,                # threshold for GO / NO-GO\n",
    "\n",
    "        \"alpha\": 0.10,                        # used only if q must be computed from predictions\n",
    "        \"out_dir\": \"Opt_results\"\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "8rj3TnIMd44w"
   },
   "outputs": [],
   "source": [
    "**Summerizer_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCs9G_pfs0Yo",
    "outputId": "355651fa-e390-4ca0-bd0c-f3567cbe6e3f"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/summarizer_agent.py\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# LangGraph\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def _ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _load_json(path: str) -> Any:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _load_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _pick_run_base(state: Dict[str, Any]) -> str:\n",
    "    return state.get(\"run_base\") or \".\"\n",
    "\n",
    "def _default_paths(run_base: str) -> Dict[str, str]:\n",
    "\n",
    "    return {\n",
    "        \"predictions_dir\":       os.path.join(run_base, \"Predictive_Model\", \"predictions\"),\n",
    "        \"models_dir\":            os.path.join(run_base, \"Predictive_Model\", \"lstm_models\"),\n",
    "        \"eval_summary_json\":     os.path.join(run_base, \"Model_Evaluation\", \"summaries\", \"model_evaluation_summary.json\"),\n",
    "\n",
    "\n",
    "        \"desc_summary_json\":     os.path.join(run_base, \"Descriptive\", \"summary.json\"),\n",
    "\n",
    "        # risk / opt\n",
    "        \"risk_summary_json\":     os.path.join(run_base, \"Risk_Assessment\", \"summary.json\"),\n",
    "        \"opt_summary_json\":      os.path.join(run_base, \"Opt_results\", \"summary.json\"),\n",
    "\n",
    "        # explainability\n",
    "        \"xai_summary_json\":      os.path.join(run_base, \"Explainability\", \"xai_summary.json\"),\n",
    "        \"ig_dir\":                os.path.join(run_base, \"Explainability\", \"IG_XAI\"),\n",
    "        \"shap_dir\":              os.path.join(run_base, \"Explainability\", \"SHAP_XAI\"),\n",
    "\n",
    "        # plots from agents\n",
    "        \"eval_plots_dir\":        os.path.join(run_base, \"Model_Evaluation\", \"plots\"),\n",
    "        \"risk_plots_dir\":        os.path.join(run_base, \"Risk_Assessment\", \"plots\"),\n",
    "\n",
    "        # final report\n",
    "        \"out_dir\":               os.path.join(run_base, \"reports\"),\n",
    "        \"out_payload_json\":      os.path.join(run_base, \"reports\", \"final_payload.json\"),\n",
    "        \"out_assets_json\":       os.path.join(run_base, \"reports\", \"assets.json\"),\n",
    "    }\n",
    "\n",
    "def _discover_tickers(predictions_dir: str, tickers: List[str] = None) -> List[str]:\n",
    "    if tickers:\n",
    "        return sorted(list({t.upper() for t in tickers}))\n",
    "    if not os.path.isdir(predictions_dir):\n",
    "        return []\n",
    "    toks = []\n",
    "    for fn in os.listdir(predictions_dir):\n",
    "        if fn.endswith(\"_test_predictions.csv\"):\n",
    "            toks.append(fn.split(\"_\")[0].upper())\n",
    "    return sorted(list(set(toks)))\n",
    "\n",
    "def _safe_get(d: dict, *path, default=None):\n",
    "    cur = d\n",
    "    try:\n",
    "        for k in path:\n",
    "            if cur is None:\n",
    "                return default\n",
    "            cur = cur[k]\n",
    "        return cur if cur is not None else default\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _latest_pred_row(pred_csv: str) -> dict:\n",
    "    df = _load_csv(pred_csv)\n",
    "    if df.empty or \"Date\" not in df.columns:\n",
    "        return {}\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n",
    "    return df.tail(1).to_dict(orient=\"records\")[0] if len(df) else {}\n",
    "\n",
    "def _eval_json_to_dict(obj: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluation agent writes a LIST of rows.\n",
    "    Convert to { TICKER: row } for easy lookup.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, list):\n",
    "        out = {}\n",
    "        for row in obj:\n",
    "            t = row.get(\"Ticker\") or row.get(\"ticker\")\n",
    "            if t:\n",
    "                out[str(t).upper()] = row\n",
    "        return out\n",
    "    return obj if isinstance(obj, dict) else {}\n",
    "\n",
    "def _compute_strong_thr_from_preds(pred_csv: str, pct: int = 75) -> float:\n",
    "\n",
    "    df = _load_csv(pred_csv)\n",
    "    if df.empty:\n",
    "        return 0.002\n",
    "    if \"Pred_Return\" not in df.columns:\n",
    "        if \"Pred_Close_Return\" in df.columns:\n",
    "            df = df.rename(columns={\"Pred_Close_Return\": \"Pred_Return\"})\n",
    "        elif \"Pred_Open_Return\" in df.columns:\n",
    "            df = df.rename(columns={\"Pred_Open_Return\": \"Pred_Return\"})\n",
    "    if \"Pred_Return\" not in df.columns:\n",
    "        return 0.002\n",
    "    try:\n",
    "        thr = float(np.percentile(np.abs(df[\"Pred_Return\"].astype(float).values), pct))\n",
    "        return thr\n",
    "    except Exception:\n",
    "        return 0.002\n",
    "\n",
    "def _derive_decision_for_ticker(t: str,\n",
    "                                opt_json: dict,\n",
    "                                risk_json: dict,\n",
    "                                desc_json: dict,\n",
    "                                pred_csv_path: str) -> Dict[str, Any]:\n",
    "\n",
    "    go = _safe_get(opt_json, \"latest\", \"go_nogo\")\n",
    "    w  = _safe_get(opt_json, \"latest\", \"weight\", default=None)\n",
    "    reason = None\n",
    "\n",
    "    if go is None:\n",
    "        risk_label = _safe_get(risk_json, t, \"risk\", \"Risk_Label\")\n",
    "        thr = _compute_strong_thr_from_preds(pred_csv_path, 75)\n",
    "\n",
    "        last = _latest_pred_row(pred_csv_path)\n",
    "        pred_ret = None\n",
    "        for key in [\"Pred_Return\", \"Pred_Close_Return\", \"Pred_Open_Return\"]:\n",
    "            if key in last:\n",
    "                try:\n",
    "                    pred_ret = float(last[key]); break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if pred_ret is None:\n",
    "            go, reason = \"NO-GO\", \"No latest Pred_Return available\"\n",
    "        else:\n",
    "            if str(risk_label).upper() == \"HIGH\":\n",
    "                go, reason = \"NO-GO\", f\"Risk {risk_label}\"\n",
    "            elif abs(pred_ret) < thr:\n",
    "                go, reason = \"NO-GO\", f\"|Pred_Return|<{thr:.4f}\"\n",
    "            else:\n",
    "                go, reason = \"GO\", f\"|Pred_Return|≥{thr:.4f}\"\n",
    "        w = None if go == \"NO-GO\" else 1.0\n",
    "\n",
    "    return {\"go_nogo\": go, \"weight\": w, \"reason\": reason}\n",
    "\n",
    "def _find_first(paths: List[str]) -> str:\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return \"\"\n",
    "\n",
    "def _plots_for_ticker(paths: Dict[str, str], tslug: str) -> Dict[str, str]:\n",
    "\n",
    "    out = {}\n",
    "    tU, tl = tslug.upper(), tslug.lower()\n",
    "\n",
    "    # Model Evaluation line/scatter (close head)\n",
    "    cand_line = [\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tU}_close_price_line.png\"),\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tl}_close_price_line.png\"),\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tU}_price_line.png\"),\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tl}_price_line.png\"),\n",
    "    ]\n",
    "    cand_scatter = [\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tU}_close_price_scatter.png\"),\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tl}_close_price_scatter.png\"),\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tU}_price_scatter.png\"),\n",
    "        os.path.join(paths[\"eval_plots_dir\"], f\"{tl}_price_scatter.png\"),\n",
    "    ]\n",
    "    p_line = _find_first(cand_line)\n",
    "    p_scatter = _find_first(cand_scatter)\n",
    "    if p_line:    out[\"actual_vs_pred_line\"] = p_line\n",
    "    if p_scatter: out[\"actual_vs_pred_scatter\"] = p_scatter\n",
    "\n",
    "    # Risk plots\n",
    "    rp = paths.get(\"risk_plots_dir\")\n",
    "    for name, fnameU, fnamel in [\n",
    "        (\"drawdown\",      f\"{tU}_drawdown.png\",      f\"{tl}_drawdown.png\"),\n",
    "        (\"rolling_vol\",   f\"{tU}_rolling_vol.png\",   f\"{tl}_rolling_vol.png\"),\n",
    "        (\"tomorrow_band\", f\"{tU}_tomorrow_band.png\", f\"{tl}_tomorrow_band.png\"),\n",
    "        (\"fanchart\",      f\"{tU}_fan_chart.png\",     f\"{tl}_fan_chart.png\"),\n",
    "    ]:\n",
    "        p = _find_first([os.path.join(rp, fnameU), os.path.join(rp, fnamel)])\n",
    "        if p:\n",
    "            out[f\"risk_{name}\"] = p\n",
    "\n",
    "    # ---- XAI (IG & SHAP) ----\n",
    "    # IG (heatmap + importance)\n",
    "    ig_heat = _find_first([\n",
    "        os.path.join(paths[\"ig_dir\"],   tU, \"ig_heatmap_close.png\"),\n",
    "        os.path.join(paths[\"ig_dir\"],   tl, \"ig_heatmap_close.png\"),\n",
    "        os.path.join(paths[\"ig_dir\"],   tU, \"AAPL_heatmap_close.png\"),\n",
    "        os.path.join(paths[\"ig_dir\"],   tl, \"AAPL_heatmap_close.png\"),\n",
    "    ])\n",
    "    ig_import = _find_first([\n",
    "        os.path.join(paths[\"ig_dir\"],   tU, \"ig_global_importance_close.png\"),\n",
    "        os.path.join(paths[\"ig_dir\"],   tl, \"ig_global_importance_close.png\"),\n",
    "    ])\n",
    "\n",
    "    # SHAP (global importance / beeswarm / local)\n",
    "    shap_global = _find_first([\n",
    "        os.path.join(paths[\"shap_dir\"], tU, f\"{tU}_global_importance_close.png\"),\n",
    "        os.path.join(paths[\"shap_dir\"], tl, f\"{tl}_global_importance_close.png\"),\n",
    "    ])\n",
    "    shap_bees = _find_first([\n",
    "        os.path.join(paths[\"shap_dir\"], tU, f\"{tU}_summary_beeswarm_close.png\"),\n",
    "        os.path.join(paths[\"shap_dir\"], tl, f\"{tl}_summary_beeswarm_close.png\"),\n",
    "        os.path.join(paths[\"shap_dir\"], tU, \"AAPL_summary_beeswarm_close.png\"),\n",
    "        os.path.join(paths[\"shap_dir\"], tl, \"AAPL_summary_beeswarm_close.png\"),\n",
    "    ])\n",
    "    shap_local = _find_first([\n",
    "        os.path.join(paths[\"shap_dir\"], tU, f\"{tU}_local_waterfall_last_close.png\"),\n",
    "        os.path.join(paths[\"shap_dir\"], tl, f\"{tl}_local_waterfall_last_close.png\"),\n",
    "        os.path.join(paths[\"shap_dir\"], tU, \"AAPL_local_waterfall_last_close.png\"),\n",
    "        os.path.join(paths[\"shap_dir\"], tl, \"AAPL_local_waterfall_last_close.png\"),\n",
    "    ])\n",
    "\n",
    "    # What to expose to the Report Agent\n",
    "    if ig_heat:    out[\"ig_heatmap\"]   = ig_heat\n",
    "    if ig_import:  out[\"ig_importance\"] = ig_import\n",
    "    if shap_global: out[\"shap_bar\"]    = shap_global\n",
    "    elif shap_bees: out[\"shap_bar\"]    = shap_bees\n",
    "    if shap_local:  out[\"shap_local\"]  = shap_local\n",
    "\n",
    "    return out\n",
    "\n",
    "# ------------------ Sharpe & MaxDD from risk json ------------------\n",
    "def _extract_sharpe_and_mdd(risk_json_for_ticker: dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try multiple common key paths/names for Sharpe (annualised) and Max Drawdown.\n",
    "    Returns dict with keys: {'sharpe_ann': float|None, 'max_drawdown': float|None}\n",
    "    \"\"\"\n",
    "    # Check nested 'kpis'\n",
    "    sharpe = _safe_get(risk_json_for_ticker, \"kpis\", \"Sharpe_ann\")\n",
    "    if sharpe is None: sharpe = _safe_get(risk_json_for_ticker, \"kpis\", \"Sharpe\")\n",
    "    if sharpe is None: sharpe = _safe_get(risk_json_for_ticker, \"risk\", \"Sharpe_ann\")\n",
    "    if sharpe is None: sharpe = _safe_get(risk_json_for_ticker, \"Sharpe_ann\")\n",
    "    if sharpe is None: sharpe = _safe_get(risk_json_for_ticker, \"Sharpe\")\n",
    "    if sharpe is None: sharpe = _safe_get(risk_json_for_ticker, \"SharpeRatio\")\n",
    "\n",
    "    mdd = _safe_get(risk_json_for_ticker, \"kpis\", \"Max_Drawdown\")\n",
    "    if mdd is None: mdd = _safe_get(risk_json_for_ticker, \"risk\", \"Max_Drawdown\")\n",
    "    if mdd is None: mdd = _safe_get(risk_json_for_ticker, \"Max_Drawdown\")\n",
    "    if mdd is None: mdd = _safe_get(risk_json_for_ticker, \"max_drawdown\")\n",
    "    if mdd is None: mdd = _safe_get(risk_json_for_ticker, \"MDD\")\n",
    "\n",
    "    # Cast to floats where possible\n",
    "    try: sharpe = None if sharpe is None else float(sharpe)\n",
    "    except: sharpe = None\n",
    "    try: mdd = None if mdd is None else float(mdd)\n",
    "    except: mdd = None\n",
    "\n",
    "    return {\"sharpe_ann\": sharpe, \"max_drawdown\": mdd}\n",
    "\n",
    "# ------------------ LangGraph nodes ------------------\n",
    "def node_collect(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    run_base = _pick_run_base(state)\n",
    "    paths = _default_paths(run_base)\n",
    "\n",
    "    # allow overrides\n",
    "    for k, v in (state.get(\"paths\") or {}).items():\n",
    "        paths[k] = v\n",
    "\n",
    "    tickers = _discover_tickers(paths[\"predictions_dir\"], state.get(\"tickers\"))\n",
    "\n",
    "    # load json artifacts\n",
    "    eval_js_raw = _load_json(paths[\"eval_summary_json\"]) or []\n",
    "    eval_js     = _eval_json_to_dict(eval_js_raw)  # normalize to dict by ticker\n",
    "    desc_js     = _load_json(paths[\"desc_summary_json\"]) or {}\n",
    "    risk_js     = _load_json(paths[\"risk_summary_json\"]) or {}\n",
    "    opt_js      = _load_json(paths[\"opt_summary_json\"])  or {}\n",
    "    xai_js      = _load_json(paths[\"xai_summary_json\"])  or {}\n",
    "\n",
    "    _ensure_dir(paths[\"out_dir\"])\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"paths\": paths,\n",
    "        \"tickers\": tickers,\n",
    "        \"eval_json\": eval_js,\n",
    "        \"desc_json\": desc_js,\n",
    "        \"risk_json\": risk_js,\n",
    "        \"opt_json\": opt_js,\n",
    "        \"xai_json\": xai_js,\n",
    "        \"status\": \"collected\"\n",
    "    }\n",
    "\n",
    "def node_summarize(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    tickers = state[\"tickers\"]\n",
    "    P = state[\"paths\"]\n",
    "\n",
    "    # labels\n",
    "    ui_labels = {\n",
    "        \"prediction_accuracy\": \"Prediction Accuracy\",   # R², RMSE, MAPE\n",
    "        \"trend_prediction\":    \"Trend Prediction\",      # Directional Accuracy %\n",
    "        \"statistical_insights\":\"Statistical Insights\",  # Sharpe (ann.) and Max Drawdown\n",
    "        \"risk_assessment\":     \"Risk Assessment\",       # Label, SNR, Crosses Zero\n",
    "        \"model_explainability\":\"Model Explainability\",  # SHAP / IG status\n",
    "        \"final_recommendation\":\"Final Recommendation\",  # Go / No-Go\n",
    "    }\n",
    "\n",
    "    out = {\n",
    "        \"as_of\": None,\n",
    "        \"run_base\": _pick_run_base(state),\n",
    "        \"tickers\": tickers,\n",
    "        \"ui_labels\": ui_labels,\n",
    "        \"per_ticker\": {}\n",
    "    }\n",
    "\n",
    "    for t in tickers:\n",
    "        tslug = t.upper()\n",
    "\n",
    "        # evaluation\n",
    "        e = state[\"eval_json\"].get(tslug, {})\n",
    "\n",
    "        # risk (summary per ticker)\n",
    "        r_all = state[\"risk_json\"] if isinstance(state[\"risk_json\"], dict) else {}\n",
    "        r_t   = r_all.get(tslug, {}) if isinstance(r_all, dict) else {}\n",
    "        # Extract Sharpe & MaxDD\n",
    "        kpis  = _extract_sharpe_and_mdd(r_t)\n",
    "\n",
    "        # opt (single summary)\n",
    "        o = state[\"opt_json\"] or {}\n",
    "\n",
    "        # Pred file path\n",
    "        pred_csv = os.path.join(P[\"predictions_dir\"], f\"{tslug}_test_predictions.csv\")\n",
    "        decision = _derive_decision_for_ticker(tslug, o, state[\"risk_json\"], state[\"desc_json\"], pred_csv)\n",
    "\n",
    "        # assemble\n",
    "        per = {\n",
    "            \"ticker\": tslug,\n",
    "            \"head\": \"close\",\n",
    "\n",
    "            # --- Prediction Accuracy ---\n",
    "            \"evaluation\": {\n",
    "                \"R2_price\": e.get(\"R2(price)\"),\n",
    "                \"RMSE_price\": e.get(\"RMSE_price\"),\n",
    "                \"MAE_price\": e.get(\"MAE_price\"),\n",
    "                \"MAPE_pct\": e.get(\"MAPE\"),\n",
    "                \"DirAcc_pct\": e.get(\"DirAcc(%)\") or e.get(\"DirAcc\"),\n",
    "                \"Rows\": e.get(\"Rows\"),\n",
    "                \"Test_Start\": e.get(\"Test_Start\"),\n",
    "                \"Test_End\": e.get(\"Test_End\"),\n",
    "            },\n",
    "\n",
    "            # --- Statistical Insights  ---\n",
    "            \"statistical\": {\n",
    "                \"Sharpe_ann\": kpis.get(\"sharpe_ann\"),      # annualised Sharpe\n",
    "                \"Max_Drawdown\": kpis.get(\"max_drawdown\"),  # max drawdown\n",
    "            },\n",
    "\n",
    "            # --- Risk Assessment ---\n",
    "            \"risk\": {\n",
    "                \"label\": _safe_get(r_t, \"risk\", \"Risk_Label\"),\n",
    "                \"snr_latest\": _safe_get(r_t, \"risk\", \"SNR\"),\n",
    "                \"crosses_zero_latest\": _safe_get(r_t, \"risk\", \"Crosses_Zero\"),\n",
    "            },\n",
    "\n",
    "            # --- Optimization / Decision ---\n",
    "            \"optimization\": {\n",
    "                \"decision\": _safe_get(state[\"opt_json\"], \"latest\", \"go_nogo\"),\n",
    "                \"weight\": _safe_get(state[\"opt_json\"], \"latest\", \"weight\"),\n",
    "                \"kpis\": state[\"opt_json\"].get(\"kpis\", {})\n",
    "            },\n",
    "\n",
    "            # XAI summary\n",
    "            \"xai\": {\"ig_top\": None, \"shap_top\": None, \"notes\": \"Populate if you persist a combined XAI summary.\"},\n",
    "\n",
    "            # Final decision\n",
    "            \"decision\": decision,\n",
    "\n",
    "            # Collect plots\n",
    "            \"plots\": _plots_for_ticker(P, tslug)\n",
    "        }\n",
    "\n",
    "        out[\"per_ticker\"][tslug] = per\n",
    "        if out[\"as_of\"] is None:\n",
    "\n",
    "            out[\"as_of\"] = e.get(\"Test_End\")\n",
    "\n",
    "    return {**state, \"payload\": out, \"status\": \"summarized\"}\n",
    "\n",
    "def node_save(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    P = state[\"paths\"]\n",
    "    _ensure_dir(P[\"out_dir\"])\n",
    "    with open(P[\"out_payload_json\"], \"w\") as f:\n",
    "        json.dump(state[\"payload\"], f, indent=2)\n",
    "\n",
    "    assets = {t: state[\"payload\"][\"per_ticker\"][t][\"plots\"] for t in state[\"payload\"][\"per_ticker\"]}\n",
    "    with open(P[\"out_assets_json\"], \"w\") as f:\n",
    "        json.dump(assets, f, indent=2)\n",
    "\n",
    "    print(f\"Summarizer wrote:\\n  - {P['out_payload_json']}\\n  - {P['out_assets_json']}\")\n",
    "    return {**state, \"status\": \"saved\", \"out_payload\": P[\"out_payload_json\"], \"out_assets\": P[\"out_assets_json\"]}\n",
    "\n",
    "# ------------------ Build workflow ------------------\n",
    "def build_summarizer_workflow():\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"collect\", node_collect)\n",
    "    g.add_node(\"summarize\", node_summarize)\n",
    "    g.add_node(\"save\", node_save)\n",
    "    g.set_entry_point(\"collect\")\n",
    "    g.add_edge(\"collect\", \"summarize\")\n",
    "    g.add_edge(\"summarize\", \"save\")\n",
    "    g.set_finish_point(\"save\")\n",
    "    return g.compile()\n",
    "\n",
    "# ------------------ Standalone run ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Running Agent Summarizer…\")\n",
    "    app = build_summarizer_workflow()\n",
    "    _ = app.invoke({\n",
    "        \"run_base\": \"/content/drive/MyDrive/A2A_prediction_system/RUN_XXXXXXXX_XXXXXX\",\n",
    "        \"tickers\": [\"AAPL\"],\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "s47OhBADeq45"
   },
   "outputs": [],
   "source": [
    "**Report_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VidPXMfH-FCl",
    "outputId": "5d9bc35b-f5d9-49f3-904e-6f44dbf88f72"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/A2A_prediction_system/backend/a2a/report_agent.py\n",
    "\n",
    "# backend/a2a/report_agent.py\n",
    "import os, json, glob, datetime, base64\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# ---------------------------- utils ----------------------------\n",
    "def _ensure_dir(p: str): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _safe_get(d: dict, *path, default=None):\n",
    "    cur = d\n",
    "    for k in path:\n",
    "        if cur is None:\n",
    "            return default\n",
    "        if isinstance(cur, dict):\n",
    "            cur = cur.get(k)\n",
    "        elif isinstance(cur, list) and isinstance(k, int):\n",
    "            cur = cur[k] if 0 <= k < len(cur) else None\n",
    "        else:\n",
    "            return default\n",
    "    return default if cur is None else cur\n",
    "\n",
    "def _dash(msg: str | None = None, html: bool = True) -> str:\n",
    "    if html:\n",
    "        tip = (msg or \"not computed\").replace(\"'\", \"\\\\'\")\n",
    "        return f\"<span title='{tip}'>—</span>\"\n",
    "    return \"—\"\n",
    "\n",
    "def _fmt_str(x: Optional[str], html: bool = True) -> str:\n",
    "    s = (x or \"\").strip()\n",
    "    return s if s else _dash(html=html)\n",
    "\n",
    "def _fmt_bool(x, html: bool = True) -> str:\n",
    "    if x is True:  return \"True\"\n",
    "    if x is False: return \"False\"\n",
    "    return _dash(\"not available\", html=html)\n",
    "\n",
    "def _fmt_num(x, digits=3, money=False):\n",
    "    try:\n",
    "        v = float(x)\n",
    "    except Exception:\n",
    "        return \"—\"\n",
    "    return (f\"${v:.{digits}f}\" if money else f\"{v:.{digits}f}\")\n",
    "\n",
    "def _decision_badge(decision: Optional[str]) -> str:\n",
    "    d = (decision or \"\").strip().upper()\n",
    "    if d == \"GO\":    return \"<b>GO</b>\"\n",
    "    if d == \"NO-GO\": return \"<b>NO-GO</b>\"\n",
    "    return \"—\"\n",
    "\n",
    "def _autodetect_run_base(run_base: str) -> str:\n",
    "    if os.path.exists(os.path.join(run_base, \"reports\", \"final_payload.json\")):\n",
    "        return run_base\n",
    "    candidates = []\n",
    "    for p in glob.glob(os.path.join(\"runs\", \"*\", \"reports\", \"final_payload.json\")):\n",
    "        try:\n",
    "            mtime = os.path.getmtime(p)\n",
    "            base = os.path.dirname(os.path.dirname(p))\n",
    "            candidates.append((mtime, base))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if candidates:\n",
    "        candidates.sort()\n",
    "        return candidates[-1][1]\n",
    "    return run_base\n",
    "\n",
    "def _img_data_uri(path: str) -> Optional[str]:\n",
    "    if not path or not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "        return f\"data:image/png;base64,{b64}\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------------------- main render ----------------------------\n",
    "def render_report(run_base: str = \".\", out_name: str = \"llm_report.md\",\n",
    "                  order: Optional[List[str]] = None) -> str:\n",
    "\n",
    "    run_base = _autodetect_run_base(run_base)\n",
    "\n",
    "    reports_dir = os.path.join(run_base, \"reports\")\n",
    "    payload_path = os.path.join(reports_dir, \"final_payload.json\")\n",
    "    assets_path  = os.path.join(reports_dir, \"assets.json\")\n",
    "\n",
    "    if not os.path.exists(payload_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing {payload_path}. Run the Summarizer agent for the same run_base first.\"\n",
    "        )\n",
    "\n",
    "    payload = json.load(open(payload_path, \"r\"))\n",
    "    assets  = json.load(open(assets_path, \"r\")) if os.path.exists(assets_path) else {}\n",
    "\n",
    "    as_of = payload.get(\"as_of\") or datetime.date.today().isoformat()\n",
    "    tickers = order or payload.get(\"tickers\") or list((_safe_get(payload, \"per_ticker\") or {}).keys())\n",
    "    tickers = [t.upper() for t in tickers]\n",
    "\n",
    "    # ---- UI labels\n",
    "    ui = payload.get(\"ui_labels\") or {\n",
    "        \"prediction_accuracy\": \"Prediction Accuracy\",\n",
    "        \"trend_prediction\":    \"Trend Prediction\",\n",
    "        \"statistical_insights\":\"Statistical Insights\",\n",
    "        \"risk_assessment\":     \"Risk Assessment\",\n",
    "        \"model_explainability\":\"Model Explainability\",\n",
    "        \"final_recommendation\":\"Final Recommendation\",\n",
    "    }\n",
    "\n",
    "    # Helper to choose a plot path from candidates, with fuzzy/glob + path resolution\n",
    "    def _get_plot(node_plots: Dict[str, str], asset_plots: Dict[str, str], key_candidates: List[str]) -> Optional[str]:\n",
    "        def _resolve(p: Optional[str]) -> Optional[str]:\n",
    "            if not p:\n",
    "                return None\n",
    "            for cand in (\n",
    "                p,\n",
    "                os.path.join(reports_dir, p),\n",
    "                os.path.join(run_base, p),\n",
    "            ):\n",
    "                if os.path.exists(cand):\n",
    "                    return cand\n",
    "            return None\n",
    "\n",
    "        # exact keys\n",
    "        for k in key_candidates:\n",
    "            p = (node_plots or {}).get(k) or (asset_plots or {}).get(k)\n",
    "            rp = _resolve(p)\n",
    "            if rp:\n",
    "                return rp\n",
    "\n",
    "        # fuzzy on dict keys\n",
    "        def _match_any(d: Dict[str, str], must_tokens: List[str]) -> Optional[str]:\n",
    "            for k, v in (d or {}).items():\n",
    "                lab = (k or \"\").lower()\n",
    "                if all(tok in lab for tok in must_tokens):\n",
    "                    rp = _resolve(v)\n",
    "                    if rp:\n",
    "                        return rp\n",
    "            return None\n",
    "\n",
    "        toks_sets = []\n",
    "        # infer token sets from candidates provided\n",
    "        cand_l = \" \".join(key_candidates).lower()\n",
    "        if \"ig\" in cand_l:\n",
    "            toks_sets += [[\"ig\", \"heatmap\"], [\"ig\", \"importance\"]]\n",
    "        if \"shap\" in cand_l:\n",
    "            toks_sets += [[\"shap\", \"beeswarm\"], [\"shap\", \"importance\"], [\"shap\", \"global\"]]\n",
    "\n",
    "        for dsrc in (node_plots or {}), (asset_plots or {}):\n",
    "            for toks in toks_sets:\n",
    "                hit = _match_any(dsrc, toks)\n",
    "                if hit:\n",
    "                    return hit\n",
    "\n",
    "        # glob search on disk for common patterns\n",
    "        import glob as _glob\n",
    "        def _glob_first(patterns: List[str]) -> Optional[str]:\n",
    "            roots = [reports_dir, os.path.join(run_base, \"reports\"), run_base]\n",
    "            for root in roots:\n",
    "                for pat in patterns:\n",
    "                    hits = _glob.glob(os.path.join(root, \"**\", pat), recursive=True)\n",
    "                    if hits:\n",
    "                        return hits[0]\n",
    "            return None\n",
    "\n",
    "        patterns = []\n",
    "        if \"ig\" in cand_l:\n",
    "            patterns += [\"*ig*heatmap*.png\", \"*ig*importance*.png\"]\n",
    "        if \"shap\" in cand_l:\n",
    "            patterns += [\"*shap*beeswarm*.png\", \"*shap*importance*.png\", \"*global*importance*shap*.png\"]\n",
    "\n",
    "        patterns += [\n",
    "            \"ig_heatmap_close.png\",\n",
    "            \"ig_global_importance_close.png\",\n",
    "            \"*_summary_beeswarm_close.png\",\n",
    "            \"*_local_waterfall_last_close.png\",\n",
    "            \"*_global_importance_close.png\",\n",
    "        ]\n",
    "        ghit = _glob_first(patterns)\n",
    "        if ghit:\n",
    "            return ghit\n",
    "\n",
    "        return None\n",
    "\n",
    "    make_html = out_name.lower().endswith(\".html\")\n",
    "    blocks: List[str] = []\n",
    "\n",
    "    # ---------- Header ----------\n",
    "    if make_html:\n",
    "        blocks.append(\"\"\"<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\">\n",
    "<title>Stock Insights Report</title>\n",
    "<style>\n",
    " body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif; line-height:1.5; padding:24px; max-width:960px; margin:auto;}\n",
    " h1,h2{margin:0.6em 0;}\n",
    " .muted{color:#666}\n",
    " .section{margin:24px 0;}\n",
    " img{max-width:100%; height:auto; border:1px solid #eee; border-radius:8px; padding:4px; background:#fff;}\n",
    " details{margin:8px 0 16px 0;}\n",
    " table{border-collapse:collapse; width:100%; font-size:14px;}\n",
    " th,td{border:1px solid #ddd; padding:6px 8px;}\n",
    " th{background:#fafafa; text-align:left;}\n",
    " .hr{border-top:1px solid #eee; margin:24px 0;}\n",
    "</style>\n",
    "</head><body>\n",
    "\"\"\")\n",
    "        blocks.append(f\"<h1>Stock Insights Report</h1>\")\n",
    "        blocks.append(f\"<p class='muted'>Report Date <b>{as_of}</b></p>\")\n",
    "        blocks.append(\"<p><i>Note: This report summarizes model predictions and risk metrics. It is informational and <b>not investment advice</b>.</i></p>\")\n",
    "    else:\n",
    "        blocks.append(\"# Stock Insights Report\\n\")\n",
    "        blocks.append(f\"_As of **{as_of}**_\\n\")\n",
    "        blocks.append(\"> **Note:** This report summarizes model predictions and risk metrics. It is informational and **not investment advice**.\\n\")\n",
    "\n",
    "    # ---------- Per-ticker sections ----------\n",
    "    for t in tickers:\n",
    "        node = _safe_get(payload, \"per_ticker\", t) or {}\n",
    "        eval_ = node.get(\"evaluation\", {}) or {}\n",
    "        stat_ = node.get(\"statistical\", {}) or {}\n",
    "        risk_ = node.get(\"risk\", {}) or {}\n",
    "        opt_  = node.get(\"optimization\", {}) or {}\n",
    "        final = node.get(\"decision\", {}) or {}\n",
    "        node_plots = node.get(\"plots\", {}) or {}\n",
    "        asset_plots = assets.get(t, {}) if isinstance(assets, dict) else {}\n",
    "\n",
    "        r2     = _fmt_num(_safe_get(eval_, \"R2_price\"), 3)\n",
    "        rmse   = _fmt_num(_safe_get(eval_, \"RMSE_price\"), 3, money=True)\n",
    "        mape   = _fmt_num(_safe_get(eval_, \"MAPE_pct\"), 2)\n",
    "        diracc = _fmt_num(_safe_get(eval_, \"DirAcc_pct\"), 1)\n",
    "        rows   = _safe_get(eval_, \"Rows\", default=\"—\")\n",
    "\n",
    "        # Risk\n",
    "        html_mode = make_html\n",
    "        raw_label = risk_.get(\"label\")\n",
    "        risk_label = (raw_label.upper() if raw_label else \"—\")\n",
    "        snr        = _fmt_num(risk_.get(\"snr_latest\"), 2)\n",
    "        cz         = _fmt_bool(risk_.get(\"crosses_zero_latest\"), html=html_mode)\n",
    "\n",
    "        # Decision\n",
    "        raw_decision = final.get(\"go_nogo\") or opt_.get(\"decision\")\n",
    "        decision     = (raw_decision or \"NO-GO\")\n",
    "        weight       = final.get(\"weight\") if final.get(\"weight\") is not None else opt_.get(\"weight\")\n",
    "        reason       = _fmt_str(final.get(\"reason\") or \"\", html=html_mode)\n",
    "\n",
    "        # SHAP / IG plots existence\n",
    "        p_shap = _get_plot(node_plots, asset_plots, [\"shap_bar\", \"xai_shap_bar\", \"shap\", \"beeswarm\", \"importance\"])\n",
    "        p_ig   = _get_plot(node_plots, asset_plots, [\"ig_heatmap\", \"xai_ig_heatmap\", \"ig\", \"importance\", \"heatmap\"])\n",
    "        shap_status = \"Shown\" if p_shap else _dash(\"SHAP skipped or not available\", html=html_mode)\n",
    "        ig_status   = \"Shown\" if p_ig   else _dash(\"IG not available\", html=html_mode)\n",
    "\n",
    "        # Section title + executive summary\n",
    "        if make_html:\n",
    "            blocks.append(f\"<div class='section'><h2>{t}</h2>\")\n",
    "            blocks.append(\"<h3>Executive Summary</h3>\")\n",
    "            blocks.append(\n",
    "                f\"<ul>\"\n",
    "                f\"<li><b>{ui['prediction_accuracy']}</b>: R² <b>{r2}</b>, RMSE <b>{rmse}</b>, MAPE <b>{mape}%</b> (N={rows}).</li>\"\n",
    "                f\"<li><b>{ui['risk_assessment']}</b>: label <b>{risk_label}</b>, SNR <b>{snr}</b>, crosses-zero=<code>{cz}</code>.</li>\"\n",
    "                f\"<li><b>{ui['final_recommendation']}</b>: \" + _decision_badge(decision) +\n",
    "                (f\" (weight={_fmt_num(weight, 2)})\" if weight is not None else \"\") +\n",
    "                (f\" — {reason}\" if reason.strip('— ').strip() else \"\") + \".</li></ul>\"\n",
    "            )\n",
    "\n",
    "            # Metrics (HTML)\n",
    "            blocks.append(\"<details><summary><b>Metrics</b> (click to expand)</summary>\")\n",
    "            blocks.append(\"<table><thead><tr><th>Category</th><th>Metric</th><th>Value</th></tr></thead><tbody>\")\n",
    "            rows_tbl = [\n",
    "                (ui[\"prediction_accuracy\"], \"R² (price)\", r2),\n",
    "                (ui[\"prediction_accuracy\"], \"RMSE (price)\", rmse),\n",
    "                (ui[\"prediction_accuracy\"], \"MAPE\", f\"{mape}%\"),\n",
    "                (ui[\"trend_prediction\"], \"Directional Accuracy\", f\"{diracc}%\"),\n",
    "                (ui[\"statistical_insights\"], \"Sharpe (annualised)\", _fmt_num(stat_.get(\"Sharpe_ann\"), 2)),\n",
    "                (ui[\"statistical_insights\"], \"Max Drawdown\", _fmt_num(stat_.get(\"Max_Drawdown\"), 2)),\n",
    "                (ui[\"risk_assessment\"], \"Label\", risk_label),\n",
    "                (ui[\"risk_assessment\"], \"SNR (latest)\", snr),\n",
    "                (ui[\"risk_assessment\"], \"Crosses zero\", cz),\n",
    "                (ui[\"model_explainability\"], \"SHAP status\", shap_status),\n",
    "                (ui[\"model_explainability\"], \"IG status\", ig_status),\n",
    "                (ui[\"final_recommendation\"], \"Final\", decision),\n",
    "            ]\n",
    "            for cat, m, v in rows_tbl:\n",
    "                blocks.append(f\"<tr><td>{cat}</td><td>{m}</td><td>{v}</td></tr>\")\n",
    "            blocks.append(\"</tbody></table></details>\")\n",
    "        else:\n",
    "            blocks.append(f\"## {t}\\n\")\n",
    "            blocks.append(\"**Executive Summary**\\n\")\n",
    "            blocks.append(f\"- **{ui['prediction_accuracy']}**: R² **{r2}**, RMSE **{rmse}**, MAPE **{mape}%** _(N={rows})_.\")\n",
    "            blocks.append(f\"- **{ui['risk_assessment']}**: label **{risk_label}**, SNR **{snr}**, crosses-zero={cz}.\")\n",
    "            badges = _decision_badge(decision)\n",
    "            wtxt = (f\" (weight={_fmt_num(weight, 2)})\" if weight is not None else \"\")\n",
    "            rtxt = (f\" — {reason}\" if reason.strip('— ').strip() else \"\")\n",
    "            blocks.append(f\"- **{ui['final_recommendation']}**: {badges}{wtxt}{rtxt}.\\n\")\n",
    "\n",
    "            # Metrics (Markdown + HTML <details> for collapsible)\n",
    "            blocks.append(\"<details><summary><b>Metrics</b> (click to expand)</summary>\\n\")\n",
    "            blocks.append(\"| Category | Metric | Value |\")\n",
    "            blocks.append(\"|---|---|---|\")\n",
    "            md_rows = [\n",
    "                (ui[\"prediction_accuracy\"], \"R² (price)\", r2),\n",
    "                (ui[\"prediction_accuracy\"], \"RMSE (price)\", rmse),\n",
    "                (ui[\"prediction_accuracy\"], \"MAPE\", f\"{mape}%\"),\n",
    "                (ui[\"trend_prediction\"], \"Directional Accuracy\", f\"{diracc}%\"),\n",
    "                (ui[\"statistical_insights\"], \"Sharpe (annualised)\", _fmt_num(stat_.get(\"Sharpe_ann\"), 2)),\n",
    "                (ui[\"statistical_insights\"], \"Max Drawdown\", _fmt_num(stat_.get(\"Max_Drawdown\"), 2)),\n",
    "                (ui[\"risk_assessment\"], \"Label\", risk_label),\n",
    "                (ui[\"risk_assessment\"], \"SNR (latest)\", snr),\n",
    "                (ui[\"risk_assessment\"], \"Crosses zero\", cz),\n",
    "                (ui[\"model_explainability\"], \"SHAP status\", (\"Shown\" if p_shap else \"— (SHAP skipped or not available)\")),\n",
    "                (ui[\"model_explainability\"], \"IG status\", (\"Shown\" if p_ig else \"— (IG not available)\")),\n",
    "                (ui[\"final_recommendation\"], \"Final\", decision),\n",
    "            ]\n",
    "            for cat, m, v in md_rows:\n",
    "                blocks.append(f\"| {cat} | {m} | {v} |\")\n",
    "            blocks.append(\"\\n</details>\\n\")\n",
    "\n",
    "        # Plots (auto-hide missing)\n",
    "        p_line    = _get_plot(node_plots, asset_plots, [\"actual_vs_pred_line\"])\n",
    "        p_scatter = _get_plot(node_plots, asset_plots, [\"actual_vs_pred_scatter\"])\n",
    "        p_dd      = _get_plot(node_plots, asset_plots, [\"risk_drawdown\"])\n",
    "        p_vol     = _get_plot(node_plots, asset_plots, [\"risk_rolling_vol\"])\n",
    "        p_tb      = _get_plot(node_plots, asset_plots, [\"risk_tomorrow_band\"])\n",
    "        p_fc      = _get_plot(node_plots, asset_plots, [\"risk_fanchart\"])\n",
    "        # XAI (broaden keys; fuzzy+glob matching inside _get_plot handles variations)\n",
    "        p_shap    = _get_plot(node_plots, asset_plots, [\"shap_bar\", \"xai_shap_bar\", \"shap\", \"beeswarm\", \"importance\"])\n",
    "        p_ig      = _get_plot(node_plots, asset_plots, [\"ig_heatmap\", \"xai_ig_heatmap\", \"ig\", \"importance\", \"heatmap\"])\n",
    "\n",
    "        ordered = [\n",
    "            (\"Actual vs Predicted Close (test)\", p_line),\n",
    "            (\"Actual vs Predicted Close (scatter)\", p_scatter),\n",
    "            (\"Drawdown\", p_dd),\n",
    "            (\"Rolling Volatility (20d)\", p_vol),\n",
    "            (\"Tomorrow Close Uncertainty\", p_tb),\n",
    "            (\"7-Day Forecast Fan Chart\", p_fc),\n",
    "            (\"Explainability — SHAP\", p_shap),\n",
    "            (\"Explainability — Integrated Gradients\", p_ig),\n",
    "        ]\n",
    "        for title, src in ordered:\n",
    "            if not src:\n",
    "                continue\n",
    "            if make_html:\n",
    "                data_uri = _img_data_uri(src)\n",
    "                if data_uri:\n",
    "                    blocks.append(f\"<p><b>{title}</b><br><img src='{data_uri}' alt='{title}'></p>\")\n",
    "                elif os.path.exists(src):\n",
    "                    # Fallback to relative path if base64 fails\n",
    "                    rel = os.path.relpath(src, reports_dir) if os.path.exists(reports_dir) else src\n",
    "                    blocks.append(f\"<p><b>{title}</b><br><img src='{rel}' alt='{title}'></p>\")\n",
    "            else:\n",
    "                blocks.append(f\"**{title}**\\n\")\n",
    "                blocks.append(f\"![{title}]({src})\\n\")\n",
    "\n",
    "        if make_html:\n",
    "            blocks.append(\"<div class='hr'></div></div>\")\n",
    "        else:\n",
    "            blocks.append(\"---\\n\")\n",
    "\n",
    "    # ---------- Footer ----------\n",
    "    if make_html:\n",
    "        blocks.append(\"<p class='muted'><i>This report is automatically generated from model outputs and stock market data. \"\n",
    "                      \"It is <b>not</b> investment advice.</i></p></body></html>\")\n",
    "        content = \"\\n\".join(blocks)\n",
    "    else:\n",
    "        blocks.append(\"> _This report is automatically generated from model outputs and stock market data. It is **not** investment advice._\\n\")\n",
    "        content = \"\\n\".join(blocks)\n",
    "\n",
    "    _ensure_dir(reports_dir)\n",
    "    out_path = os.path.join(reports_dir, out_name)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Report written → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# ---------------------------- LangGraph wrapper ----------------------------\n",
    "try:\n",
    "    from langgraph.graph import StateGraph\n",
    "    HAS_LG = True\n",
    "except Exception:\n",
    "    HAS_LG = False\n",
    "\n",
    "def node_render(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    run_base = state.get(\"run_base\") or \".\"\n",
    "    order    = state.get(\"tickers\")\n",
    "    out_name = state.get(\"out_name\", \"llm_report.html\")  # default to HTML\n",
    "    path = render_report(run_base=run_base, out_name=out_name, order=order)\n",
    "    return {**state, \"status\": \"ok\", \"report_path\": path}\n",
    "\n",
    "def build_llm_report_workflow():\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"render\", node_render)\n",
    "    g.set_entry_point(\"render\")\n",
    "    g.set_finish_point(\"render\")\n",
    "    return g.compile()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_base = os.environ.get(\"RUN_BASE\", \".\")\n",
    "    tickers  = os.environ.get(\"TICKERS\")\n",
    "    order = [t.strip().upper() for t in (tickers.split(\",\") if tickers else [])] if tickers else None\n",
    "    try:\n",
    "        render_report(run_base=run_base, out_name=\"llm_report.html\", order=order)\n",
    "    except Exception as e:\n",
    "        print(\"Report generation failed:\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
